{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Corridor GenGuardX (\"GGX\")?","text":"<p>Corridor GenGuardX is a Responsible AI Governance &amp; Testing Automation Platform designed by risk management experts to help companies harness the benefits of GenAI.</p> <p>Enabling them to move from experimentation stage to high ROI use cases which usually require strong end-to-end pipeline testing, regulatory governance and continual human in the loop monitoring.</p> <p>Whether it's leveraging external agents or in-house custom solutions. Corridor provides trust and comfort to facilitate efficient @scale deployment of GenAI - agnostic of industry.</p>      Your browser does not support the video tag.  <p>It enables the secure and compliant deployment of high-impact GenAI applications, including IVR systems, agent assist tools, and chatbots. It addresses critical challenges often overlooked by existing tools, including robust model risk management, hallucinations, PII leakage, and fair lending bias. GGX ensures comprehensive governance through rigorous full-pipeline testing, out-of-the-box standardized evaluation metrics, regulatory compliance checks, and continuous human-in-the-loop oversight. By streamlining these processes across the LLM pipeline, GGX empowers organizations to confidently transition from use-case experimentation to production, scaling trusted, high-ROI customer-facing solutions.</p> <p>GGX offers a structured framework for registering, refining, evaluating, approving, deploying, and monitoring GenAI applications. It provides:</p> <ul> <li>\u2705 Register &amp; Refine \u2013 A point-and-click interface for building GenAI applications, along with tools for optimizing prompts, retrieval-augmented generation (RAG), and pipelines.</li> <li>\u2705 Evaluate &amp; Approve \u2013 Standardized and customizable testing protocols, dashboards with human-in-the-loop testing, and approval tracking.</li> <li>\u2705 Deploy &amp; Monitor \u2013 Direct-to-production deployment with ongoing monitoring.</li> </ul>"},{"location":"#key-pillars-of-ggx-platform","title":"Key Pillars of GGX Platform:","text":""},{"location":"#1-centralized-governed-platform","title":"1. Centralized Governed Platform","text":"<ul> <li>Organized GenAI Studio for registering, evaluating, and governing LLM pipelines and their components like RAG, LLMs, Prompts</li> <li>Version tracking with comprehensive audit and governance capabilities</li> <li>Flexibility to recreate production pipelines for iterative testing and updates</li> <li>Automated approval workflows for direct-to-production deployment</li> <li>Built-in Role Governance for proper approval, access, and monitoring rights</li> <li>Strong change management with automated documentation of all the modifications done to an object</li> <li>Automation of end-to-end testing and CI/CD deployment for rapid iteration with control</li> </ul>"},{"location":"#2-standardized-mrmfl-tests","title":"2. Standardized MRM/FL Tests","text":"<ul> <li>Curated datasets and evaluation reports to identify and mitigate risks like toxicity and bias</li> <li>Controlled dashboards for Model Risk Management (MRM) and Fair Lending (FL)</li> <li>Human Integrated Testing (HIT) for real-time interaction and validation</li> <li>Annotation Queues for labeling and performance tracking of production data</li> </ul>"},{"location":"#3-easy-ecosystem-connect-via-apis","title":"3. Easy Ecosystem Connect via APIs","text":"<ul> <li>Seamless connectivity to foundational models (e.g., OpenAI's GPT, Google Gemini, Claude)</li> <li>API integration with banking applications for Retrieval-Augmented Generation (RAG), Models etc.</li> <li>Monitoring of conversation logs with automated dashboards for transparency and compliance</li> <li>Export artifact directly to the production system</li> </ul>"},{"location":"deploy-and-monitor/","title":"Deployment and Monitoring","text":"<p>Once a pipeline is registered on the GGX platform, it can be evaluated and approved within the system. After approval, the locked pipeline artifact can be exported directly to production.</p> <p>For a pipeline in production, monitoring is essential to maintaining the reliability, performance, and accuracy of systems in real-world scenarios. The platform automates production monitoring by ingesting data from relevant systems, generating performance metrics, providing intuitive monitoring dashboards and alerting capabilities. Additionally, it offers Annotation Queues, enabling human reviewers to evaluate and label production data, with automated dashboards for key insights and statistics.</p>"},{"location":"deploy-and-monitor/annotation-queues/","title":"Annotation Queues","text":"<p>It is important to add a Human element when monitoring activity in Production as not all trends can be caught in an automated way. New trends are not always caught using programmatic approaches, but programmatic approaches are useful for repetitive tasks. Bringing a balance of both evaluation methods is key to maintaining a good healthy production system.</p> <p>The Annotation Queue capability is designed to bring in the human evaluation and labelling of production data - but also do it in an efficient way to reduce human error. This capability allows the validation of model outputs in real time by having annotators assess and label the production outcome. The capability can help track a variety of metrics, both standard and custom, on raw and annotated production data, organized digitally for the stakeholders.</p> <p>The capability significantly improves the usual approach to annotating production data using Excel and Google Sheets, creating a more transparent and auditable system to monitor live performance.</p> <p></p>"},{"location":"deploy-and-monitor/annotation-queues/#maintaining-annotation-queues-on-the-platform","title":"Maintaining Annotation Queues on the Platform:","text":"<p>The Annotation Queues module organizes all the existing queues, which are available in the top right dropdown under \"Select an annotation queue\", for easier tracking, searching, and creating new ones.</p>"},{"location":"deploy-and-monitor/annotation-queues/#annotation-queue-registration","title":"Annotation Queue Registration:","text":"<ol> <li>Click on Select an Annotation Queue dropdown in the Annotation Queue module and click on + Create New Annotation.</li> <li>Fill in important details like Name, Description.</li> <li>Upload data files to begin annotation or connect to production tables.</li> <li>Click on the Save button to register the Annotation Queue. Once registered, it will be available in the dropdown to be chosen for annotation work.</li> <li>Select the registered Annotation Queue, and it should open up the details page.</li> </ol> <p>Note:</p> <ul> <li>Multiple files can be added, and the platform will concatenate them automatically as long as the schema matches.</li> <li>The object can be edited to map the Input, Output, and Date Columns.</li> <li>The Date column should be in the format (mm/dd/yyyy).</li> <li>Platform maintains \"Is Accurate\", \"Notes\" and \"Ground Truth\" columns which can be used for labelling.</li> <li>Data will be deduplicated for the labelling process using similar Input and Output values.</li> <li>For new data uploads, \"Is Accurate\", \"Notes\" and \"Ground Truth\" will be filled using the last labelled matching sample.</li> </ul> <p>Once the registration is complete, the data is ready for annotation:</p> <ul> <li>View or export the raw data from the Data Tab.</li> <li>Start data annotation in the Labeling Tab.</li> <li>View standardized and custom reports and metrics in the Statistics Tab.</li> <li>Lastly, add guides and notes for best practices in the Instruction Tab.</li> </ul>"},{"location":"deploy-and-monitor/annotation-queues/#inviting-an-annotator","title":"Inviting an Annotator:","text":"<p>All users onboarded on the platform with access to the Annotation Queues module can collaborate on labeling. The platform tracks all label changes, allows annotators to leave notes, and provides keyboard shortcuts for faster annotations.</p>"},{"location":"deploy-and-monitor/annotation-queues/#benefits-of-annotation-queues","title":"Benefits of Annotation Queues:","text":"<ul> <li>Easy onboarding of data annotated in an external environment.</li> <li>Easy to follow and intuitive user-interface to annotate data.</li> <li>Auditable annotations, platform records all modifications to the data and labels.</li> <li>Automatic ingestion of production data.</li> <li>Enhanced Collaboration with other reviewers.</li> <li>Automated Performance and Progress tracking.</li> <li>Smart algorithms to expedite the labeling process.</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/","title":"CICD & Direct to Production","text":"<p>Typically, production execution environments are kept separate and managed to ensure 100% uptime as these are mission-critical systems for organizations.</p> <p>To tackle the air gap that these systems require - all analytics registered on the platform can be exported out from the system to be put into a Production Execution Environment. These production artifacts are locked to ensure they are not tampered with when they are promoted to production. And there is NO extra dependency required on the production side from Corridor - i.e. there is no requirement for a license key or a corridor installation in production!</p> <p>The production artifact aims to:</p> <ul> <li>Extract the logic/items registered in Corridor - to then use it outside Corridor</li> <li>Self-sufficient with all information encapsulated in the artifact</li> <li>Have minimal dependencies on the runtime-environment where the artifact is run later</li> </ul> <p>Typically a robust Continuous Deployment system is recommended to ensure that Governance is maintained while the production-artifacts are promoted. For example, you can use:</p> <ul> <li>Jenkins</li> <li>GitHub Actions</li> <li>AWS Code Pipelines</li> <li>Azure DevOps Pipelines</li> <li>Gitlab CICD</li> <li>CircleCI</li> </ul> <p>And many others.</p>"},{"location":"deploy-and-monitor/direct-to-production/#deploying-as-apis","title":"Deploying as APIs","text":"<p>If the production system supports calling APIs - the production-artifact can also be wrapped using an API layer and exposed as a REST or SOAP API which can be called by the production system.</p> <p>Typically the APIs should be considered state-less and any extra state management should be handled outside the API, but can be provided in the request payload of the API.</p> <p>Production Artifacts can be deployed as APIs using containerized solutions like:</p> <ul> <li>Docker Compose</li> <li>Kubernetes</li> <li>AWS Elastic Container Service (ECS)</li> <li>AWS Elastic Kubernetes Service (EKS)</li> <li>AWS Fargate</li> <li>Pivotal Cloud Foundry</li> <li>Azure Container Instances</li> <li>Google Cloud Run</li> <li>Google Kubernetes Engine (GKE)</li> <li>VMWare TKGI</li> </ul> <p>The API can also be deployed using application management solutions (server-based or serverless) like:</p> <ul> <li>AWS Beanstalk</li> <li>AWS Lambda</li> <li>Google App Engine (GAE)</li> <li>Google Functions</li> <li>Azure App Service</li> <li>Azure Functions</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/#custom-production-setup","title":"Custom Production Setup","text":"<p>Not all production systems support running direct Python scripts or calling APIs - and some require providing specific GenAI components in a custom interface. To handle cases like this, while Robotic Process Automation (RPA) could be used - many times it is not worth the trouble that RPA brings with it.</p> <p>Because of the transparency that Corridor's Inventory management provides, each part of the pipeline can be deployed independently. For example:</p> <ul> <li>All the LLM configurations like <code>seed</code>, <code>temperature</code>, <code>top_k</code>, etc. can be extracted from the pipeline</li> <li>The prompt templates can be extracted and provided to the production system directly</li> <li>Knowledge files from RAGs can be locked and sent to production</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/#internals-of-the-production-artifact","title":"Internals of the Production Artifact","text":"<p>The artifact generated from the platform is independent of the platform and can run in an isolated runtime environment or production environment. The information stored in the artifact is useful in many cases, where we might want to:</p> <ul> <li>check the metadata for the objects</li> <li>check the input tables and columns used</li> <li>see the lineage and relationship between the objects</li> <li>run the entire artifact or some components of the artifact to get complete or intermediate results</li> </ul> <p>The artifact consists of the following files:</p> <pre><code>model_a.b.c\n\u251c\u2500\u2500 metadata.json\n\u251c\u2500\u2500 input_info.json\n\u251c\u2500\u2500 ... (additional information about features etc. used)\n\u251c\u2500\u2500 python_dict\n|     \u251c\u2500\u2500 __init__.py\n|     \u251c\u2500\u2500 versions.json\n|     \u2514\u2500\u2500 Additional information\n\u2514\u2500\u2500 pyspark_dataframe\n \u251c\u2500\u2500 __init__.py\n \u251c\u2500\u2500 versions.json\n \u2514\u2500\u2500 Additional information\n</code></pre> <p>The metadata.json contains metadata information about the folder it is in. It will have information about the model, its inputs, its dependent variable, etc. It also has any other metadata information registered in the platform like Groups, Permissible Purpose, etc.</p> <p>The versions.json contains the versions of libraries that were used during the artifact creation - python version, any ML libraries, etc.</p> <p>The input_info.json contains the input data tables needed to be sent to the artifact's main() function.</p> <p>The <code>__init__.py</code> file inside pyspark_dataframe and python_dict folders contain the end-to-end Python function which can be used to run the entire artifact. They support different execution engines:</p> <ul> <li>Batch execution with PySpark (pyspark_dataframe)</li> <li>API execution in a Python environment (python_dict)</li> </ul> <p>To run the artifact, simply call the <code>main()</code> function in the artifact with the needed data. The <code>python_dict/__init__.py</code> contains a <code>main()</code> function into which data can be sent - in the form of a python-dict for low-latency execution. A dataset in the dictionary format is described as a dict with type/values</p>"},{"location":"deploy-and-monitor/direct-to-production/#using-corridor-runtime","title":"Using <code>corridor-runtime</code>","text":"<p><code>corridor-runtime</code> is a utility package created by Corridor, which can help us in doing the above-mentioned tasks in a very easy manner, without having to worry about extracting the artifact bundle (<code>bundle.tar.gz</code>) or the files inside the bundle.</p>"},{"location":"deploy-and-monitor/oversight/","title":"Governance Oversight","text":"<p>The Monitoring Dashboard provides users with a comprehensive overview of all registered objects on the platform which helps in providing a clear Oversight of all activities happening. It offers an interface that enables users to access snapshots and trend statistics related to various objects, jobs, and users. The dashboard provides various metadata information such as properties, attributes, and statuses of the registered objects.</p> <p>Monitoring Dashboard is an indispensable tool for review committees and project managers, offering a rich set of features to monitor, analyze, and review all elements registered on the platform efficiently.</p> <p>The \"Monitoring Dashboard\" is accessible in the \"GenAI Studio\" the sub-menu of all modules and is available at various levels (Pipelines, Models, Prompts, RAGs) - and can be used to</p>"},{"location":"deploy-and-monitor/oversight/#base-views","title":"Base Views","text":"<p>This is the default view that the organization decides to show to all users of the platform. Typically the primary cockpit is to quickly find the overall status of governance across all objects on the Platform.</p> <p>By default, the baseview contains examples of monitoring reports that can help better understand governance activities. They can be adopted or swapped with custom views that better fit the organization's interest.</p> <ul> <li> <p>Approval Status View: It presents an overview of the approval status of different objects grouped under their respective Object Groups.</p> </li> <li> <p>Review History View: The columns show the distribution of reviews based on their history, with time intervals.</p> </li> <li> <p>Last Review Status View: It focuses on the most recent review status of objects, organized by Object Groups. The columns display different review statuses, including Accepted with Flag, Accepted without Flag, Pending Acceptance, and Rejected.</p> </li> <li> <p>Schedule Review Status View: The \"Schedule Review Status View\" offers insights into the scheduled review periods for objects within Object Groups.</p> </li> </ul> <p>Explore these pre-configured views and customize them further to cater to specific monitoring needs and gain deeper insights into the platform's governance.</p>"},{"location":"deploy-and-monitor/oversight/#data-view","title":"Data View","text":"<p>This view presents the complete data for the selected object type in a tabular format. Users can easily navigate to specific objects by clicking on the rows. Additionally, they can apply filters or sort rows based on any specified column.</p> <p>The data view is a comprehensive place to access all information across the entire platform - and is the base for nearly all other types of monitoring - be it creating Custom Views or creating automated Alerts.</p>"},{"location":"deploy-and-monitor/oversight/#automated-alerts","title":"Automated Alerts","text":"<p>Alerts are defined as a set of rules that are designed to identify specific items or events that require immediate attention or further action. These rules are created based on predefined criteria, enabling the system to detect critical situations, anomalies, or deviations from expected behaviour. When the conditions specified in the alert rules are met, the system triggers events such as notifications, emails etc. Ensuring that appropriate actions can be taken promptly to address the identified issues.</p>"},{"location":"deploy-and-monitor/oversight/#creating-alerts","title":"Creating Alerts","text":"<p>On the platform, users have the flexibility to create custom alerts tailored to their specific needs. Custom alerts encompass essential properties, including name, description, conditions, severity, and associated actions.</p> <ul> <li>Click on Settings icon and select Alerts in the dropdown menu option. Now you can view a list of alerts configured for the dashboard.   </li> <li>Click on Create.</li> <li>Define the Alert Name by editing the New Alert header.</li> <li>Severity: Each custom alert can be assigned a severity level, such as High, Medium, or Low. This categorization allows users to prioritize alerts based on their importance and urgency. Different severity levels help stakeholders focus on critical issues</li> <li>Table Name: Select the table name from the dropdown menu.</li> <li>Activate: Click the activate checkbox to mark the alert as active. Activated alerts are evaluated, and the corresponding data is displayed as columns in the Monitoring Dashboard. Muted alerts are temporarily disabled and not evaluated, meaning they won't appear as columns in the dashboard during that period.</li> <li> <p>Fill in the description for the new Custom Alert.</p> </li> <li> <p>Actions   Choose an action type from the \"Type\" dropdown menu to determine the action that will be executed when the alert is triggered. The following actions can be associated with custom alerts:</p> <ul> <li>Send Notification: The system can send notifications to selected users, or user roles informing them about the triggered alert.</li> <li>Add Alert Flag: When an alert condition is met, users have the option to add a predefined flag to the object responsible for the alert. Flags serve as visual indicators to highlight objects that require attention.</li> <li>Create Review: For Approved objects, users can choose to add an ongoing review to the object's responsibility and assign reviewers. This action facilitates a thorough review process for objects flagged by the custom alert.</li> <li>Send Email: Users can configure the system to send email notifications to external users when an alert is triggered. The custom field associated with the objects should contain a string of comma-separated email addresses for users who should receive these emails.</li> </ul> </li> </ul> <p>Note</p> <p>Multiple actions can be triggered based on an alert.</p> <ul> <li>Conditions: Define the conditions that need to be met for an alert to be generated. These conditions are specified using rules based on the columns available in the Monitoring Dashboard Data. By utilizing data from the dashboard, users can set up criteria that trigger the alert when specific thresholds or patterns are detected.</li> </ul> <p></p> <ul> <li>Click on Create to register the alert. A pop-up toast message will be displayed with the text reading Alert Created Successfully.</li> </ul>"},{"location":"deploy-and-monitor/performance/","title":"Performance Tracking","text":""},{"location":"deploy-and-monitor/performance/#overview","title":"Overview","text":"<p>The Metrics Dashboard is a crucial component of the Monitoring Dashboard, providing users with valuable insights into the performance metrics of various objects on the platform.</p>"},{"location":"deploy-and-monitor/performance/#populating-metrics-dashboard-with-data","title":"Populating Metrics Dashboard With Data","text":"<ul> <li>To populate the Metrics Dashboard with recurring simulations for objects on the platform, users need to follow these steps:</li> <li>Go to the approved Object's page and navigate to the Jobs tab.</li> <li> <p>For each row in the Jobs table, an action link labelled \"Select for Metrics MD\" will be visible under specific conditions:</p> </li> <li> <p>The job must be the Iteration0 of a recurring job (only iteration 0 has this button)</p> </li> <li>There should be at least two iterations left to be completed in the recurring job</li> <li>The job can be of any type (Simulation, Comparison, Validation, etc.)</li> <li>The job should not be marked as \"old\"</li> </ul> <p>Note: Users are allowed to select jobs run by other individuals, enabling stakeholders to leverage this capability as needed.</p>"},{"location":"deploy-and-monitor/performance/#unselecting-a-job","title":"Unselecting a Job","text":"<ul> <li> <p>To unselect a previously selected job, follow these steps:</p> </li> <li> <p>Go to the approved Object's page and access the Jobs Tab</p> </li> <li>Find the Iteration0 of the recurring job that was previously selected and click on the action link \"Unselect Job for MD.\"</li> <li>Confirmation popup will appear. Upon confirmation, the job will no longer be tracked in the Metrics Dashboard.</li> </ul>"},{"location":"deploy-and-monitor/performance/#selecting-another-job-when-a-job-was-previously-selected","title":"Selecting Another Job (When a Job Was Previously Selected)","text":"<ul> <li>To select another job when a job was previously chosen, follow these steps:</li> <li>Go to the approved Object's page and access the Jobs Tab.</li> <li> <p>For each row in the Jobs tab, observe the following conditions:</p> </li> <li> <p>If there is no action link for MD, the job is not eligible based on the criteria mentioned earlier</p> </li> <li>If the action link \"Select for Metrics MD\" is shown, the job is eligible for selection</li> <li> <p>If the action link \"Unselect Job for MD\" shows, the currently selected row corresponds to the job previously chosen.</p> </li> <li> <p>Click on \"Select for Metrics MD\" for the job you wish to select.</p> </li> <li>Upon confirmation, the newly selected job will be used to track the object in the Metrics Dashboard.</li> </ul>"},{"location":"deploy-and-monitor/performance/#tracking-metrics-and-handling-job-iterations","title":"Tracking Metrics and Handling Job Iterations","text":"<ul> <li>Users can continue tracking metrics with recurring simulations. When job iterations end:</li> <li>Before job completion, notifications/emails can be sent to warn that there is only one iteration left.</li> <li>Upon job tracking completion, additional notifications/emails can be sent.</li> <li> <p>After job completion:</p> </li> <li> <p>If a user selects a new recurring job, it will be used from the time the new job was selected</p> </li> <li>If a user unselects the current recurring job, it will be stopped from the time the job was selected</li> <li>If a user takes no action, the last iteration will continue to be shown in the Metrics Dashboard.</li> </ul>"},{"location":"deploy-and-monitor/performance/#metrics-display-and-thresholds","title":"Metrics Display and Thresholds","text":"<ul> <li> <p>The Metrics Dashboard will exclusively showcase the latest completed job selected by the user.</p> </li> <li> <p>There won't be a default base view, ensuring that the dashboard remains streamlined and focused on user preferences.</p> </li> <li> <p>Only the metrics registered through the UI for the specific object will be presented.</p> </li> </ul> <p>Note: The thresholds utilized on the Job page during individual simulations will not be visible within the Metrics Dashboard.</p>"},{"location":"deploy-and-monitor/performance/#data-view","title":"Data View","text":"<p>This view presents the complete data for the selected object type in a tabular format. Users can easily navigate to specific objects by clicking on the rows. Additionally, they can apply filters or sort rows based on any specified column.</p> <p>The data view is a comprehensive place to access all information across the entire platform - and is the base for nearly all other types of monitoring - be it creating Custom Views or creating automated Alerts.</p>"},{"location":"deploy-and-monitor/performance/#automated-alerts","title":"Automated Alerts","text":"<p>Alerts are defined as a set of rules that are designed to identify specific items or events that require immediate attention or further action. These rules are created based on predefined criteria, enabling the system to detect critical situations, anomalies, or deviations from expected behaviour. When the conditions specified in the alert rules are met, the system triggers events such as notifications, emails, etc., ensuring that appropriate actions can be taken promptly to address the identified issues.</p>"},{"location":"deploy-and-monitor/performance/#creating-alerts","title":"Creating Alerts","text":"<p>On the platform, users have the flexibility to create custom alerts tailored to their specific needs. Custom alerts encompass essential properties, including name, description, conditions, severity, and associated actions.</p> <ul> <li>Click on Settings icon and select Alerts in the dropdown menu option. Now you can view a list of alerts configured for the dashboard.</li> <li>Click on Create.</li> <li>Define the Alert Name by editing the New Alert header.</li> <li>Severity: Each custom alert can be assigned a severity level, such as High, Medium, or Low. This categorization allows users to prioritize alerts based on their importance and urgency. Different severity levels help stakeholders focus on critical issues</li> <li>Table Name: Select the table name from the dropdown menu.</li> <li>Activate: Click the activate checkbox to mark the alert as active. Activated alerts are evaluated, and the corresponding data is displayed as columns in the Monitoring Dashboard. Muted alerts are temporarily disabled and not evaluated, meaning they won't appear as columns in the dashboard during that period.</li> <li> <p>Fill in the description for the new Custom Alert.</p> </li> <li> <p>Actions   Choose an action type from the \"Type\" dropdown menu to determine the action that will be executed when the alert is triggered. The following actions can be associated with custom alerts:</p> </li> <li> <p>Send Notification: The system can send notifications to selected users, or user roles informing them about the triggered alert.</p> </li> <li>Add Alert Flag: When an alert condition is met, users have the option to add a predefined flag to the object responsible for the alert. Flags serve as visual indicators to highlight objects that require attention.</li> <li>Create Review: For Approved objects, users can choose to add an ongoing review to the object's responsibility and assign reviewers. This action facilitates a thorough review process for objects flagged by the custom alert.</li> <li>Send Email: Users can configure the system to send email notifications to external users when an alert is triggered. The custom field associated with the objects should contain a string of comma-separated email addresses for users who should receive these emails.</li> </ul> <p>Note</p> <p>Multiple actions can be triggered based on an alert.</p> <ul> <li> <p>Conditions: Define the conditions that need to be met for an alert to be generated. These conditions are specified using rules based on the columns available in the Monitoring Dashboard Data. By utilizing data from the dashboard, users can set up criteria that trigger the alert when specific thresholds or patterns are detected.</p> </li> <li> <p>Click on Create to register the alert. A pop-up toast message will be displayed with the text reading Alert Created Successfully.</p> </li> </ul>"},{"location":"evaluate-and-approve/","title":"Evaluations and Approval","text":""},{"location":"evaluate-and-approve/#purpose-of-evaluations","title":"Purpose of Evaluations","text":"<p>Evaluating GenAI pipelines is key to making sure they generate reliable, high-quality responses. Without a solid evaluation process, it is hard to tell if changes\u2014like tweaking prompts, removing LLMs, adjusting parameters, or refining retrieval steps\u2014are actually improving performance or breaking something. By measuring factors like relevance, hallucination rates, and latency, teams can make informed decisions about how to optimize their pipelines. Integrating evaluations into CI/CD pipelines ensures that every update is tested, so performance stays consistent and issues are caught early.</p> <p>The quality of an evaluation depends on having a well-rounded dataset and metrics. If test cases are too limited, models might appear to perform well but fail in real-world scenarios. A diverse dataset ensures that evaluation metrics truly reflect how the model will behave in production. At the end of the day, evaluations help build trust, keeping LLM applications accurate, scalable, and dependable across different use cases.</p>"},{"location":"evaluate-and-approve/#choosing-the-right-evaluation-method","title":"Choosing the Right Evaluation Method","text":"<p>There are two main types of evaluators for assessing LLM performance: automated evaluation (using LLMs or code) and human annotations. Each method serves a different purpose depending on the type of assessment needed.</p> Method How it Works Best For Automated (LLM or Code-Based) Uses an LLM to evaluate another LLM\u2019s output or code to measure accuracy, performance, or behavior. - Fast, scalable qualitative evaluation.  - Reducing cost and latency.  - Automating evaluations with hard-coded criteria (e.g., code generation). Human Annotations Experts manually review and label LLM outputs. - Evaluating automated evaluation methods.  - Applying subject matter expertise for nuanced assessments.  - Providing real-world application feedback. <p>Automated evaluation is efficient for objective assessments and large-scale testing, while human annotations provide deeper insights at a higher cost. Combining both methods can ensure a balanced and reliable evaluation process.</p>"},{"location":"evaluate-and-approve/#genai-evaluation-framework","title":"GenAi Evaluation Framework","text":"<p>Gen AI pipelines can introduce significant risks, making a robust evaluation framework essential for comprehensive testing and validation. Follow these structured steps to ensure effective evaluation:</p> Step Description Identify Risks &amp; Define Evaluation Scope - Assess potential risks across the pipeline.  - Create a comprehensive list of necessary evaluations. Define Evaluation Metrics - Align metrics with business objectives, MRM, and FL requirements.  - Implement custom metrics as needed.  - Use GGX evaluation reports curated by GenAI and risk experts. Prepare Evaluation Datasets - Ensure datasets accurately represent the use case.  - Cover all critical business scenarios for thorough validation.  - Utilize GGX datasets curated by experts to evaluate risks. Run Evaluations - Use standard/custom reports and dashboards for structured testing.  - Interpret evaluation results to refine and enhance the pipeline. Compare with Challengers - Establish alternative components and pipelines for comparison. <p>By following these steps, teams can systematically evaluate their Gen AI pipelines, mitigate risks, and enhance performance with data-driven insights. Corridor provides the ability to run evaluation jobs for registered GenAI components.</p>"},{"location":"evaluate-and-approve/#introduction-to-jobs","title":"Introduction to Jobs","text":"<p>Corridor provides the ability to perform evaluations by running jobs on the registered objects and generating standardised and customized reports/metrics. Evaluations are controlled by Corridor and run in a dedicated locked environment to ensure reproducibility of results.</p> <p>The platform supports batch evaluation of objects through simulation jobs or comparisons with similar objects on given datasets. Reports and metrics for these evaluations can be customized within Corridor under Resources \u2192 Reports Section. For more details, refer to the Reporting section.</p> <p>Once the job is completed, Corridor records all the details about the specific steps of the jobs, logs resource usages and publishes the dashboard containing all the selected reports which can be shared across the team on Corridor for feedback and approval process. The results can be exported outside Corridor or used for automated documentation.</p>"},{"location":"evaluate-and-approve/#approvals-post-evaluations","title":"Approvals post Evaluations","text":"<p>The approval process is a key governance capability of the platform. After the object is fully evaluated using automated dashboard or manual tests, all the evaluation results can be shared with predefined roles within an Approval Workflow, ensuring structured reviews, feedback, and approvals for production use.</p> <p>Once approved, the object is locked, preventing any modifications within the system. This guarantees the artifact remains unchanged before being exported directly to the production system.</p>"},{"location":"evaluate-and-approve/#standardized-ggx-reports","title":"Standardized GGX Reports","text":"<p>Apart from the ability to create customized reports, Corridor already has a suite of tests registered for evaluation and validation of different components of Corridor. GGX Reports are crafted by a team of generative AI risk experts following thorough research and analysis. The list of reports is expanding with all the latest developments in the GenAI industry. All these reports can be used if applicable and can be maintained or forked for a specific use case.</p> Component Test Name Risk Attribute Description Type Classification Response Pipeline Accuracy Inaccurate Classification or Response Evaluate ability to correctly classify utterances or provide accurate responses to user queries. MRM Yes Yes Pipeline Stability Repeated Utterances Output Variability Evaluate the ability to produce the same classification or nearly similar responses for repetitions of the same utterance. MRM Yes Yes Pipeline Stability Perturbed Utterances Output Variability Evaluate the ability to produce the same classification or similar responses across minor variations of an utterance (e.g., synonyms, grammatical mistakes). MRM Yes Yes Pipeline Bias (Comparative Prompt Analysis) Implicit / Explicit Bias Evaluate bias in outputs (e.g., classification accuracy or response accuracy) based on inferred segments for gender, race, and age. Fair Lending Yes Yes Pipeline Vulnerability Prompt Injection &amp; Prompt Leakage Evaluate the LLM pipeline\u2019s resilience against jailbreak methods for out-of-context or ambiguous inputs. MRM / Fair Lending No Yes Pipeline Toxicity Hate Speech, Toxic Language, Sarcasm Evaluate the LLM\u2019s avoidance of generating or repeating toxic language (e.g., inappropriate, offensive, or harmful language). MRM / Fair Lending No Yes Pipeline Faithfulness Hallucination, Inaccurate Facts Evaluate adherence to provided context without hallucination. MRM / Fair Lending No Yes Prompt Prompt Trust Score Prompt Quality, Prompt Vulnerability and Leakage Evaluate the prompt quality in different dimensions like Grammar, Logical Coherence, Toxicity, Bias, etc., and generate a Trust Score. MRM / Fair Lending N/A N/A Prompt Prompt Classification Accuracy Inaccurate Classification Evaluate the classification accuracy of the prompt on sample data to help in the hill-climbing process. MRM / Fair Lending Yes No LLM Vocabulary Understanding Misinterpretation, Lack of Context Awareness Determine which LLM is best at defining financial services-specific terminology across seven context categories. MRM / Fair Lending N/A N/A LLM Subject Understanding Context Misalignment, Incorrect Reasoning General subject understanding across Math, Science, etc. MRM / Fair Lending N/A N/A LLM Reasoning Capability Logical Fallacies, Incorrect Inferences Assess LLM's capabilities such as complex reasoning, knowledge utilization, language generation, etc. MRM / Fair Lending N/A N/A LLM Toxicity Understanding Failure to Detect Harmful Requests Evaluate a model's ability to identify and classify text statements that could be considered toxic across six toxicity labels. MRM / Fair Lending N/A N/A LLM Toxicity Evaluation Hate Speech, Toxic Language, Sarcasm Evaluating the tendency of LLM generating toxic replies. MRM / Fair Lending N/A N/A LLM Dialect Bias Underrepresentation of Linguistic Variants Assess which LLM responds most consistently to general queries phrased in different language dialects. MRM / Fair Lending N/A N/A LLM Gender Bias with Income as Proxy Fair Lending Risk Evaluation focuses on understanding if there is any systematic bias in assigning job titles to different genders and profiles. MRM / Fair Lending N/A N/A LLM Model Latency Scalability Issues, User Frustration Determine which LLM has the best response latency performance when varying prompt and response length. Business / Tech N/A N/A RAG Retrieval Accuracy Incorrect context retrieval, hallucination Accuracy of the retrieved documents by the RAG system. MRM N/A N/A RAG Knowledge Evaluation Lack of Coverage Coverage of different scenarios and business flows in Knowledge Data. Business N/A N/A RAG Validation Data Evaluation Lack of Coverage, Lack of Potential Scenarios Coverage of different scenarios and business flows in Evaluation Data. MRM N/A N/A"},{"location":"evaluate-and-approve/approval-workflows/","title":"Approval Workflows","text":""},{"location":"evaluate-and-approve/approval-workflows/#what-is-approval-workflow","title":"What is Approval Workflow?","text":"<p>After the development of GenAI pipelines, it is important to have a set of review processes to ensure various committees can collectively review and approve the pipeline. In Corridor, workflows that comprise multiple responsibilities can be created - and every responsibility is comprised of one or more reviewers. An object goes through the approval process from each of the responsibilities and reviewers.</p>"},{"location":"evaluate-and-approve/approval-workflows/#why-it-is-important","title":"Why it is important?","text":"<ul> <li>Creates an approval trail for model updates and changes.</li> <li>Provides clear documentation of who approved what and why.</li> <li>Ensures that only validated versions of models are deployed.</li> <li>Involves key stakeholders (developers, legal, compliance, business teams) in decision-making.</li> <li>Establishes clear ownership over AI pipeline governance.</li> </ul>"},{"location":"evaluate-and-approve/approval-workflows/#registering-approval-workflows","title":"Registering Approval Workflows:","text":"<p>Approval workflows can be created and edited within the Settings section by anyone with the right authority level (mainly Admin and Master roles). Approval workflows are specific to object types.</p> <ol> <li>Go to Settings and click on Approval Workflow Tab.</li> <li>On the listing page click on the Create button to create a new one.</li> <li>Fill in important details like Name, Attributes (Object Types, Description and Status).</li> <li>Click on the Create button at the end to register the Approval Workflow.</li> <li>Once registered the Approval Workflow can be edited to add responsibilities in the Responsibilities tab by clicking on the Add New Responsibility button.</li> <li>Decide on Veto and Editing power to reviewers.</li> <li>Select all the reviewers for the responsibility.</li> <li>Add more responsibilities if required and Save to finally register the Approval Workflow.</li> </ol> <p>Note: The external tools option is available when third-party tools are configured and allows a third-party application to be defined as a responsibility within an approval workflow.</p> <p>Once registered the Approval Workflow can be chosen while registering any object. This would ensure that the model goes through a proper approval process from all the responsibilities and reviewers before getting approved for production usage.</p> <p></p>"},{"location":"evaluate-and-approve/approval-workflows/#keeping-track-of-findingslimitations","title":"Keeping track of Findings/Limitations","text":"<p>Once an object is approved, it is locked. No further changes can be made to it. But sometimes, reviews are done with findings or limitations on the usage and need follow-ups.</p> <p>Corridor provides a flagging capability that can be used to Flag an Object (Even Post Approval) and can be created by anyone with Write access to Settings. An object can be flagged for any reason (e.g. <code>NeedShadowResultsFor2Months</code> or <code>NeedRetrainIn6Months</code>) by any member of any Workflow that oversees objects of the same type. Similarly, a flag can be dropped (i.e., deactivated) by anyone with the authority to add the flag.</p> <p>Once the flag is activated, a warning flag appears beside the object's name on the details page, indicating that the object and its assigned flags should be carefully reviewed before being used in downstream applications.</p>"},{"location":"evaluate-and-approve/comparison/","title":"Comparisons","text":"<p>Platform provides the ability to compare the registered objects for a specific task using standard and customized metrics. Using this comparison capability one can quickly evaluate a list of candidates to select the best one.</p> <p>A comparison task typically involves:</p> <ul> <li>Current object: Object which is currently selected and needs to be compared with others.</li> <li>Challenger objects: Challenger objects are objects of the same type (e.g., models can be   compared with other models, prompts can be compared with prompts, etc.).</li> <li>Data Source: A common data source on which objects would be evaluated.</li> <li>Report &amp; Metrices: Exact evaluation metrics to be compared.</li> </ul> <p>Note: On the platform one can quickly create Copy of current objects and change the definitions, swap in swap out components (Like Models, Prompts, Processing etc.) to create challengers.</p>"},{"location":"evaluate-and-approve/comparison/#how-to-run-a-comparison-task","title":"How to run a Comparison Task?","text":"<ul> <li>Register the object and its challenger versions on the platform.</li> <li>Go to the Details page of an object to be compared and click on the Run -&gt; Comparison button.</li> <li>Provide description about the comparison run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Select challenger objects which need to be compared in Dependencies Section.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li> <p>Click on Run at the bottom and wait for job completion.</p> <ul> <li>Once a job has been submitted it starts in the NEW status</li> <li>The job will go through the following statuses: COMPILING &gt; QUEUED &gt; RUNNING and finally stop at COMPLETED of FAILED</li> </ul> </li> </ul> <p>All comparison tasks are systematically recorded on the platform and displayed on the Jobs page of an object in a structured format. They can also be exported as part of the automated documentation process.</p> <p>Note: The platform allows customizing reports and dashboards specifically for comparison tasks. Note: Corridor allows running jobs in parallel and multiple threads at a time within a job to expedite the evaluation process.</p>"},{"location":"evaluate-and-approve/document-generation/","title":"Document Generation","text":"<p>As Corridor starts to understand the kind of pipeline being created, what models, prompts, etc. it is using and also starts to understand the evaluations being run on the pipeline. It becomes a great central knowledge base of all work done throughout the GenAI lifecycle.</p> <p>Corridor can export all this information into various formats (like Word or PDF) to make it easy to submit documentation and audit reports external to the system.</p> <ul> <li>Automated Documentation are generated for the Prompts, Models, Pipelines in their respective Registry</li> <li>Automated Documentation for evaluations is available on the Job Details tab.</li> </ul> <p>Simply click on the Export button and select the required format for the document to be downloaded</p>"},{"location":"evaluate-and-approve/human-testing/","title":"Human Integrated Testing","text":"<p>The Human Integrated Testing module enables comprehensive testing of GenAI pipelines both before approval (Pre-Approval) and after deployment (Post-Approval/Post-Deployment), involving humans in the loop.</p>"},{"location":"evaluate-and-approve/human-testing/#what-is-a-pre-approval-testing","title":"What is a Pre-Approval Testing?","text":"<p>The Pre-Approval Testing allows users to test the full end-to-end pipeline in a production-like environment, providing an opportunity to manually validate the final solution before deployment. It simulates real-world scenarios by replicating end-user experiences. This module abstracts the internal technical details, presenting only the final inputs and outputs for a streamlined evaluation process.</p> <p>Note: Currently, only the chat-based pipelines registered on the platform are available for manual testing.</p> <p>It enables reviewers to capture their feedback and scores during testing, offering valuable insights for the development team to improve the solution. Additionally, all testing data can be exported from the platform for different purposes, making it easier for developers and reviewers to collaborate.</p>"},{"location":"evaluate-and-approve/human-testing/#performing-pre-approval-testing-on-the-platform","title":"Performing Pre-Approval Testing on the Platform:","text":"<p>All the chat-based pipelines are available for Pre-Approval Testing.</p> <ol> <li>Click on Start Session in the Pre-Approval Testing module.</li> <li>Choose the pipeline to be tested from a list of registered pipelines.</li> <li>Once the pipeline is selected, a new window will open where you can start interacting.    </li> <li>Start interacting in the chat window. Optionally, select a Persona to simulate real-life scenarios, or begin chatting directly.    </li> <li>Provide feedback: Use \ud83d\udc4d if satisfactory, \ud83d\udc4e if not; reasons/comments are prompted for \ud83d\udc4e.</li> <li>Once testing is completed, an experience summary can be recorded by providing an overall session rating and testing notes.    </li> <li>You can download the transcript of the session for external documentation or further analysis.</li> </ol> <p>Note:</p> <p>More information about the Testing Session and GenAI Pipeline can be seen in the information panel on the left. There are 4 buttons in the information panel:</p> <ul> <li>View other sessions from the same pipeline.</li> <li>View the context being used in the pipeline. This is additional information that will be utilized by LLM to answer user questions. For example, it can be customer metadata being pulled from some knowledge base.</li> <li>View more details about the pipeline and the Gen AI assets used in this pipeline.</li> <li>View any configurations provided to the pipeline by the Modeler, such as temperature, random seed, etc.   </li> </ul> <p>Every interaction, feedback, and comment in the Test Session is auto-saved immediately and recorded in history and recorded along with report cards in customized groups.</p>"},{"location":"evaluate-and-approve/human-testing/#benefits-of-pre-approval-testing","title":"Benefits of Pre-Approval Testing:","text":"<ul> <li>Human-in-the-loop testing before approving the pipeline for production.</li> <li>Easy to follow and intuitive user interface to test pipelines.</li> <li>Abstraction of complex pipeline logic from MRM/Fair-Lending/Business teams.</li> <li>Record feedback and generate report cards for continuous improvement.</li> <li>Allows Multi-Turn testing for real-world scenarios.</li> <li>Load transcripts from other environments for human scoring.</li> </ul> <p>Similar to this a pipeline can be tested/monitored post approval also. Visit Deploy and Monitor section to know more on this.</p>"},{"location":"evaluate-and-approve/reporting/","title":"Reporting","text":""},{"location":"evaluate-and-approve/reporting/#what-is-a-report","title":"What is a Report?","text":"<p>A report is an analytical entity created to derive insights from a given data source. It presents statistical evaluations and key findings through visual elements such as charts and graphs, supporting stakeholders in making data-driven decisions.</p> <p>A typical report comprises the following components:</p> <ul> <li>Data Source: The foundational datasets from which insights are derived.</li> <li>Computation Logic: The processing and transformations applied to the data, such as statistical calculations, grouping, filtering, and model-based analysis.</li> <li>Visualization Logic: The presentation of raw or computed results using charts, tables, and other visual elements to improve interpretation.</li> </ul> <p>Note: Multiple reports can be structured in a relevant manner to create a dashboard (details below) in GGX.</p> <p></p>"},{"location":"evaluate-and-approve/reporting/#benefits-of-report-registration","title":"Benefits of Report Registration:","text":"<ul> <li>Customize reports to meet MRM, Fair Lending, Business, and Development requirements.</li> <li>Use multiple reports to create use-case-specific dashboards while running jobs.</li> <li>Track all modifications and enable enhanced version management.</li> <li>Enhanced collaboration and approval process with MRM, FL, and other team members.</li> <li>Usage tracking using the Lineage Tracking capability.</li> <li>Reusability across multiple cases reduces the need to go through the approval process again.</li> <li>Allows for quick updates to meet changing needs and business logic.</li> </ul>"},{"location":"evaluate-and-approve/reporting/#managing-reports-on-the-platform","title":"Managing Reports on the Platform:","text":"<p>The Report Registry organizes all reports in a customized manner at a centralized location, allowing easier tracking, monitoring, and custom report creation.</p>"},{"location":"evaluate-and-approve/reporting/#report-registration","title":"Report Registration:","text":"<ol> <li>Go to Resources - Reports and click on the Create button.</li> <li> <p>A new page will appear, requiring the following fields to be filled in:</p> <ul> <li>Name</li> <li>Attributes:<ul> <li>Object Types: The objects for which the report will be used (e.g., Foundation Model, Pipeline, RAG, etc.).</li> </ul> </li> <li>Properties:<ul> <li>Description: Brief description of what the report aims to accomplish.</li> <li>Group: The custom group where the report should be displayed for better organization.</li> <li>Approval Workflow: Predefined approval chain for transitioning from Draft to Approved.</li> </ul> </li> <li>Any other relevant details.</li> </ul> </li> <li> <p>Define the report parameters:</p> <ul> <li>Alias: Python name for the parameter.</li> <li>Type:<ul> <li>String: A string constant (e.g., a specific group).</li> <li>Number: A float constant (e.g., thresholds like 0.5).</li> <li>String Column: A dataset column used for evaluation.</li> <li>Registered Object: An object already registered (e.g., GPT-4 evaluator model).</li> </ul> </li> <li>Is Mandatory: Whether the parameter is required for running the simulation.</li> <li>Description: Short description of the report parameter.</li> </ul> </li> <li> <p>Define the example data to run the report. Example data can be created using registered tables or uploaded files.</p> </li> <li> <p>Write the formula for the report:</p> <ul> <li>Source Data: The data from the job would be present in the form of a dictionary (see below). </li> <li>Select any additional inputs (like Global Functions, Models, etc.) to aid in writing report logic.</li> <li>Report Computation: Filtering, processing, and statistical calculations based on the job data.</li> </ul> </li> </ol> <p>The computation logic can use two default variables (<code>job</code> and <code>data</code>):</p> <pre><code>job: A Corridor Job object containing metadata.\n\njob.current - Access to the current object.\njob.challengers - Dictionary of challengers where the key is object name and value is object (only available when running Comparison jobs).\njob.current.name gives the name of the current object\n\ndata: Dictionary containing job result data.\n\n{'current': pyspark_dataframe} for Simulation jobs.\n{'current': pyspark_dataframe, 'Challenger #1': pyspark_dataframe} for Comparison jobs.\n</code></pre> <ol> <li> <p>Define report visualization logic: This can use the computed results from the report computation and allow building visualizations like tables, charts, etc. The computed results would be present in a variable called <code>raw_output</code></p> <ul> <li>Select any additional inputs (like Global Functions, Models, etc.) to assist in report logic.</li> <li>Use Markdown, Plotly, Seaborn, or Matplotlib to generate figures.</li> <li>Metric outputs should return a dictionary, e.g., <code>{'current': accuracy_value}</code>.</li> <li>Add more such outputs using the Add Output button.</li> </ul> </li> <li> <p>Add any additional notes or attachments.</p> </li> <li>Click on the Create button to finalize the report registration.</li> </ol> <p>Notes:</p> <ul> <li>It is advisable to create a clear sketch of the dashboard in advance and carefully plan the types of charts and visualizations required.</li> <li>Most of the time, the String Column type will be used, as the column would already be present in the dataset being used for simulation. Clearly define the report parameter for the string column.</li> <li>If a report parameter is marked as Non-Mandatory, ensure the report code handles it correctly.</li> <li>For Comparison and Validation jobs, dynamically rendered names are used for challengers and benchmarks dictionary.</li> <li>It is recommended to have an ID column in datasets to merge challenger results with current data.</li> <li>If Figure is selected and a pandas DataFrame is returned from the report output, it is rendered as a Grid Table, enabling sorting and filtering without additional code.</li> </ul> <p>Once the report registration is completed, it can be used while running jobs on the platform. The Dashboard would be created once the job is completed.</p> <p></p>"},{"location":"evaluate-and-approve/simulation/","title":"Simulation","text":"<p>The most common type of execution is a Simulation - used to execute analytics contained in the definition of an object that has been registered in the platform. And additionally, run dashboards and reports on that output.</p> <p>A Simulation task typically involves:</p> <ul> <li>Current object: Object which is currently selected and being tested. This can be a Pipeline, Model, RAG, or Prompt.</li> <li>Data Source: The data to run the object on.</li> <li>Report &amp; Metrices: Exact evaluation metrics to be run on the output.</li> </ul>"},{"location":"evaluate-and-approve/simulation/#how-to-run-a-simulation-task","title":"How to run a Simulation Task?","text":"<ul> <li>Register the object on the platform.</li> <li>Go to the Details page of an object to be compared and click on the Run -&gt; Simulation button.</li> <li>Provide description about the run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li> <p>Click on Run at the bottom and wait for job completion.</p> <ul> <li>Once a job has been submitted it starts in the NEW status</li> <li>The job will go through the following statuses: COMPILING &gt; QUEUED &gt; RUNNING and finally stop at COMPLETED of FAILED</li> </ul> </li> </ul> <p>All simulation tasks are systematically recorded on the platform and displayed on the Jobs page of an object in a structured format. They can also be exported as part of the automated documentation process.</p> <p>Note: The platform allows customizing reports and dashboards specifically for simulation tasks. Note: Corridor allows running jobs in parallel and multiple threads at a time within a job to expedite the evaluation process.</p>"},{"location":"integrations/","title":"Overview","text":""},{"location":"integrations/#ggx-integrations","title":"GGX Integrations","text":"<p>Corridor GGX is built with extensibility in mind and supports a wide range of integrations that allow you to seamlessly plug into existing workflows and tools.</p> <p>These integrations are organized into the following categories:</p> <ol> <li> <p>LLM Providers:    Connect to popular foundational models from leading providers to power your generative experiences.    Examples: OpenAI, Google Vertex AI,Azure AI,Amazon Bedrock, DeepSeek, Anthropic, HuggingFace, Nvidia NIM, GitHub Models</p> </li> <li> <p>Agent Providers &amp; Frameworks:    Leverage pre-built agent providers or bring your own orchestration frameworks to create and manage intelligent, multi-step agent workflows with minimal setup.    Examples: Vertex AI Agent Playbooks, AgentForce (Salesforce), Microsoft Copilot Studio, Vapi AI,GitHub Copilot, Amazon Lex, CustomGPT</p> </li> <li> <p>Report Providers:    Plug in evaluation tools to assess your data, RAGs, pipelines, agents, etc. effectively.    Examples: CleanLabs, Perspective API</p> </li> <li> <p>Voice Providers:    Setup integrations to services that provide speech-to-text, text-to-speech, Voice Agents    Examples: Deepgram</p> </li> <li> <p>Single Sign-On (SSO) Integrations:    Setup integrations to services for seamless Authentication and Authorization    Examples: Google Workspace, Auth0 (by Okta), WorkOS, OneLogin</p> </li> </ol>"},{"location":"integrations/evaluation-providers/cleanlab/","title":"Cleanlab","text":""},{"location":"integrations/evaluation-providers/cleanlab/#about-cleanlab","title":"About Cleanlab","text":"<p>Pioneered at MIT and proven at 50+ Fortune 500 companies, Cleanlab provides software to detect and remediate inaccurate responses from Enterprise AI applications. Cleanlab detection serves as a trust and reliability layer ensuring Agents, RAG, and Chatbots remain safe and helpful.</p> <p>Recognized among the Forbes AI 50, CB Insights GenAI 50, and Analytics India Magazine\u2019s Top AI Hallucination Detection Tools, the company was founded by three MIT computer science PhDs and is backed by $30M investment from Databricks, Menlo, Bain, TQ, and the founders of GitHub, Okta, and Yahoo.</p>"},{"location":"integrations/evaluation-providers/cleanlab/#integrating-cleanlab","title":"Integrating Cleanlab","text":"<p>Simply enter your Cleanlab API key once in the Integrations section of GGX. This enables authorized users to access Cleanlab\u2019s capabilities within the platform. Once integrated, Cleanlab can be used as any other python package on the platform.</p> <pre><code>from cleanlab_tlm import TLM\ntlm = TLM()\ntrustworthiness_score = tlm.get_trustworthiness_score(\"&lt;your prompt&gt;\", response=\"&lt;your response&gt;\")\n</code></pre>"},{"location":"integrations/evaluation-providers/cleanlab/#potential-usage-within-ggx","title":"Potential Usage Within GGX","text":"<p>Cleanlab strengthens GGX by adding a trust and reliability layer that evaluates the accuracy, relevance, and safety of agents and their underlying components. With Cleanlab, one can systematically identify hallucinations, off-topic responses, unsafe outputs, and other critical issues across agents, RAG pipelines, and broader LLM workflows. Cleanlab\u2019s metrics and scores can be used to create insightful reports for validating systems and can also serve as guardrails to ensure your agents respond safely.</p>"},{"location":"integrations/evaluation-providers/cleanlab/#key-use-cases","title":"Key Use Cases","text":"<ul> <li> <p>RAG System Evaluation   Automatically detect inaccuracies, assess retrieval quality, and surface knowledge gaps in RAG pipelines.</p> </li> <li> <p>Agent &amp; Pipeline Response Evaluation   Cleanlab\u2019s TLM (Trustworthiness Language Model) assigns confidence scores to LLM responses, flagging hallucinations, ambiguous answers, and unsafe content with detailed explanations.</p> </li> <li> <p>Data Quality &amp; Reliability   Identify and resolve issues such as mislabeled data, ambiguous examples, or statistical outliers ensuring your models are trained or validated on clean, trustworthy data.</p> </li> </ul>"},{"location":"integrations/evaluation-providers/cleanlab/#example-use-case-enhancing-ivr-system-evaluation-with-cleanlab","title":"Example Use Case: Enhancing IVR System Evaluation with Cleanlab","text":"<p>Cleanlab significantly helps test IVR (Interactive Voice Response) systems by providing trustworthiness scores for various AI-driven processes. It evaluates the reliability of LLM decisions in classifying caller intent and routing calls, providing responses. Furthermore, Cleanlab can assesses the accuracy of human data labels used in validation, helping to pinpoint mislabeled data. For the auto-generated call summaries and candidate responses, Cleanlab's scores can highlight problematic outputs and areas where the agent may need further improvement.</p> <p></p> <p>For example, check below, Cleanlab can score how trustworthy each response is, making it easy to spot which types of questions or categories have issues.</p> <p></p>"},{"location":"integrations/evaluation-providers/cleanlab/#want-to-learn-more","title":"Want to Learn More ?","text":"<ul> <li>Cleanlab Codex Documentation</li> <li>Cleanlab TLM Overview</li> <li>Cleanlab Studio Overview</li> </ul>"},{"location":"integrations/llm-providers/","title":"Setting up Integrations","text":""},{"location":"integrations/llm-providers/#setting-up-integrations","title":"Setting up Integrations","text":"<p>Setup integrations to services that provide LLMs as an API. Configure your API keys once to access multiple AI providers across the platform.</p>"},{"location":"integrations/llm-providers/#getting-started","title":"Getting Started","text":"<p>Navigate to Settings &gt; Platform Integrations to configure your LLM providers. Each provider requires an API key that creates secure environment variables for your models.</p>"},{"location":"integrations/llm-providers/#llm-providers","title":"LLM Providers","text":""},{"location":"integrations/llm-providers/#openai","title":"OpenAI","text":"<p>Models available: o4-mini, o3, o3-mini, etc</p> <ol> <li>Click the OpenAI card</li> <li>Enter your API key from platform.openai.com</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#anthropic","title":"Anthropic","text":"<p>Models available: claude-4-opus, claude-4-sonnet, claude-3.7-sonnet, etc</p> <ol> <li>Click the Anthropic card</li> <li>Enter your API key from console.anthropic.com</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#azure-ai-active","title":"Azure AI (Active)","text":"<p>Models available: o4-mini, o3, o3-mini, etc</p> <ol> <li>Click the Azure AI card</li> <li>Enter your Azure OpenAI API key</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#amazon-bedrock","title":"Amazon Bedrock","text":"<p>Models available: amazon.titan-text-premier-v1:0, amazon.titan-text-express-v1, amazon.titan-text-lite-v1, etc</p> <ol> <li>Click the Amazon Bedrock card</li> <li>Configure AWS credentials:</li> <li>Access Key ID: Your AWS access key</li> <li>Secret Access Key: Your AWS secret key</li> <li>Session Token: Optional temporary session token</li> <li>Default Region: AWS region (e.g., us-east-1)</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#deepseek-active","title":"DeepSeek (Active)","text":"<p>Models available: deepseek-r1, deepseek-v3, deepseek-v2.5</p> <ol> <li>Click the DeepSeek card</li> <li>Get your API key from DeepSeek Platform</li> <li>Enter your API key in the field</li> <li>Click \"Test Connection\" to verify</li> <li>Click \"Save\" to complete setup</li> </ol>"},{"location":"integrations/llm-providers/#google-vertex-ai-active","title":"Google Vertex AI (Active)","text":"<p>Models available: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash, etc</p> <ol> <li>Click the Google Vertex AI card</li> <li>Upload service account JSON key</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#hugging-face-active","title":"Hugging Face (Active)","text":"<p>Models available: llama-3.1-8b-instruct, llama-3.1-70b-instruct, llama-3.1-405b-instruct, etc</p> <ol> <li>Click the Hugging Face card</li> <li>Enter your HF token from huggingface.co/settings/tokens</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#nvidia-nim-active","title":"Nvidia NIM (Active)","text":"<p>Models available: llama-3.1-nemotron-instruct-70b, llama-3.3-nemotron-super-49b-reasoning, llama-3.1-nemotron-ultra-253b-v1-reasoning, etc</p> <ol> <li>Click the Nvidia NIM card</li> <li>Enter your Nvidia API key</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#github-models-active","title":"GitHub Models (Active)","text":"<p>Models available: o4-mini, o3, o3-mini, etc</p> <ol> <li>Click the GitHub Models card</li> <li>Enter your GitHub token</li> <li>Test connection and save</li> </ol>"},{"location":"integrations/llm-providers/#integration-status","title":"Integration Status","text":"<ul> <li>Active: Ready to use in model registration</li> <li>Inactive: Needs configuration or has connection issues</li> </ul>"},{"location":"integrations/llm-providers/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>Each integration creates environment variables that you can use in your model code:</p> <ul> <li>OpenAI: <code>OPENAI_API_KEY</code></li> <li>Anthropic: <code>ANTHROPIC_API_KEY</code></li> <li>Azure AI: <code>AZURE_ENDPOINT</code></li> <li>DeepSeek: <code>DEEPSEEK_API_KEY</code></li> <li>Google Vertex AI: <code>GOOGLE_API_TOKEN</code></li> <li>Hugging Face: <code>HUGGING_FACE_HUB_TOKEN</code></li> <li>GitHub Models: <code>GITHUB_TOKEN</code></li> <li>Amazon Bedrock: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_SESSION_TOKEN</code>, <code>AWS_DEFAULT_REGION</code></li> </ul>"},{"location":"integrations/llm-providers/#using-environment-variables","title":"Using Environment Variables","text":"<p>Once configured, the environment variables are automatically available in your model code:</p> <pre><code>import os\n\n# Access your API keys\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\nanthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\nazure_endpoint = os.getenv(\"AZURE_ENDPOINT\")\ndeepseek_key = os.getenv(\"DEEPSEEK_API_KEY\")\ngoogle_token = os.getenv(\"GOOGLE_API_TOKEN\")\nhf_token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\ngithub_token = os.getenv(\"GITHUB_TOKEN\")\n\n# AWS Bedrock credentials\naws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\naws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\naws_session_token = os.getenv(\"AWS_SESSION_TOKEN\")\naws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n</code></pre>"},{"location":"integrations/llm-providers/#need-help","title":"Need Help?","text":"<ul> <li>Use \"Test Connection\" to verify your API keys</li> <li>Check provider documentation for API key setup</li> <li>Ensure proper permissions for your API keys</li> </ul>"},{"location":"integrations/llm-providers/anthropic/","title":"Anthropic","text":""},{"location":"integrations/llm-providers/anthropic/#anthropic-integration","title":"Anthropic Integration","text":"<p>The Anthropic integration provides access to Claude models through a unified interface. Configure once, use everywhere with enterprise-grade safety features and constitutional AI principles built into every interaction.</p>"},{"location":"integrations/llm-providers/anthropic/#integrating-anthropic","title":"Integrating Anthropic","text":"<p>Simply enter your Anthropic API key once in the Platform Integrations section. This enables authorized users to access Claude models within the platform. Once integrated, models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered Anthropic model\nresult = claude_sonnet_model(text=\"Analyze this document\", max_tokens=1500)\n</code></pre>"},{"location":"integrations/llm-providers/anthropic/#example-of-models-supported","title":"Example of Models Supported","text":"<p>Anthropic provides access to Claude models with different capabilities and performance characteristics:</p> <p>Claude 3.5 Sonnet - Balanced performance for most use cases with strong reasoning Claude 3 Opus - Most capable model for complex tasks requiring deep analysis Claude 3 Haiku - Fast, cost-effective model for simple tasks Claude 3.5 Haiku - Enhanced version with improved speed and capabilities Additional Models - Latest Claude variants with enhanced reasoning capabilities</p>"},{"location":"integrations/llm-providers/anthropic/#registering-a-new-anthropic-model","title":"Registering a New Anthropic Model","text":"<p>Navigate to New Model to begin registration. The registration form connects your Anthropic integration with custom model configurations.</p>"},{"location":"integrations/llm-providers/anthropic/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"Claude 3.5 Sonnet optimized for document analysis and content generation. Use for enterprise content processing with built-in safety features. Ideal for complex reasoning tasks and multi-step analysis.\"</p>"},{"location":"integrations/llm-providers/anthropic/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>claude_sonnet_analyzer</code>, <code>claude_document_processor</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your Anthropic integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"Anthropic\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/anthropic/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for Anthropic models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>max_tokens</code> Numerical Yes 1500 Maximum response length <code>temperature</code> Numerical Yes 0.7 Controls response creativity <code>system_prompt</code> String Yes \"\" System instructions <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/anthropic/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <p>Example implementation for a text analysis model:</p> <pre><code># Arguments: text, max_tokens, temperature are automatically available\nimport os\nimport anthropic\n\n# Direct initialization\nclient = anthropic.Anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n)\n\nif text is None:\n    return None\n\nmessage = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=int(max_tokens),\n    temperature=float(temperature),\n    system=system_prompt if system_prompt else \"You are a helpful AI assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": text}\n    ]\n)\n\nreturn {\"output\": message.content[0].text, \"context\": None}\n</code></pre>"},{"location":"integrations/llm-providers/anthropic/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>Before registering models, configure your Anthropic credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on Anthropic</li> <li>Enter your Anthropic API key</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>ANTHROPIC_API_KEY</code></p>"},{"location":"integrations/llm-providers/anthropic/#example-use-case-document-analysis-model","title":"Example Use Case: Document Analysis Model","text":"<p>An Anthropic Claude model configured for enterprise document analysis demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/anthropic/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>max_tokens</code> (Numerical, optional, default: \"2000\")</li> <li><code>temperature</code> (Numerical, optional, default: \"0.3\")</li> <li><code>system_prompt</code> (String, optional, default: \"You are a helpful document analysis assistant.\")</li> </ul>"},{"location":"integrations/llm-providers/anthropic/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: claude_document_analyzer\nresult = claude_document_analyzer(\n    text=\"Your document text here...\",\n    max_tokens=2000,\n    temperature=0.3,\n    system_prompt=\"Analyze this document and provide key insights with detailed explanations.\"\n)\n</code></pre>"},{"location":"integrations/llm-providers/anthropic/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Review Anthropic documentation</li> <li>Check Claude model capabilities</li> <li>Monitor usage through Anthropic Console</li> <li>Set up rate limiting and cost management</li> </ul>"},{"location":"integrations/llm-providers/aws-bedrock/","title":"AWS Bedrock","text":""},{"location":"integrations/llm-providers/aws-bedrock/#aws-bedrock-integration","title":"AWS Bedrock Integration","text":"<p>The AWS Bedrock integration provides access to foundation models from multiple AI providers through a unified interface. Configure once, use everywhere with enterprise-grade security, scalability, and comprehensive model selection from leading AI companies.</p>"},{"location":"integrations/llm-providers/aws-bedrock/#integrating-aws-bedrock","title":"Integrating AWS Bedrock","text":"<p>Configure your AWS credentials once in the Platform Integrations section. This enables authorized users to access Bedrock foundation models within the platform. Once integrated, models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered Bedrock model\nresult = bedrock_claude_model(text=\"Analyze this data\", temperature=0.8)\n</code></pre>"},{"location":"integrations/llm-providers/aws-bedrock/#example-of-models-supported","title":"Example of Models Supported","text":"<p>AWS Bedrock provides access to foundation models from multiple leading AI providers:</p> <p>Amazon Titan - Amazon's own foundation models for text generation and embeddings Anthropic Claude - Claude 3.7 Sonnet, Claude 3.5 Sonnet, and other Claude variants Meta Llama - Llama 3.2 series with fine-tuning capabilities Cohere Command - Command R/R+ and Embed v3 families for text generation and embeddings AI21 Labs Jamba - Jamba 1.5 series for advanced language processing Stability AI - Stable Diffusion series for image generation Additional Models - +122 models available through Amazon Bedrock Marketplace</p>"},{"location":"integrations/llm-providers/aws-bedrock/#registering-a-new-bedrock-model","title":"Registering a New Bedrock Model","text":"<p>Navigate to New Model to begin registration. The registration form connects your AWS Bedrock integration with custom model configurations.</p>"},{"location":"integrations/llm-providers/aws-bedrock/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"Claude 3.7 Sonnet on AWS Bedrock optimized for enterprise content analysis. Use for document processing with AWS compliance features. Ideal for complex reasoning and multi-step analysis tasks.\"</p>"},{"location":"integrations/llm-providers/aws-bedrock/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>bedrock_claude_analyzer</code>, <code>titan_embedder</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your AWS integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"Amazon Bedrock\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/aws-bedrock/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for Bedrock models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>temperature</code> Numerical Yes 0.7 Controls response creativity <code>max_tokens</code> Numerical Yes 1500 Maximum response length <code>system_prompt</code> String Yes \"\" System instructions <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/aws-bedrock/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <pre><code># Arguments: text, temperature are automatically available\n\nimport os\nimport boto3\n\n# Direct initialization\nclient = boto3.client(\n    service_name=\"bedrock-runtime\",\n    region_name=os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\"),\n    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    aws_session_token=os.getenv(\"AWS_SESSION_TOKEN\")\n)\n\nif text is None:\n    return None\n\n# Prepare the conversation for Claude models\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [{\"text\": text}]\n    }\n]\n\n# Add system message if provided\nif system_prompt:\n    conversation.insert(0, {\n        \"role\": \"system\", \n        \"content\": [{\"text\": system_prompt}]\n    })\n\nresponse = client.converse(\n    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    messages=conversation,\n    inferenceConfig={\n        \"temperature\": float(temperature),\n        \"maxTokens\": int(max_tokens)\n    }\n)\n\nreturn {\n    \"output\": response[\"output\"][\"message\"][\"content\"][0][\"text\"],\n    \"context\": response[\"usage\"][\"inputTokens\"] + response[\"usage\"][\"outputTokens\"]\n}\n</code></pre>"},{"location":"integrations/llm-providers/aws-bedrock/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>Before registering models, configure your AWS credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on Amazon Bedrock</li> <li>Configure AWS credentials:</li> <li>Access Key ID: Your AWS access key</li> <li>Secret Access Key: Your AWS secret key</li> <li>Session Token: Optional temporary session token</li> <li>Default Region: AWS region (e.g., us-east-1)</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>AWS_ACCESS_KEY_ID</code> - <code>AWS_SECRET_ACCESS_KEY</code> - <code>AWS_SESSION_TOKEN</code> - <code>AWS_DEFAULT_REGION</code></p>"},{"location":"integrations/llm-providers/aws-bedrock/#example-use-case-document-analysis-model","title":"Example Use Case: Document Analysis Model","text":"<p>An AWS Bedrock Claude model configured for enterprise document analysis demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/aws-bedrock/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>temperature</code> (Numerical, optional, default: \"0.3\")</li> <li><code>max_tokens</code> (Numerical, optional, default: \"2000\")</li> <li><code>system_prompt</code> (String, optional, default: \"You are an expert document analyst.\")</li> </ul>"},{"location":"integrations/llm-providers/aws-bedrock/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: document_analyzer\nresult = document_analyzer(\n    text=\"Your document content here...\",\n    temperature=0.3,\n    max_tokens=2000,\n    system_prompt=\"Analyze this document for key insights, themes, and actionable recommendations.\"\n)\n</code></pre>"},{"location":"integrations/llm-providers/aws-bedrock/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Review AWS Bedrock documentation</li> <li>Check supported foundation models</li> <li>Monitor usage through AWS Console</li> <li>Set up cost management and billing alerts</li> </ul>"},{"location":"integrations/llm-providers/azureai/","title":"AzureAI","text":""},{"location":"integrations/llm-providers/azureai/#azure-ai-integration","title":"Azure AI Integration","text":"<p>The Azure AI integration provides access to OpenAI models hosted on Microsoft Azure through a unified interface. Configure once, use everywhere with enterprise-grade security and compliance.</p>"},{"location":"integrations/llm-providers/azureai/#integrating-azure-ai","title":"Integrating Azure AI","text":"<p>Simply enter your Azure OpenAI API key and endpoint once in the Platform Integrations section. This enables authorized users to access Azure-hosted OpenAI models within the platform. Once integrated, models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered Azure model\nresult = azure_gpt4_model(text=\"Analyze this data\", temperature=0.8)\n</code></pre>"},{"location":"integrations/llm-providers/azureai/#supported-models","title":"Supported Models","text":"<p>Azure AI provides access to OpenAI models hosted on Microsoft Azure:</p> <p>GPT-4 - Advanced reasoning and complex task completion GPT-3.5 Turbo - Fast, efficient responses for most use cases o4-mini, o3, o3-mini - Latest OpenAI models with enhanced capabilities Additional Models - other OpenAI variants available</p>"},{"location":"integrations/llm-providers/azureai/#registering-a-new-azure-model","title":"Registering a New Azure Model","text":"<p>Navigate to New Model to begin registration. The registration form connects your Azure AI integration with custom model configurations.</p>"},{"location":"integrations/llm-providers/azureai/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"GPT-4 on Azure optimized for document analysis. Use for enterprise content processing with Azure compliance. Ideal for sensitive data workflows.\"</p>"},{"location":"integrations/llm-providers/azureai/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>azure_gpt4_analyzer</code>, <code>corridor_gpt4</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your Azure integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"Azure AI\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/azureai/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for Azure models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>temperature</code> Numerical Yes 0.7 Controls response creativity <code>max_tokens</code> Numerical Yes 1500 Maximum response length <code>system_prompt</code> String Yes \"\" System instructions <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/azureai/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <pre><code># Arguments: text, temperature are automatically available\nimport os\nfrom openai import AzureOpenAI\n\n# Direct initialization\nclient = AzureOpenAI(\n    azure_endpoint=\"https://corridor-genai-demo.openai.azure.com/\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    api_version=\"2024-12-01-preview\",\n)\n\nchat_prompt = [{\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": text}]}]\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4.1\",\n    messages=chat_prompt,\n    max_tokens=1500,\n    temperature=float(temperature),\n    top_p=0.95,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None,\n    stream=False,\n)\n\nreturn {\"output\": completion.choices[0].message.content, \"context\": None}\n</code></pre>"},{"location":"integrations/llm-providers/azureai/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>Before registering models, configure your Azure credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on Azure AI</li> <li>Enter your Azure OpenAI API key</li> <li>Provide your Azure endpoint URL</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>AZURE_ENDPOINT</code></p>"},{"location":"integrations/llm-providers/azureai/#example-use-case-document-processing-model","title":"Example Use Case: Document Processing Model","text":"<p>An Azure GPT-4 model configured for enterprise document processing demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/azureai/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>temperature</code> (Numerical, optional, default: \"0.3\")</li> <li><code>max_tokens</code> (Numerical, optional, default: \"1500\")</li> <li><code>system_prompt</code> (String, optional, default: \"\")</li> </ul>"},{"location":"integrations/llm-providers/azureai/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: document_processor\nresult = document_processor(\n    text=\"Your document text here...\",\n    temperature=0.3,\n    max_tokens=2000,\n    system_prompt=\"Process the document and extract key information.\"\n)\n</code></pre>"},{"location":"integrations/llm-providers/azureai/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Review Azure OpenAI documentation</li> <li>Check Azure compliance and security features</li> <li>Monitor usage through Azure Portal</li> <li>Set up cost management and billing alerts</li> </ul>"},{"location":"integrations/llm-providers/gcp-vertexai/","title":"GCP VertexAI","text":""},{"location":"integrations/llm-providers/gcp-vertexai/#google-cloud-vertex-ai-integration","title":"Google Cloud Vertex AI Integration","text":"<p>The Google Cloud Vertex AI integration provides access to Gemini models and AI agents through a unified interface. Configure once, use everywhere with enterprise-grade security and scalability.</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#integrating-google-cloud-vertex-ai","title":"Integrating Google Cloud Vertex AI","text":"<p>Simply upload your service account JSON key once in the Platform Integrations section. This enables authorized users to access Google Cloud Vertex AI models within the platform. Once integrated, models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered Gemini model\nresult = gemini_model(text=\"Analyze this data\", temperature=0.8)\n</code></pre>"},{"location":"integrations/llm-providers/gcp-vertexai/#supported-models","title":"Supported Models","text":"<p>Google Cloud Vertex AI provides access to Gemini models and agents:</p> <p>Gemini 2.5 Pro - Advanced reasoning and multimodal capabilities Gemini 2.5 Flash - Fast, efficient responses for high-volume use cases Gemini 2.0 Flash - Latest generation model with improved performance Additional Models - other Gemini variants available</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#registering-a-new-gemini-model","title":"Registering a New Gemini Model","text":"<p>Navigate to New Model to begin registration. The registration form connects your Google Cloud integration with custom model configurations.</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"Gemini 2.5 Pro optimized for content analysis. Use for document summarization and multimodal tasks. Ideal for complex reasoning workflows.\"</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>gemini_analyzer</code>, <code>content_summarizer</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your Google Cloud integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"Google Vertex AI\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for Gemini models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>temperature</code> Numerical Yes 0.7 Controls response creativity <code>system_instruction</code> String Yes \"\" System prompt for model behavior <code>seed</code> Numerical Yes 2025 Deterministic generation seed <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <pre><code># Arguments: text, temperature, system_instruction are automatically available\nimport os\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=os.getenv(\"GOOGLE_API_TOKEN\"))\n\nconfig = types.GenerateContentConfig(\n    temperature=temperature, \n    seed=2025, \n    system_instruction=system_instruction\n)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\", \n    contents=text, \n    config=config\n)\n\nreturn {\"response\": response.text}\n</code></pre>"},{"location":"integrations/llm-providers/gcp-vertexai/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>Before registering models, configure your Google Cloud credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on Google Vertex AI</li> <li>Upload your service account JSON key file</li> <li>Enter your Google Cloud project ID</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>GOOGLE_API_TOKEN</code> - <code>GOOGLE_CLOUD_PROJECT</code></p>"},{"location":"integrations/llm-providers/gcp-vertexai/#example-use-case-content-analysis-model","title":"Example Use Case: Content Analysis Model","text":"<p>A Gemini 2.5 Pro model configured for content analysis demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/gcp-vertexai/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>temperature</code> (Numerical, optional, default: \"0.3\")</li> <li><code>system_instruction</code> (String, optional, default: \"You are an expert content analyst.\")</li> <li><code>seed</code> (Numerical, optional, default: \"2025\")</li> </ul>"},{"location":"integrations/llm-providers/gcp-vertexai/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: content_analyzer\nresult = content_analyzer(\n    text=\"Your document content here...\",\n    temperature=0.3,\n    system_instruction=\"Provide a comprehensive analysis with key insights and recommendations.\",\n    seed=2025\n)\n</code></pre>"},{"location":"integrations/llm-providers/gcp-vertexai/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Review Google Cloud Vertex AI documentation</li> <li>Check Gemini model specifications</li> <li>Monitor usage and costs through Google Cloud Console</li> <li>Set up billing alerts for cost management</li> </ul>"},{"location":"integrations/llm-providers/huggingface/","title":"HuggingFace","text":""},{"location":"integrations/llm-providers/huggingface/#hugging-face-integration","title":"Hugging Face Integration","text":"<p>The Hugging Face integration provides direct access to thousands of open-source models from the Hugging Face Hub through a unified interface. Load once, cache efficiently, and use enterprise-grade model management with full governance and tracking.</p>"},{"location":"integrations/llm-providers/huggingface/#integrating-hugging-face","title":"Integrating Hugging Face","text":"<p>No platform-level integration required - Hugging Face models are accessed directly through the <code>transformers</code> library. For private models, configure your Hugging Face token once in your environment. Models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered Hugging Face model\nresult = sentiment_analyzer(text=\"This product is amazing!\", confidence_threshold=0.8)\n</code></pre>"},{"location":"integrations/llm-providers/huggingface/#supported-models","title":"Supported Models","text":"<p>Hugging Face provides access to thousands of models across multiple categories:</p> <p>Text Classification - Sentiment analysis, content moderation, topic classification Text Generation - LLaMA, Mistral, CodeLlama, and other foundation models Text Embedding - Sentence transformers, multilingual embeddings Guardrail Models - Prompt injection detection, content safety filters Additional Models - other models available on Hugging Face Hub</p>"},{"location":"integrations/llm-providers/huggingface/#registering-a-new-hugging-face-model","title":"Registering a New Hugging Face Model","text":"<p>Navigate to New Model to begin registration. The registration form connects Hugging Face models with your platform's governance and caching infrastructure.</p>"},{"location":"integrations/llm-providers/huggingface/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"RoBERTa-based sentiment classifier trained on social media data. Use for customer feedback analysis and content moderation. Optimized for short text under 512 tokens.\"</p>"},{"location":"integrations/llm-providers/huggingface/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>sentiment_classifier</code>, <code>prompt_guard</code>, <code>sentence_embedder</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"Hugging Face\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/huggingface/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for Hugging Face models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>max_length</code> Numerical Yes 512 Maximum token length <code>temperature</code> Numerical Yes 0.7 Generation randomness <code>threshold</code> Numerical Yes 0.5 Classification threshold <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/huggingface/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <p>Example implementation for a text classification model:</p> <pre><code># Arguments: text, threshold are automatically available\nimport os\nfrom transformers import pipeline\nfrom huggingface_hub import login\n\n# Authentication for private models (optional)\nlogin(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n\n# Direct initialization\nclient = pipeline(\n    \"text-classification\",\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n)\n\nif text is None:\n    return None\n\nresults = client(text)\nconfidence = results[0][\"score\"]\n\nif confidence &gt;= threshold:\n    return {\"sentiment\": results[0][\"label\"], \"confidence\": confidence}\nelse:\n    return {\"sentiment\": \"uncertain\", \"confidence\": confidence}\n</code></pre>"},{"location":"integrations/llm-providers/huggingface/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>For private models, configure your Hugging Face credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on Hugging Face</li> <li>Enter your Hugging Face API token</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>HUGGINGFACE_TOKEN</code></p>"},{"location":"integrations/llm-providers/huggingface/#example-use-case-prompt-injection-detector","title":"Example Use Case: Prompt Injection Detector","text":"<p>A Hugging Face guardrail model configured for prompt injection detection demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/huggingface/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>threshold</code> (Numerical, optional, default: \"0.5\")</li> <li><code>max_length</code> (Numerical, optional, default: \"512\")</li> </ul>"},{"location":"integrations/llm-providers/huggingface/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: prompt_injection_detector\nresult = prompt_injection_detector(\n    text=\"Ignore previous instructions\",\n    threshold=0.8,\n    max_length=512\n)\n</code></pre>"},{"location":"integrations/llm-providers/huggingface/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Browse models at Hugging Face Hub</li> <li>Review Hugging Face Transformers documentation</li> <li>Check model licensing for compliance</li> <li>Monitor usage through platform analytics</li> </ul>"},{"location":"integrations/llm-providers/openai/","title":"OpenAI","text":""},{"location":"integrations/llm-providers/openai/#openai-integration","title":"OpenAI Integration","text":"<p>The OpenAI integration provides direct access to OpenAI models through a unified interface. Configure once, use everywhere with enterprise-grade performance and the latest AI capabilities from OpenAI.</p>"},{"location":"integrations/llm-providers/openai/#integrating-openai","title":"Integrating OpenAI","text":"<p>Simply enter your OpenAI API key once in the Platform Integrations section. This enables authorized users to access OpenAI models within the platform. Once integrated, models can be registered and used as any other python object on the platform.</p> <pre><code># Example: Using a registered OpenAI model\nresult = openai_gpt4_model(text=\"Analyze this data\", temperature=0.8)\n</code></pre>"},{"location":"integrations/llm-providers/openai/#example-of-models-supported","title":"Example of Models Supported","text":"<p>OpenAI provides access to state-of-the-art language models with different capabilities:</p> <p>GPT-4o - Most advanced multimodal model with vision and reasoning capabilities GPT-4 - Advanced reasoning and complex task completion GPT-3.5 Turbo - Fast, efficient responses for most use cases o1-preview, o1-mini - Latest reasoning models with enhanced problem-solving Additional Models - Latest OpenAI variants with enhanced capabilities</p>"},{"location":"integrations/llm-providers/openai/#registering-a-new-openai-model","title":"Registering a New OpenAI Model","text":"<p>Navigate to New Model to begin registration. The registration form connects your OpenAI integration with custom model configurations.</p>"},{"location":"integrations/llm-providers/openai/#basic-information","title":"Basic Information","text":"<p>Description: Document your model's purpose, use cases, and limitations. For example: \"GPT-4o optimized for content analysis and generation. Use for enterprise content processing with multimodal capabilities. Ideal for complex reasoning and creative tasks.\"</p>"},{"location":"integrations/llm-providers/openai/#code-configuration","title":"Code Configuration","text":"<p>Alias: A unique identifier for your model (e.g., <code>openai_gpt4_analyzer</code>, <code>content_generator</code>). This becomes the variable name you'll use in code.</p> <p>Output Type: Define the return format: - <code>Map[String, String]</code> - Key-value pairs for structured responses - <code>String</code> - Simple text responses - <code>List</code> - Array of items</p> <p>Input Type: Select your implementation approach: - API Based: Platform handles API calls automatically using your OpenAI integration - Python Function: Custom function implementation with full control - Custom: Advanced configurations for specialized use cases</p> <p>Model Provider: Select \"OpenAI\" from your configured integrations.</p>"},{"location":"integrations/llm-providers/openai/#arguments-configuration","title":"Arguments Configuration","text":"<p>Define input parameters that your model will accept. Important: Variables declared here are automatically available in the Scoring Logic section.</p> <p>Common argument patterns for OpenAI models:</p> Alias Type Optional Default Value Usage <code>text</code> String No N/A Main input content <code>temperature</code> Numerical Yes 0.7 Controls response creativity <code>max_tokens</code> Numerical Yes 1500 Maximum response length <code>system_prompt</code> String Yes \"\" System instructions <p>Use + Add Argument to include additional parameters.</p>"},{"location":"integrations/llm-providers/openai/#scoring-logic-implementation","title":"Scoring Logic Implementation","text":"<p>In the Scoring Logic section, you can directly reference any variable declared in the Arguments section. The platform automatically makes these available in your code.</p> <pre><code># Arguments: text, temperature are automatically available\nimport os\nfrom openai import OpenAI\n\n# Direct initialization\nclient = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\nif text is None:\n    return None\n\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt if system_prompt else \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": text}\n]\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=messages,\n    max_tokens=int(max_tokens),\n    temperature=float(temperature),\n    top_p=0.95,\n    frequency_penalty=0,\n    presence_penalty=0\n)\n\nreturn {\"output\": completion.choices[0].message.content, \"context\": None}\n</code></pre>"},{"location":"integrations/llm-providers/openai/#platform-integration-setup","title":"Platform Integration Setup","text":"<p>Before registering models, configure your OpenAI credentials:</p> <ol> <li>Navigate to Settings &gt; Platform Integrations</li> <li>Click on OpenAI</li> <li>Enter your OpenAI API key</li> <li>Test the connection</li> </ol> <p>The platform creates environment variables automatically: - <code>OPENAI_API_KEY</code></p>"},{"location":"integrations/llm-providers/openai/#example-use-case-content-analysis-model","title":"Example Use Case: Content Analysis Model","text":"<p>An OpenAI GPT-4o model configured for enterprise content analysis demonstrates the complete workflow:</p>"},{"location":"integrations/llm-providers/openai/#arguments-configuration_1","title":"Arguments Configuration:","text":"<ul> <li><code>text</code> (String, required)</li> <li><code>temperature</code> (Numerical, optional, default: \"0.3\")</li> <li><code>max_tokens</code> (Numerical, optional, default: \"2000\")</li> <li><code>system_prompt</code> (String, optional, default: \"You are an expert content analyst.\")</li> </ul>"},{"location":"integrations/llm-providers/openai/#usage","title":"Usage:","text":"<pre><code># Model becomes available as: content_analyzer\nresult = content_analyzer(\n    text=\"Your content text here...\",\n    temperature=0.3,\n    max_tokens=2000,\n    system_prompt=\"Analyze this content for key themes, sentiment, and actionable insights.\"\n)\n</code></pre>"},{"location":"integrations/llm-providers/openai/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>Review OpenAI API documentation</li> <li>Check OpenAI model capabilities</li> <li>Monitor usage through OpenAI Dashboard</li> <li>Set up rate limiting and cost management</li> </ul>"},{"location":"register-and-refine/","title":"GenerativeAI Lifecycle Management","text":""},{"location":"register-and-refine/#generative-ai-lifecycle","title":"Generative AI Lifecycle:","text":"<p>The Generative AI development lifecycle is a systematic framework designed to help businesses create effective AI-driven solutions across various applications. Each phase is vital in ensuring the technology\u2019s accuracy, dependability, and ethical implementation.</p> <p></p> <p>Below is a breakdown of each stage and its significance:</p>"},{"location":"register-and-refine/#1-business-use-case-identification","title":"1. Business Use Case Identification","text":"<p>The first step in the lifecycle involves defining the problem statement and objectives that the solution aims to address. Clearly defining the business goals is a very crucial first step as it lays the foundation for a well-structured development and validation strategy.</p>"},{"location":"register-and-refine/#2-training-and-validation-data-collection","title":"2. Training and Validation Data Collection","text":"<p>A GenAI pipeline relies heavily on high-quality, diverse datasets that cover all the business scenarios for testing and validation. Data must be gathered from reliable sources, preprocessed for consistency, and cleaned to remove noise. The platform enforces strong data governance and data quality checks that ensure integrity, compliance, and fairness, which directly impact model performance and validation. Usually, sample data is created out of this for quick testing during the pipeline development phase to expedite iterations.</p>"},{"location":"register-and-refine/#3-pipeline-development","title":"3. Pipeline Development","text":"<p>Developing an AI pipeline is an iterative process aimed at rapidly building the first version of prompts, models, and Retrieval-Augmented Generation (RAG) components to address a specific business use case. Once the initial version is ready, incremental improvements can be made by refining each component and analyzing performance gains.</p> <ul> <li> <p>Crafting the Initial Prompt   Designing an effective prompt is crucial for generating accurate and relevant responses. However, prompt engineering is an iterative process that requires multiple refinements as development progresses.</p> </li> <li> <p>Connecting to External Knowledge   Enhancing model responses with external knowledge sources improves contextual accuracy. In GenAI pipelines, this is achieved using RAG techniques. Building a robust RAG architecture ensures the pipeline retrieves the most relevant information, significantly impacting overall accuracy.</p> </li> <li> <p>Choosing the Best LLM for the Use Case   Selecting an optimal LLM depends on various factors such as the business problem, expected outcomes, cost, and operational efficiency. Choosing the right model ensures the pipeline aligns with business objectives.</p> </li> <li> <p>Pipeline Creation   Once the foundational components are established, end-to-end pipeline development begins. Often creating multiple pipeline versions and comparing them helps in selecting the best-performing one. One can swap in and swap out multiple building blocks of the pipeline to get to the best state.</p> </li> <li> <p>Experimentation with Algorithms and Settings   Optimizing a pipeline requires experimenting with different models, algorithms, and hyperparameter settings (e.g., temperature, top-k, top-p). Fine-tuning these parameters ensures the model generates responses that align with performance goals.</p> </li> </ul>"},{"location":"register-and-refine/#4-evaluations-automated-and-human-based","title":"4. Evaluations - Automated and Human-based","text":"<p>Before deploying a GenAI pipeline, it must be rigorously tested for accuracy, fairness, stability and robustness. A combination of automated evaluation techniques and human assessments ensures comprehensive validation. Tracking performance metrics and refining the model based on evaluation results helps maintain reliability and transparency.</p>"},{"location":"register-and-refine/#5-move-to-production","title":"5. Move to Production","text":"<p>Once the pipeline is validated, it is deployed into production environments where it integrates with existing business systems so that it can be used for real-world decision-making.</p>"},{"location":"register-and-refine/#6-production-monitoring","title":"6. Production Monitoring","text":"<p>Continuous monitoring is crucial to maintaining performance and mitigating issues such as response drift and biases. Implementing monitoring tools ensures sustained accuracy and compliance with business and regulatory requirements.</p>"},{"location":"register-and-refine/#how-does-corridor-help-with-genai-lifecycle-management","title":"How does Corridor help with GenAI Lifecycle Management?","text":"<ul> <li>Data Integration: Supports validation and testing data registration by integrating with production data lakes and in-house storage solutions.</li> <li>Component Inventory: Maintains a registry of essential GenAI pipeline components, including models, RAGs, prompts, and use-case-specific pipelines to enhance reusability and develop smaller building blocks.</li> <li>Governance and Collaboration: Provides features such as version management, change history, approvals, ongoing reviews, lineage tracking, impact assessment, and team collaboration to enhance governance, experiment tracking, MRM, FL and business approvals.</li> <li>Evaluation and Reporting: Enables assessment of complete pipelines and individual components using standardized (tailored for MRM, Fair Lending, and business needs) and custom reports.</li> <li>Human-Integrated Pre-Production Testing: Facilitates real-world-like testing by allowing different teams to conduct robust validation before deployment.</li> <li>Artifact Export and Documentation: Supports production artifact export and ODD generations.</li> <li>Monitoring: Connects to production data sources for response labelling and automated performance monitoring and alerting.</li> </ul>"},{"location":"register-and-refine/lineage-tracking/","title":"Lineage Tracking","text":"<p>Lineage tracking records and visualizes the complete lifecycle of an object and depicts the flow of execution, showing how objects interact, are transformed and executed. It captures its origins, building blocks used, transformations, and usages across the organization.</p>"},{"location":"register-and-refine/lineage-tracking/#why-lineage-tracking-is-important","title":"Why Lineage Tracking is Important?","text":"<ul> <li>Ensures Traceability by providing a clear link between all artefacts, ensuring transparency in dependencies and usages.</li> <li>Easier Impact Assessment for modifications.</li> <li>Provides a clear view of all dependent objects that would be affected by changes.</li> <li>Identifies necessary updates across dependencies to introduce changes.</li> <li>Enhances Collaboration by providing visibility into how different teams and models interact within the pipeline.</li> <li>Ensures Compliance with regulations like the EU AI Act (Article 13 &amp; 14), which mandates transparency and human oversight.</li> <li>Identifying redundant processes.</li> </ul>"},{"location":"register-and-refine/lineage-tracking/#lineage-structure-on-ggx-platform","title":"Lineage Structure on GGX Platform:","text":"<p>A Lineage is created and automatically maintained by the platform as soon as an object is registered. The lineage graph which is present on the details page is a horizontal-tree representation that maps how an object is created, used, and executed within the pipeline.</p> <p>It consists of three key elements:</p> <ul> <li>Precedents (Inputs): Represent the sources or dependencies that contribute to the creation of an object.</li> <li>Dependents (Usage): Indicate other objects that rely on a given object.</li> </ul> <p></p>"},{"location":"register-and-refine/prompt-optimization/","title":"Prompt Optimization","text":"<p>Prompt optimization is the process of refining prompts to get better and more accurate responses from AI models. When we create prompts for LLMs to handle complex tasks, the goal is to make intent as clear as possible. However, fully conveying every detail in one attempt is extremely difficult. The process often involves repeated testing and refining, making small adjustments until the model delivers the desired outcome.</p> <p>This process of making changes testing impact and maintaining a clear track record of all the experiments can be challenging if done manually. The GGX Platform simplifies and automates this process by introducing Hill Climbing Experimentations.</p>"},{"location":"register-and-refine/prompt-optimization/#what-is-hill-climbing","title":"What is Hill Climbing?","text":"<p>Hill Climbing is a method used to solve optimization problems by gradually improving a solution. It starts with an initial guess and makes small adjustments, keeping any changes that lead to a better result. This process continues until no further improvements can be made, similar to climbing a hill by always moving upward.</p>"},{"location":"register-and-refine/prompt-optimization/#how-it-works","title":"How It Works?","text":"<ul> <li>Initialization \u2013 Start with an initial prompt.</li> <li>Evaluation \u2013 Run an evaluation using fixed components:</li> <li>Predefined LLM with set hyperparameters</li> <li>Standardized metrics for performance measurement</li> <li>A consistent dataset for testing</li> <li>Modification \u2013 Make small, targeted improvements to the prompt.</li> <li>Comparison \u2013 Rerun the evaluation and measure performance changes.</li> <li>Update \u2013 If the new prompt improves results, it becomes the new baseline.</li> <li>Iteration \u2013 Repeat the process until the desired performance is achieved.</li> </ul>"},{"location":"register-and-refine/prompt-optimization/#benefits-of-hill-climbing-capability","title":"Benefits of Hill-Climbing Capability:","text":"<ul> <li>The platform maintains clear logs of all prompt updates and their impact on model performance.</li> <li>Once the best-performing prompt is identified, it can be synced back to the base pipeline.</li> <li>Enables multiple team members to collaborate simultaneously on optimization.</li> <li>Customizable reports for tracking the Hill-Climbing progress.</li> </ul> <p>Note: The platform also provides flexibility to integrate external prompt optimizers like Google Prompt Optimizer.</p>"},{"location":"register-and-refine/prompt-optimization/#how-to-run-hill-climbing-tasks-on-platform","title":"How to run Hill Climbing Tasks on Platform?","text":"<ul> <li>Go to object Details page.</li> <li>Click on the Run -&gt; Hill Climbing button.</li> <li>Provide description about the run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Make changes to the prompts which are part of the object.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li>Click on Run at the bottom and wait for job completion.</li> <li>Once the job is completed, one can check the optimization progress in Dashboards.</li> </ul>"},{"location":"register-and-refine/collaboration/","title":"Collaboration","text":"<p>The platform is designed to enhance collaboration among teams involved in the development, testing, and monitoring of various GenAI pipelines. Assets from different workstreams can be organized in a central location, enabling developers, reviewers, and testers to build, iterate, and reuse together.</p>"},{"location":"register-and-refine/collaboration/#requesting-object-access-and-sharing-objects","title":"Requesting Object Access and Sharing Objects:","text":"<p>Registered objects in Draft or Pending Approval status can be shared with users on the platform, or users can raise an access request.</p> <p>There are currently two types of access (Read and Write) that can be requested or provided:</p> <ul> <li>Read \u2013 With Read access, the user can view, run evaluations, download documentation and artifacts, and reuse the object in another object.</li> <li>Write \u2013 With Write access, the user can edit, delete, send for approval, and share the object with others.</li> </ul> <p>When requesting access, the user can choose to request access to the object only, or to the object along with its lineage.</p> <p>Important Notes:</p> <ul> <li>Users can only request access to objects they are eligible for, based on the user role configuration in Settings.</li> <li>Re-sharing of objects with complete lineage is required if new objects are added to it.</li> <li>When an object is shared, a notification is sent to the receiver indicating that an object has been shared.</li> <li>A non-owner will not be able to share the object if they have 'Read' permissions only.</li> <li>Users can also share access to objects using Jupyter Notebook via the Corridor package.</li> <li>Access granted through sharing can be changed or revoked by either the object owner or anyone with write access to the object.</li> </ul>"},{"location":"register-and-refine/collaboration/#access-management","title":"Access Management:","text":"<p>Role-Based Access Management is available to govern how users, based on their assigned roles, can use the platform. Every onboarded user is assigned a user role.</p> <p>Roles can be created based on the following framework:</p> <ul> <li>The platform is structured as a set of modules (Data Vault, GenAI Studio, Resources, etc.). Modules can be enabled, disabled, or hidden.</li> <li>Each module comprises different pages (e.g., Table Registry, Quality Procedure, Prompts, RAGs, Reports, etc.). Pages can be enabled, disabled, or hidden.</li> <li>Within each page, objects such as Tables, Models, Pipelines, and Reports can be accessed.</li> <li>Objects have properties and attributes on which access can also be controlled.</li> <li>Access can be of different types: read, write, approve, and superuser.</li> <li>Access to each of the above components can be granted independently using the Roles capability.</li> </ul> <p>Note: Granular access control can be given to roles (such as restricting access to a particular collection of objects) using custom rules.</p>"},{"location":"register-and-refine/collaboration/#integrating-external-updates-to-objects","title":"Integrating External Updates to Objects:","text":"<p>The registered object definition can be exported from the platform, modified externally, and re-synced using Corridor commands. The platform automatically tracks and records all external changes for clarity and consistency.</p>"},{"location":"register-and-refine/collaboration/#grouping-objects","title":"Grouping Objects:","text":"<p>Groups provide a way to classify objects for control and display purposes. Groups are specific to object types.</p> <ul> <li>When defining roles, administrators can use groups to specify access rights. Different teams can create custom groups that are accessible only to the relevant team members to register, test, and monitor their assets.</li> <li>In the registries, objects are displayed by group, making them easier to find and work on.</li> </ul>"},{"location":"register-and-refine/collaboration/#workspaces","title":"Workspaces:","text":"<p>The platform enables the creation of multiple workspaces, allowing teams to work independently without visibility into each other's work.</p>"},{"location":"register-and-refine/collaboration/#monitoring-dashboard-and-alerting","title":"Monitoring Dashboard and Alerting:","text":"<p>The platform enables the creation of customized dashboards for key stakeholders and top management, providing a bird's-eye view of activities across different teams and stages of the application lifecycle.</p>"},{"location":"register-and-refine/inventory-management/","title":"Inventory Management","text":""},{"location":"register-and-refine/inventory-management/#overview","title":"Overview","text":"<p>The platform allows registering, tracking, and monitoring of Data and GenAI assets (like RAG, Models, LLMs and Pipelines) at a centralized location.</p>"},{"location":"register-and-refine/inventory-management/#why-inventory-management-is-helpful","title":"Why Inventory Management is Helpful?","text":"<ul> <li>Governance and Compliance: Helps track AI models, datasets, and dependencies for regulatory audits.</li> <li>Reusability &amp; Efficiency: Prevents duplication of efforts by enabling teams to reuse registered and approved assets and standardized inventories reducing onboarding time for new teams.</li> <li>Security &amp; Access Control: Centralized inventories allow proper role-based access management.</li> <li>Monitoring &amp; Continuous Improvement: Ensures GenAI systems can be tested before moving to production and periodically monitored post-production.</li> </ul>"},{"location":"register-and-refine/inventory-management/#asset-registries","title":"Asset Registries:","text":"<p>Read more on different registries below:</p> <ul> <li>Data Inventory</li> <li>LLMs and Models</li> <li>Prompts</li> <li>RAGs</li> <li>End-to-End Pipeline Assets</li> </ul>"},{"location":"register-and-refine/inventory-management/#how-platform-helps-in-managing-inventories","title":"How Platform Helps in Managing Inventories?","text":"<p>The platform offers extensive capabilities to streamline onboarding and efficiently manage assets.</p> <ul> <li>Multiple registries are available to centralize and manage smaller, reusable components of the pipeline.</li> <li>Customized groups for creating assets within a registry.</li> <li>Permissible Purpose Tracking that enables automatic validation to ensure components are used only for authorized purposes.</li> <li>Flexible and granular access management.</li> <li>Change History tracking for registered assets.</li> <li>Lineage Tracking of registered assets.</li> <li>Sharing of assets within and across teams.</li> <li>Creating Custom Fields for the registry apart from the default ones.</li> </ul>"},{"location":"register-and-refine/inventory-management/#metadata-tagging","title":"Metadata Tagging","text":"<p>When any item is added to an entity - during the registration, various fields can be tagged to that object. Some basic fields are mandatory - for example:</p> <ul> <li>Alias - A Python variable name to refer to the object by</li> <li>Type - The data type that the object returns</li> <li>Description - A free format field that can be used to describe the object being created.</li> <li>Group - Useful to organize items, making them easier to search and find later</li> <li>Permissible Purpose - A governance tracking mechanism to ensure items are used correctly</li> <li>Location (of Data) - A data lake location where the data resides and can be fetched from</li> <li>Training &amp; Validation Data - Used when creating models</li> </ul> <p>New fields can be added to any of the registries to facilitate better inventory management in Settings &gt; Fields &amp; Screens section. Fields of various types can be added:</p> <ul> <li>Short Text</li> <li>Long Text</li> <li>Date Time</li> <li>File</li> <li>Single Select</li> <li>Multi Select</li> <li>Multiple Files</li> </ul> <p>They can be marked as mandatory and customized with descriptions, placeholder values, default values, etc. and even be made mandatory to fill in. Values for fields can also be programmatically computed - with Python formulae.</p>"},{"location":"register-and-refine/inventory-management/#data-type","title":"Data Type","text":"<p>Data Types on the platform are useful to declare clear types that can be used for documentation. Data Types are very flexible on the platform. The types supported are:</p> <ul> <li> <p>Scalar Types:</p> </li> <li> <p>Numerical</p> </li> <li>String</li> <li>DateTime</li> <li> <p>Boolean</p> </li> <li> <p>Array Types:</p> </li> <li> <p>Array[Numerical]</p> </li> <li>Array[String]</li> <li>Array[Array[DateTime]]</li> <li> <p>and other types can be created by mixing existing types ...</p> </li> <li> <p>Struct Types:</p> </li> <li> <p>Struct[decision: String, ranking: Numerical]</p> </li> <li>Struct[amt: Array[Numerical], flag: Boolean]</li> <li>Struct[info: Map[Numerical,String],details: Struct[id: String,date: DateTime,age: Numerical]]</li> <li> <p>and other types can be created by mixing existing types ...</p> </li> <li> <p>Map Types:</p> </li> <li> <p>Map[Numerical, String]</p> </li> <li>Map[String, Numerical]</li> <li>Map[Numerical, Array[Boolean]]</li> <li>and other types can be created by mixing existing types ...</li> </ul>"},{"location":"register-and-refine/inventory-management/global-functions/","title":"Global Functions","text":""},{"location":"register-and-refine/inventory-management/global-functions/#what-are-global-functions","title":"What are Global Functions?","text":"<p>Global function gives the ability to execute a set of analytical operations multiple times using different objects as inputs without having to rewrite those operations every time. It supports inputs and outputs of any type, including DataFrames, dictionaries, and other standard Python data types, without mandatorily requiring predefined input-output formats.</p> <p>Global Function can be used across the platform in GenAI Studio, Reports, Simulation Data-Sources, or even within other Global Functions, offering enhanced reusability.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#maintaining-global-functions-on-the-platform","title":"Maintaining Global Functions on the Platform:","text":"<p>The Global Function registry organizes all the registered utilities into customized groups at this centralized location, allowing easier tracking, monitoring, and new function creation.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#global-function-registration","title":"Global Function Registration:","text":"<ol> <li>Click on Create button in Global function registry.</li> <li>Fill in important details like Name, Attribute (Output Type, Alias), Properties (Description, Group, Approval Workflow).</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select other global functions if required in Inputs section to help in writing logic.</li> <li>Write code logic in definition source.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Click on Save button to finally register the function.</li> </ol> <p>Note: Output type is not mandatory for function registry. If missing platform allows returning any type including dataframes, dictionaries etc.</p> <p>Once registration is completed, the Global Functions can be used all across the platform in GenAI Studio, Report Writing, etc.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#benefits-of-global-functions-registration","title":"Benefits of Global Functions Registration:","text":"<ul> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Better collaboration for continuous model building and testing.</li> <li>Full auditability and approvals for future usages.</li> </ul>"},{"location":"register-and-refine/inventory-management/model-catalog/","title":"Model Catalog","text":""},{"location":"register-and-refine/inventory-management/model-catalog/#what-is-a-model","title":"What is a Model?","text":"<p>A model is a software program that uses algorithms/rules to make informed decisions, predictions, or generations based on a set of inputs without being given explicit instructions for every scenario. (e.g., ML models, Lookup tables, If-Else rules, LLMs, etc.)</p> <p>A model typically includes one or more of the following components:</p> <ul> <li>Model file: Stores learned weights/parameters, lookup tables, tensors, and other important data to be used for initializing the model.</li> <li>Scoring Logic: Code that applies the model to provide inputs to generate/predict responses.</li> </ul> <p>GGX supports the registration of various types of models:</p> <ul> <li>API-based models: Connect to externally hosted models like OpenAI, Gemini, etc., using APIs.</li> <li>Python-based models: Lightweight Python logic using various libraries or rule-based models.</li> <li>Custom models: Uploaded model files, including Scikit-learn models, NLP models like BERT, and any fine-tuned models.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/model-catalog/#managing-models-on-the-platform","title":"Managing Models on the Platform:","text":"<p>The Model Catalog organizes all the registered models into customized groups at this centralized location, allowing easier tracking, monitoring, and model creation.</p>"},{"location":"register-and-refine/inventory-management/model-catalog/#registering-a-model","title":"Registering a Model:","text":"<ol> <li>Click on Create button in Model Catalog.</li> <li>Fill in important details like Name, Properties (Group, Permissible Purpose, Description, Approval Workflow),Attributes (Output Type, Alias) or any other relevant details.</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select registered resources (like Model, Global Functions, Prompts, etc.) to use in model definition.</li> <li>Select Input Type (API-Based, Python-based, or Custom) and Model Provider and Model if type is API-Based.</li> <li>Upload weights if required. Define model logic by writing code in Scoring Logic section.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on Save to complete the registration process.</li> </ol> <p>The registered models can be evaluated in the Model Catalog or used in downstream objects (like RAG, Model, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/model-catalog/#benefits-of-model-registration","title":"Benefits of Model Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered models using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> <li>Fingerprinting of external API connectivity.</li> <li>Better collaboration for continuous model building and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/pipelines/","title":"Pipelines","text":""},{"location":"register-and-refine/inventory-management/pipelines/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>A pipeline is a combination of multiple reusable components such as Models, RAGs, Prompts, Guardrails, and Other Sub Pipelines. It processes inputs through a logically structured sequence of these components to generate or predict an output.</p> <p>For example, consider an IVR (Interactive Voice Response) pipeline that automates customer support calls. This pipeline can be constructed by combining two subpipelines:</p> <ol> <li>Intent Classification Subpipeline:    This component analyzes the user's spoken or typed input (e.g., \"I want to check my account balance\") and classifies the intent, such as \"Check Balance\", \"Report Lost Card\", or \"Speak to Agent\".  </li> <li> <p>It can use a Large Language Model (LLM) and a Prompt to interpret the user's message and determine the intent. For example, a prompt can be designed to instruct the LLM to extract the intent from the input text.</p> </li> <li> <p>Response Generation Subpipeline:    Based on the classified intent, this subpipeline generates an appropriate response. For example, if the intent is \"Check Balance\", it fetches the account balance and formulates a response; if the intent is \"Report Lost Card\", it initiates the card blocking process and confirms to the user.  </p> </li> <li>This subpipeline can use an LLM, a Prompt, and a RAG (Retrieval-Augmented Generation) component to access customer data or knowledge bases. For instance, the RAG can retrieve the latest account information or support policies, which the LLM then uses (guided by a prompt) to generate a personalized and accurate response.</li> </ol> <p>The IVR pipeline orchestrates these subpipelines in sequence:</p> <ul> <li> <p>Step 1: Receives the user's message.</p> </li> <li> <p>Step 2: Passes the message to the Intent Classification subpipeline to determine the user's intent.</p> </li> <li> <p>Step 3: Forwards the intent (and possibly the original message) to the Response Generation subpipeline to produce the final reply.</p> </li> </ul> <p>This modular approach allows you to reuse the LLMs, Intent Classification and Response Generation subpipelines in other pipelines as well. You can register each subpipeline and then stitch them to create an overall IVR pipeline in the Pipeline Registry.</p> <p>Example Flow:</p> <ol> <li> <p>User says: \"I lost my credit card.\"</p> </li> <li> <p>Intent Classification subpipeline (using LLM + Prompt) detects intent: \"Report Lost Card\".</p> </li> <li> <p>Response Generation subpipeline (using LLM + Prompt + RAG to customer data) replies: \"I'm sorry to hear that. I have blocked your card. Would you like to request a replacement?\"</p> </li> </ol> <p>Pipelines in GGX can be categorized into two types:</p> <ul> <li>Chat-based Pipelines: Maintain history and context across multiple user messages and pipeline responses, enabling context-aware interactions.</li> <li>Free-Flow Pipelines: Process one input at a time, generating a single output without retaining context from previous interactions.</li> </ul> <p>A typical pipeline consists of:</p> <ul> <li>Resources: Like Model, RAGs, Prompts, Guardrails, Other Pipelines, etc.</li> <li>Pipeline Scoring Logic: Code that applies the pipelines to provided inputs and uses the various components to generate/predict responses.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/pipelines/#managing-pipelines-on-the-platform","title":"Managing Pipelines on the Platform","text":"<p>The Pipeline Registry organizes all the registered pipelines into customized groups at a centralized location, allowing easier tracking, monitoring, and new pipeline creation.</p>"},{"location":"register-and-refine/inventory-management/pipelines/#registering-a-pipeline","title":"Registering a Pipeline:","text":""},{"location":"register-and-refine/inventory-management/pipelines/#create-a-pipeline-from-scratch","title":"Create a Pipeline from Scratch","text":"<ol> <li>Click the Create button in the Pipeline Registry.</li> <li>Fill in important details such as Name and Properties (Group, Permissible Purpose, Description, Approval Workflow), along with any other relevant information.</li> <li>Provide an alias for the pipeline.</li> <li>Select Input Type (Python-based, Custom Registration, or External Agent) and also choose the Pipeline Type (Chat-based or Free-Flow).</li> <li>Define input arguments or configs along with their types and default values.</li> <li>Select registered resources (such as Model, Global Functions, Prompts, RAGs, etc.) to use in the pipeline definition.</li> <li>Upload custom files/models if required and define pipeline logic by writing code in the Scoring Logic section.</li> <li>Optionally, configure starting examples for human-in-the-loop testing.</li> <li>Add notes and attach documentation if available in the Additional Information section.</li> <li>Finally, click Save to complete the registration process.</li> </ol>"},{"location":"register-and-refine/inventory-management/pipelines/#connect-to-external-agentspipelines","title":"Connect to External Agents/Pipelines","text":"<p>External agents which has been built in another development environment can be connected to the GGX platform. This can be done by connecting to the external agent's API. For example, if an external agent has been built in Vertex AI Agent Playbooks, it can be connected to the GGX platform by connecting to the Vertex AI Agent Playbooks API.</p> <p>Integrations to external agent providers like Vertex AI Agent Playbooks, AgentForce, Microsoft Copilot Studio, Vapi AI, etc. are available an can be configured in the Integration Module. New provider integrations can be added to the platform by extending the existing integration framework.</p> <p>Note:</p> <ul> <li>The output type for Chat pipelines is fixed as a dictionary <code>{\"output\": string, \"context\": custom type}</code>. The context type can be defined in the Formula Section.</li> <li>For chat pipelines, there is a default argument called <code>user_message</code>.</li> <li>For chat pipelines, <code>history</code>, <code>context</code>, and <code>user_message</code> are available by default, maintaining historical messages in the standard OpenAI format (<code>Struct[content: String, role: String]</code>), context for information retention across turns, and <code>user_message</code> for the current message.</li> <li>A <code>cache</code> variable is available by default which can be used to store any intermediate and resuable object/logic to avoid recomputation. One pipeline has a single cache and it is shared across all the sessions.</li> </ul> <p>Registered pipelines can be evaluated in the Pipeline Registry, Human Integrated Testing, or used in downstream pipelines. Live monitoring of pipelines can be done Monitoring Module and Annotation Queues.</p> <p></p>"},{"location":"register-and-refine/inventory-management/pipelines/#pipeline-risk-assessment","title":"Pipeline Risk Assessment:","text":"<p>Platform provides a separate risk assessment tab for pipelines to document the assumptions, risks, and mitigations for the pipeline. The users can document the risk assessment for the pipeline in this tab in various dimensions like : Accuracy, Stability, Ethics and Vulnerability. It also allow attaching the simulation to show the testing done.</p>"},{"location":"register-and-refine/inventory-management/pipelines/#benefits-of-pipeline-registration","title":"Benefits of Pipeline Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered pipelines using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring becomes easier.</li> <li>Maintain a Risk Assessment record of the pipeline and its components.</li> <li>Human Integrated Testing and feedback logging for chat-based pipelines.</li> <li>Extract ready-to-productionize executable artifacts.</li> <li>Better Collaboration for continuous development and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/prompts/","title":"Prompt Registry","text":""},{"location":"register-and-refine/inventory-management/prompts/#what-is-a-prompt","title":"What is a Prompt?","text":"<p>A prompt is a natural language instruction provided to a generative model to direct its response or produce a desired outcome. It may include questions, commands, contextual details, few-shot examples, or partial inputs for the model to complete or extend.</p> <p>A prompt typically includes:</p> <ul> <li>Prompt Template: An instruction containing placeholders.</li> <li>Input Arguments: Dynamic inputs that replace the placeholders.</li> </ul> <p>Note: System prompts might not always have Input Arguments.</p> <p>Additionally, to generate the prompt using the template and input arguments, there should be a:</p> <ul> <li>Creation Logic: Code that generates, retrieves, or formats the input arguments and fills them into the template.</li> </ul> <p>Note: If a prompt contains no processing or formatting logic, the creation logic can directly return the prompt template.</p> <p></p>"},{"location":"register-and-refine/inventory-management/prompts/#managing-prompts-on-the-platform","title":"Managing Prompts on the Platform:","text":"<p>The Prompt Registry organizes all the registered prompts into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/prompts/#registering-a-prompt","title":"Registering a Prompt:","text":"<ol> <li>Click on Create button in Prompt Registry.</li> <li>Fill in important details like Name, Attributes (alias), Properties (Description, Group, Permissible Purpose, Approval Workflow).</li> <li>Select registered resources (like Model, Global Functions, etc.) which can help with prompt creation. These resources can be used in the prompt creation logic section.</li> <li>Define the Input Arguments, Prompt Template, Prompt Creation Logic.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on the Save button to complete the registration process.</li> </ol> <p>The registered prompts can be evaluated in the Prompt Registry or can be used in downstream objects (like RAG, Model, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/prompts/#benefits-of-prompt-registration","title":"Benefits of Prompt Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Perform evaluations using standardized and custom validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> </ul>"},{"location":"register-and-refine/inventory-management/rags/","title":"RAGs","text":""},{"location":"register-and-refine/inventory-management/rags/#what-is-a-rag","title":"What is a RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is an AI approach that helps language models by integrating a retrieval mechanism that fetches relevant external information in real-time. This information allows the model to generate more accurate, up-to-date, and context-aware responses beyond its pre-trained knowledge.</p> <p>The retrieval mechanism in a RAG system typically consists of:</p> <ul> <li>Knowledge Source: A repository of external information. This can include documents, vector databases, Neo4j, or other structured/unstructured data sources.</li> <li>Retrieval Logic: A set of algorithms designed to fetch the most relevant information based on provided inputs. It often leverages embedding models, similarity functions, and ranking techniques to define the retrieval process.</li> </ul> <p>GGX supports the registration of various types of Retrieval Systems:</p> <ul> <li>API-based Retrieval: Communicates with external knowledge sources like Neo4j, vector databases, etc., using APIs to retrieve information from outside environments.</li> <li>Python-based Retrieval: For lightweight Python logic using various libraries or rule-based retrieval systems.</li> <li>Custom Retrieval: Leverages uploaded knowledge sources like CSV, Vector databases, etc., to retrieve information.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/rags/#managing-rags-on-the-platform","title":"Managing RAGs on the platform:","text":"<p>The RAG Registry organizes all the registered RAGs into customized groups at this centralized location, allowing easier tracking, monitoring, and new RAG creation.</p>"},{"location":"register-and-refine/inventory-management/rags/#registering-a-rag","title":"Registering a RAG:","text":"<ol> <li>Click on Create button in RAG Registry.</li> <li>Fill in important details like Name, Attributes (Output Type, Alias), Properties (Group, Permissible Purpose, Description, Approval Workflow).</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select registered resources (like Model, Global Functions, Prompts, etc.) to use in RAG definition.</li> <li>Select Input Type (API-Based, Python-based, or Custom registration).</li> <li>Upload custom knowledge file if required. Define RAG logic by writing code in Retrieval Logic section.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on Save to complete the registration process.</li> </ol> <p>The registered RAGs can be evaluated in the RAG Registry or used in downstream objects (like Models, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/rags/#benefits-of-rag-registration","title":"Benefits of RAG Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered RAGs using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> <li>Fingerprinting of external API connectivity.</li> <li>Better Collaboration for continuous development and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/","title":"Data Assets","text":"<p>The Table Registry records the location, content, and structure of source data tables used for analytics.</p> <p>The table can be registered using one of the following methods:</p> <ul> <li> <p>Connection to a Data Lake: A direct link to a data lake allows specifying the location of the data file. The link must point to a valid PySpark data file.</p> </li> <li> <p>Table Upload: Datasets with fewer rows can be uploaded directly in CSV or Excel format.</p> </li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/#managing-tables-on-the-platform","title":"Managing Tables on the Platform","text":"<p>The Table Registry organizes all the registered tables into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#registering-a-table","title":"Registering a Table:","text":"<ul> <li>Click on Create button in Table Registry.</li> <li>Fill in important details like Name, and Attributes (Alias, Group, Input Type, Location, Description).</li> <li>Select an Input Type (Data Lake or Upload Data) and provide a data link or upload files accordingly.</li> <li>Finally, click on the Save button to complete the registration.</li> </ul> <p>Note: After registering the table, users can Edit and click on the Fetch Columns button to automatically load the table columns and their types.</p> <p>Once the table is registered, data quality can be evaluated through registered Quality Checks or it can be used for validation and testing.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#benefits-of-table-registration","title":"Benefits of Table Registration:","text":"<ul> <li>Automated change history records that tracks all the modifications to the tables.</li> <li>Track the lineage of table usage in downstream applications.</li> <li>Run Quality Checks on the tables.</li> <li>Use tables for validation and testing in a fully auditable manner.</li> <li>Export tables outside the platform when required with a single click.</li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/#what-is-a-quality-check","title":"What is a Quality Check?","text":"<p>The Quality Check enables the analysis of data and the creation of standard or custom reports based on registered tables in the Table Registry. It supports the generation of profiling metrics, descriptive statistics, invalid entry detection, outlier analysis, and other custom reports and metrics to assess data quality effectively before using the data for downstream tasks like running jobs.</p> <p>Note: The Quality Check object currently supports linking only one table at a time, enabling the generation of multiple metrics and reports for a single table per analysis.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#managing-quality-checks-on-the-platform","title":"Managing Quality Checks on the Platform:","text":"<p>The Quality Check Registry organizes all the registered quality checks into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#registering-a-quality-check","title":"Registering a Quality Check:","text":"<ol> <li>Click on Create button in Quality Check registry.</li> <li>Fill in important details like Name, Attributes (Data-Table, Group, Descriptions, Select Data-Columns).</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on the Save button to complete the registration process.</li> </ol>"},{"location":"register-and-refine/inventory-management/table-registry/#benefits-of-quality-check-registration","title":"Benefits of Quality Check Registration:","text":"<ul> <li>Analyze and monitor data using standard and custom reports.</li> <li>Share data analysis and evaluations with other team members.</li> </ul>"},{"location":"register-and-refine/version-management/","title":"Version Management","text":"<p>Version management is the process of systematically tracking, organizing, and controlling changes to an object. A version of an object is created every time the object\u2019s definition is either newly created or subsequently changed.</p>"},{"location":"register-and-refine/version-management/#benefits-of-version-management","title":"Benefits of Version Management?","text":"<ul> <li>Ensures Reproducibility of results and artefacts.</li> <li>Facilitates Collaboration by managing contributions from multiple users.</li> <li>Auditability &amp; Compliance by maintaining a clear history of changes and versions.</li> <li>Allows Rollback &amp; Recovery by reverting to stable versions in case of failures or unintended modifications.</li> <li>Experiment Tracking by enabling comparison of different versions.</li> <li>Supports Continuous Improvement by keeping track of incremental changes and their impact over time.</li> </ul>"},{"location":"register-and-refine/version-management/#maintaining-version-on-the-platform","title":"Maintaining Version on the platform:","text":"<p>A Draft Version (i.e. Version 1) is created as soon as an object is registered on the platform. All the modification done to a Draft Version is automatically logged in the Change History tab. Post approval process, the object can be cloned to create a new draft version (i.e. Version 2) which goes through the same cycle of changes and approvals.</p> <p></p>"},{"location":"register-and-refine/version-management/#version-tracking-within-drafts","title":"Version Tracking within Drafts","text":"<p>Change History is a structured log of modifications made to Generative AI pipelines over time ensuring a clear audit trail of changes and the ability to revert to any previous versions if needed.</p> <p>Maintaining a comprehensive change history is very important for several reasons:</p> <ul> <li>Enhances traceability, allowing teams to track when, why, and by whom changes were made.</li> <li>Improves accountability, ensuring responsible AI deployment and governance.</li> <li>Facilitates debugging, helping teams identify and roll back problematic updates.</li> <li>Ensures compliance with regulations like the EU AI Act (Article 12), which mandates record-keeping for high-risk AI systems.</li> <li>Builds trust by providing transparency into model evolution.</li> </ul> <p>All objects start their lifecycle as a \"Draft\". Drafts typically are modified multiple times before they are approved and locked. To ensure a clear record of these changes, the platform keeps track of all changes being made to a Draft.</p> <p>The history of changes on a given object is recorded in the Change History tab. The Change History is a collection of snapshots:</p> <ul> <li>The platform takes a snapshot of an object every time the object is edited and saved</li> <li>A snapshot is an exact copy of an object at the time of saving</li> <li>A change can be reverted back by restoring the snapshot at that point in time</li> <li>Snapshots can be Named and previewed or even restored as a copy</li> </ul> <p></p>"},{"location":"register-and-refine/version-management/#version-tracking-for-approved-items","title":"Version Tracking for Approved items","text":"<p>Once a Draft is approved - it can not be modified. But by creating a clone (i.e. a new version) new changes can be made to continue working on that item. The new version will be a copy of the object being cloned with the same Name, Alias, and Type.</p> <p>Only the latest approved version of an object can be cloned.</p> <p>By design, all previous uses of the older version of the object will keep using the previous versions - and not be updated to the new version unless it is explicitly updated to use the newer version. This is different compared to changes made to a Draft, as changes in a Draft are immediately propagated downstream.</p>"},{"location":"technology/self-hosting/","title":"Self-Hosted Corridor","text":"<p>Pipeline Hosting</p> <p>For guides on how the analytics and pipelines written in Corridor can be deployed to Production - refer to the Direct to Production guide.</p> <p>Guides that cover the installation, configuration, and scaling of Self-Hosted Corridor instances for analytical use.</p> <ul> <li>Minimum Requirements</li> <li> <p>Installing on your own infrastructure</p> <ul> <li>Google Cloud Platform (GCP)</li> <li>Amazon Web Services (AWS)</li> <li>Microsoft Azure</li> <li>On-Premises or Data Center</li> </ul> </li> <li> <p>Configurations: How to configure your self-hosted instance of Corridor</p> <ul> <li>SSO Integration - Microsoft AD, Okta, Google Workspace, etc.</li> <li>RDBMS - Oracle, MS SQL Server, Postgres, etc.</li> <li>Web Servers - Nginx, Apache, etc.</li> <li>Integrating packages - Wheelhouse, Artifactory, etc.</li> <li>Automated Approval Steps - Jenkins, ServiceNow, JIRA, etc.</li> <li>Data Lakes - HDFS, Hive, Snowflake, etc.</li> <li>Notifications - Email, Slack, Teams, etc.</li> <li>Process Management - Systemd, Supervisor, etc.</li> </ul> </li> <li> <p>Scaling to 100s and 1000s of users</p> <ul> <li>Concurrency - Increasing number of parallel runs</li> <li>Scaling to number of users</li> <li>Backup Management</li> </ul> </li> <li> <p>Hardening your Corridor instance</p> </li> </ul>"},{"location":"technology/self-hosting/#architectural-overview","title":"Architectural Overview","text":"<p>The Analytical layer of Corridor for analysts to test and validate their logic and get the required approvals and compliance checks. The production layer is NOT described here as Corridor is isolated from the Production side.</p> <p>Corridor is divided into various components to keep it modular and enable easy scaling for cloud-based deployments and also to manage high loads without much change. Each of the components can be installed on separate machines or any subset can be installed in the same machine.</p> <p>The components are divided into:</p> <ul> <li>Web Application Server: The web application server for the analytical UI of the platform</li> <li>API Server: The API for business logic</li> <li>API - Celery worker: The worker for asynchronous API tasks</li> <li>Spark - Celery worker: The worker for asynchronous spark tasks</li> <li>Jupyter Notebook: The Jupyter Notebook server for free-form analytical use</li> <li>File Management: The file management server to manage files</li> <li>Metadata Database (SQL RDBMS): The database with all metadata provided in the Web Application</li> <li>Messaging Queue (Redis): The messaging queue to orchestrate worker tasks</li> <li>Authentication Provider: The identity and auth provider for access and permissions</li> <li>Proxy / Load Balancers: Load Balancers / Proxies to simplify the install</li> </ul> <p>Here is a typical network diagram of how the installation would look like: </p>"},{"location":"technology/self-hosting/hardening/","title":"Hardening - Security","text":"<p>This section describes additional setup that would be needed to make Corridor secure. All the below are recommended but optional, and can be configured as needed.</p>"},{"location":"technology/self-hosting/hardening/#data-storage-security","title":"Data Storage Security","text":"<p>Corridor saves data in the following locations:</p> <ul> <li>Data Lake</li> <li>File Management System</li> <li>Metadata Database</li> <li>Redis (only short-term storage)</li> <li>Jupyter Content Manager (For notebooks)</li> </ul> <p>Any data stored in them should be encrypted and backups should be maintained as needed.</p>"},{"location":"technology/self-hosting/hardening/#network-security","title":"Network Security","text":"<p>The following network connections are created in Corridor, and should be secured:</p> <ul> <li>Web Application \u2194\ufe0e API Server: HTTPS connection</li> <li>API Server / API - Celery \u2194\ufe0e File Management: FTPS connection (if using FTP)</li> <li>API Server / API - Celery \u2194\ufe0e Metadata Database: SSL connections to Database</li> <li>API Server / API - Celery / Spark - Celery \u2194\ufe0e Redis: TLS authenticated Redis connection</li> <li>Spark - Celery / Jupyter \u2194\ufe0e Spark: Kerberos</li> <li>Corridor Package \u2194\ufe0e API Server: HTTPS connection</li> <li>Jupyter \u2194\ufe0e Web Application: HTTPS connection</li> <li>End User \u2194\ufe0e Web Application / Jupyter: HTTPS / WSS connection</li> </ul>"},{"location":"technology/self-hosting/hardening/#securing-each-component","title":"Securing each component","text":"<p>This section describes the steps to follow for each of the Corridor components to ensure it can be accessed securely.</p>"},{"location":"technology/self-hosting/hardening/#common","title":"Common","text":"<ul> <li>Ensure that all configuration and installation files are readable only by the user that the process is running with</li> </ul>"},{"location":"technology/self-hosting/hardening/#web-application-server","title":"Web Application Server","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>When setting up HTTPS, also set the JWT_COOKIE_SECURE configuration to ensure JWT cookies are sent in a secure manner</li> <li>Set a strong and unique SECRET_KEY</li> <li>It should use a reliable Authentication Provider (Avoid using the inbuilt authentication provider)</li> </ul>"},{"location":"technology/self-hosting/hardening/#api-server","title":"API Server","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>Set a strong and unique SECRET_KEY</li> <li>Ensure API Keys are set to ensure only authorized access to the APIs</li> </ul>"},{"location":"technology/self-hosting/hardening/#api-celery-worker","title":"API - Celery worker","text":"<p>No specific steps are required for the API - Celery workers as no other component connects to it directly.</p>"},{"location":"technology/self-hosting/hardening/#spark-celery-worker","title":"Spark - Celery worker","text":"<ul> <li>Ensure that the cluster is Kerberized</li> <li>Ensure the standard security practices for Spark are followed as described in the   Spark - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in   front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>Ensure the standard security practices for Jupyter are followed as described in the   Jupyter - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#file-management","title":"File Management","text":"<p>For Local File System: Ensure that the Hard disk being used is encrypted.</p> <p>For FTP: Ensure the FTPS protocol is being used and the underlying data is encrypted</p>"},{"location":"technology/self-hosting/hardening/#metadata-database-sql-rdbms","title":"Metadata Database (SQL RDBMS)","text":"<ul> <li>Ensure that the connection to the SQL database is secured using any of the authentication methods   available to the RDBMS.</li> <li>Ensure that the Database is encrypted.</li> <li>Ensure the standard security practices for the RDBS are followed as described in its documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#messaging-queue-redis","title":"Messaging Queue (Redis)","text":"<p>For Redis:</p> <ul> <li>Use a secure protocol like TLS (if using Redis) when accessing the queue</li> <li>Limit the incoming connections by whitelisting the IPs of the Redis clients</li> <li>Enable the authentication feature in Redis is enabled</li> <li>Ensure the standard security practices for Redis are followed as described in the   Redis - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#authentication-provider","title":"Authentication Provider","text":"<ul> <li>For LDAP: Ensure the secure LDAPS is used to create connections</li> <li>For SAML: Ensure a valid x509 certificate is used to authenticate messages being sent/received</li> </ul>"},{"location":"technology/self-hosting/configurations/approvals/","title":"CI CD Integrations","text":""},{"location":"technology/self-hosting/configurations/approvals/#ci-cd-integrations","title":"CI CD Integrations","text":"<p>There are scenarios when approval of an object is not confined to the reviewers registered on the platform. The client may have an external application, where other stakeholders are already onboarded and would want to review objects on the platform. Corridor provides the ability to interact with such 3<sup>rd</sup> party applications, using API calls, and integrating their review. This can be accomplished by defining a custom handler that uses the base approval handler class exposed by Corridor.</p>"},{"location":"technology/self-hosting/configurations/approvals/#handler-class","title":"Handler class","text":"<p>The user would need to define a <code>CustomApprovalHandler</code> which would inherit Corridor's base approval handler class: <code>corridor_api.config.handlers.ApprovalHandler</code></p> <p>The logic for approval would be defined by the following method(s) inside <code>CustomApprovalHandler</code>.</p> <ul> <li><code>send_action(review, action)</code></li> <li><code>receive_action(review_id, payload)</code></li> </ul>"},{"location":"technology/self-hosting/configurations/approvals/#example","title":"Example","text":"<p>The example focuses on a model, which needs to be sent for review. We can configure what information we want to send to the tool (in <code>send_action</code>). The tool will expose an endpoint that would take that information, create a model entry on their side, carry out the necessary approval process, and send the feedback to us (in <code>receive_action</code>). <code>reviewId</code> is the key and would be used for communications.</p> <pre><code>from corridor_api.config.handlers import ApprovalHandler\n\n\nclass CustomApprovalHandler(ApprovalHandler):\n name = 'external_tool'\n\n    def send_action(self, review, action):\n        '''\n :param review: review_object\n :param action: Action taken by the user on the platform for the given review\n - 'Request Approval'\n - 'Resubmit'\n - 'Cancel'\n - 'Remind'\n - 'Edit' (if the object is in the 'Pending Approval' state and it is edited)\n '''\n url = 'http://externaltool.example.com/cp_review/'  # assuming the 3rd party app is running on PORT: 7006\n review_id = review.id\n object_ = review.object  # `corridor` object\n\n        from corridor import Model\n\n        # if we need to restrict the 3rd party approvals to Models only\n        if not isinstance(object_, Model):\n            raise NotImplementedError(f'{type(object_)} is not expected to be used with \"{self.name}\" tool!!!')\n\n json_info = {\n            'modelId': object_.parent_id,\n            'modelName': object_.name,\n            'modelVersion': object_.version,\n            'modelVersionId': object_.id,\n            'modelGroup': object_.group,\n            'createdBy': object_.created_by,\n            'reviewId': review_id,\n            'responsibilityId': review.responsibility.id,\n            'responsibilityName': review.responsibility.name,\n            'action': action,\n            'comment': review.comment,\n }\n headers = {}  # any headers can be configured (optional)\n res = requests.post(url + str(review_id), json=json_info, headers=headers)\n        return {'status': res.status_code}\n\n    def receive_action(self, review_id, payload):\n        '''\n :param review_id:  id corresponding to the review object\n (this is the same id that was sent by CP when requesting the review)\n :param payload:    payload expects 2 kwargs\n - 'action': one of 'Accept'/'Need Info'/'Need Changes'/'Reject'/'Comment'\n - 'comment': any comment which the external_tool's reviewer makes\n :return:           dictionary with `action` and `comment` for the review with id: `review_id`\n '''\n        # The external app needs to do a POST call with `action` and `comment` as part of the payload.\n        # the endpoint would look like below (assuming corridor-api is running on port 5000):\n        #   `http://localhost:5000/api/v1/models/review/&lt;&lt;reviewId&gt;&gt;/external`\n action = payload.get('action')\n comment = payload.get('comment')\n\n        # do some processing, if required\n comment = 'No comment' if comment is None else comment\n\n        return {'action': action, 'comment': comment}\n</code></pre>"},{"location":"technology/self-hosting/configurations/approvals/#configurations","title":"Configurations","text":"<p>Approval handler-related configurations need to be set in <code>api_config.py</code> along with other configurations. (assuming the <code>CustomApprovalHandler</code> class is defined in the file <code>custom_approval_handler.py</code>).</p> <pre><code>THIRD_PARTY_APPROVALS = {\n    'external_tool': {\n        'handler': 'custom_approval_handler.CustomApprovalHandler',\n },\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/common-configs/","title":"Common Configs","text":"<p>After the installation, there are some configurations that need to be configured for each of the components. This section describes all the available configurations needed for each component to work correctly. Using these configurations the setup can be tweaked as needed.</p> <p>The configurations for each component needs to be present in the corresponding config file:</p> <ul> <li><code>api_config.py</code>: For the API Server and Celery Tasks</li> <li><code>app_config.py</code>: For the Web Application Server</li> <li><code>jupyterhub_config.py</code>: For the Jupyter Notebook (Jupyterhub)</li> <li><code>jupyter_notebook_config.py</code>: For the Jupyter Notebook (Notebook server)</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#configuration-using-environment-variables","title":"Configuration using Environment Variables","text":"<p>It is possible to configure the platform using environment variables instead of (or in combination with) config files mentioned above. This might be more convenient in a cloud based deployment setting or while using a centralized secret management system (like Hashicorp Vault).</p> <p>When working with secret management systems, configurations could be loaded as environment variables during deployment of the platform.</p> <p>When configuring API/APP component via environment variables, prepend the configuration key with <code>CORRIDOR_</code>. Below are some examples for some standard data types,</p> Setting in <code>api_config.py</code> Environment Variable Equivalent <code>LICENSE_KEY = xxxxxxx</code> <code>export CORRIDOR_LICENSE_KEY=xxxxxxx</code> <code>WORKER_PROCESSES = 1</code> <code>export CORRIDOR_WORKER_PROCESSES=1</code> <code>REQUIRE_SIMULATION = False</code> <code>export CORRIDOR_REQUIRE_SIMULATION=false</code> <code>WORKER_QUEUES = ['api', 'spark', 'quick_spark']</code> <code>export CORRIDOR_WORKER_QUEUES=\"['api', 'spark', 'quick_spark']\"</code>"},{"location":"technology/self-hosting/configurations/common-configs/#api","title":"API","text":"<p>The API Configurations help in controlling how the API Server and Celery workers behave.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>LICENSE_KEY</code>: The corridor license-key to use to enable the application</li> <li><code>API_KEYS</code>: The API keys to accept requests from</li> <li><code>SQLALCHEMY_DATABASE_URI</code>: The Database URI to connect to for the Metadata Database</li> <li><code>FS_URI</code>: The FileSystem URI to connect to for File Management</li> <li><code>CELERY_BROKER_URL</code>: The URL of the Celery Broker (The Redis server for task queue management)</li> <li><code>CELERY_RESULT_BACKEND</code>: The URL of the Celery Backend (The Redis server for task queue management)</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#web-application","title":"Web Application","text":"<p>These configurations help in controlling how the Web Application Server behaves.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>SECRET_KEY</code>: Ensure a unique secret key for your setup is used</li> <li><code>REST_API_SERVER_URL</code>: The URL of the API Server for business login and metadata</li> <li><code>REST_API_KEY</code>: The API Key to use when connecting to the API Server</li> <li><code>NOTEBOOK_CONFIGS__link</code>: URL to a notebook solution</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#jupyter","title":"Jupyter","text":"<p>The Jupyter configurations are divided into 2 sections: jupyterhub and jupyter-notebook configurations.</p>"},{"location":"technology/self-hosting/configurations/common-configs/#jupyterhub-configurations","title":"JupyterHub Configurations","text":"<p>The configurations used by the Corridor Platform are the same as the standard Jupyter Hub configurations.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>c.JupyterHub.bind_url</code>: The URL to host JupyterHub on</li> <li><code>c.Authenticator.auth_api_url</code>: The Corridor Web Application Server (When using the Corridor Authentication)</li> <li><code>c.Spawner.env_keep</code>: And environment variables to be kept when spawning the user jupyter-notebooks</li> <li><code>c.Authenticator.auth_api_url</code>: The API for the Authentication. The URL of the Web Application Server.</li> </ul> <p>There are also additional env variables needed by the <code>corridor</code> Python Package:</p> <ul> <li><code>os.environ['CORRIDOR_API_URL']</code>: The Corridor API Server URL</li> <li><code>os.environ['CORRIDOR_API_KEY']</code>: The Corridor API Key to use (if set)</li> </ul>"},{"location":"technology/self-hosting/configurations/database/","title":"Metadata: Database Setup","text":""},{"location":"technology/self-hosting/configurations/database/#metadata-database-setup","title":"Metadata: Database Setup","text":"<p>This section describes the different database options available for the Metadata Database. Choosing the right database is critical, as it will impact the usability of the API and Web Application of the users. The metadata database stores all the information entered into the Web Application in a structured so it can be efficiently used by other components.</p> <p>The platform currently supports the following databases:</p> <ul> <li>Oracle Database - Industry-standard</li> <li>SQLite - Meant for testing only</li> <li>MS SQL - Enterprise-level solution</li> <li>PostgreSQL - Open-source and reliable</li> </ul>"},{"location":"technology/self-hosting/configurations/database/#postgresql","title":"POSTGRESQL","text":"<p>To use postgresql, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'postgresql://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;'\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#sqlite","title":"SQLite","text":"<p>Supported versions: sqlite 3+</p> <p>Sqlite is an easy to setup database which is file-based. To use sqlite with Corridor, set the following configuration:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'sqlite:///&lt;filepath&gt;\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#oracle-db","title":"Oracle DB","text":"<p>Supported versions: Oracle DB 19+</p> <p>To use oracle, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'oracle://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;'\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#ms-sql","title":"MS SQL","text":"<p>Supported versions: SQL Server 2016+</p> <p>This required the the unixODBC devel libraries (<code>yum install unixODBC-devel</code>) and SQL Server ODBC driver to be installed.</p> <p>To use mssql, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'mssql+pyodbc://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;?driver=ODBC+Driver+17+for+SQL+Server'\n</code></pre>"},{"location":"technology/self-hosting/configurations/datalake/","title":"Datalake Integration","text":""},{"location":"technology/self-hosting/configurations/datalake/#datalake-integration","title":"Datalake Integration","text":"<p>Corridor provides the ability to connect to different kinds of data lakes which could have data saved as <code>parquet</code>, <code>orc</code>, <code>avro</code>, <code>hive tables</code> or any in any other format.</p> <p>The user could define a custom file handler that would have the logic to read/write to/from the data lake.</p>"},{"location":"technology/self-hosting/configurations/datalake/#example","title":"Example","text":"<p>The example focuses on creating a data source handler to read from hive tables. The user needs to inherit the base class: <code>DataSourceHandler</code> and define the functions:</p> <ul> <li><code>read_from_location</code></li> <li><code>write_to_location</code></li> </ul> <pre><code>from corridor_api.config.handlers import DataSourceHandler\n\n\nclass HiveTable(DataSourceHandler):\n    \"\"\"\n    Consider a case where every data table is a table in the Hive metastore.\n    The table `location` identifier is the table name.\n    \"\"\"\n\n    name = 'hive'\n    write_format = 'parquet'\n\n    def read_from_location(self, location, nrow=None):\n        try:\n            import findspark\n\n            findspark.init()\n            import pyspark\n        except ImportError:\n            import pyspark\n\n        spark = pyspark.sql.SparkSession.builder.getOrCreate()\n\n        data = spark.table(location)\n        if nrow is not None:\n            data = data.limit(nrow)\n        return data\n\n    def write_to_location(self, data, location, mode='error'):\n        return data.write.format(self.write_format).saveAsTable(location)\n</code></pre>"},{"location":"technology/self-hosting/configurations/datalake/#configuration","title":"Configuration","text":"<p>Once the handler class is created, it can be set up in <code>api_config.py</code> as below: (assuming the handler is defined in a file called <code>hive_table_hander.py</code> alongside <code>api_config.py</code>)</p> <pre><code>LAKE_DATA_SOURCE_HANDLER = 'hive_table_handler.HiveTable'\n</code></pre>"},{"location":"technology/self-hosting/configurations/notifications/","title":"Email Notifications","text":""},{"location":"technology/self-hosting/configurations/notifications/#email-notifications","title":"Email Notifications","text":"<p>Corridor provides an option to send email notifications to the user on their registered email id when an event occurs on the platform (e.g. completion of simulation, approval request for an object).</p> <p>The different events for which notifications are triggered on the Platform:</p> <ul> <li>Completion or failure of job run</li> <li>Workflow Status change of an object</li> <li>Review status changes or comments added during approval process</li> <li>Sharing of an object</li> </ul> <p>Note</p> <p>The user cannot customize which notifications will be sent as email.</p>"},{"location":"technology/self-hosting/configurations/notifications/#configuration","title":"Configuration","text":"<p>The email notifications can be configured in <code>api_config.py</code> file with the parameter: <code>NOTIFICATION_PROVIDERS</code>. The value should be a dictionary with the key being <code>email</code>. The value for <code>email</code> should be a dictionary again with the email configuration details. The email configuration details include:</p> <ul> <li><code>from</code>: The email id from which the notifications are to be sent</li> <li><code>username</code>: Username corresponding to the email id</li> <li><code>password</code>: The password for the email id</li> <li><code>host</code>: The host of the SMTP server</li> <li><code>port</code>: The port number to use</li> <li><code>ssl</code>: Should ssl be used</li> <li><code>html</code>: Should the email be parsed as an HTML file</li> </ul>"},{"location":"technology/self-hosting/configurations/notifications/#example","title":"Example","text":"<pre><code>NOTIFICATION_PROVIDERS = {\n    'email': {\n        'from': 'user@example.com',\n        'username': 'user',\n        'password': 'password',\n        'host': 'smtp.server.com',\n        'port': 465,\n        'ssl': True,\n        'html': True,\n    },\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/packages/","title":"Additional Libraries","text":""},{"location":"technology/self-hosting/configurations/packages/#additional-libraries","title":"Additional Libraries","text":"<p>Corridor provides an option to allow the users to be able to use additional libraries which are not available out of the box.</p>"},{"location":"technology/self-hosting/configurations/packages/#configuration","title":"Configuration","text":"<p>There are 5 steps which have to be followed so that the user is able to validate the definition and run successful jobs on the platform.</p> <ol> <li>Install the library in the virtual environment of API servers.</li> <li>Install the library in the virtual environment of Worker-API servers.</li> <li>Install the library in the virtual environment of Worker-Spark servers.</li> <li>Install the library on the spark cluster.</li> <li>Add the library to the Allowed Python Imports in the <code>Platform Settings</code> tab in the platform UI</li> </ol>"},{"location":"technology/self-hosting/configurations/process-management/","title":"Process Management","text":""},{"location":"technology/self-hosting/configurations/process-management/#process-management","title":"Process Management","text":"<p>For ease of maintenance and monitoring, it is recommended to use a process management tool to ensure the component daemons are running correctly. Using a process manager can simplify restarts, reboots, status-checks, logging, and configurations.</p> <p>The process management tools that are frequently used are Systemd, init-script, etc. In general, it is recommended to use the tool that the OS provides to handle these tasks.</p> <p>To simplify the installation, Supervisor can also be used, which provides application-level process management that is independent of the host setup.</p> <p>The tools available for process management require <code>sudo</code> access which might not be an option in some situations. Corridor has its own process manager for each of the components, which uses daemonisation to handle long-running, background processes.</p>"},{"location":"technology/self-hosting/configurations/process-management/#daemon-mode","title":"Daemon Mode","text":"<p>No additional configurations are required when running Corridor processes using Corridor-Daemons. We can use the below set of commands to start/stop/check_status. The logfile and pidfile for each process can be saved at custom locations by using the parameters:</p> <ul> <li>logfile: location of the log file</li> <li>pidfile: location of the pid file</li> </ul> <p>The default logfile directory is: <code>INSTALL_DIR/instances/INSTANCE/logs</code></p> <p>The default pidfile directory is: <code>INSTALL_DIR/instances/INSTANCE/pids</code></p> <p>To start the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon start\nINSTALL_DIR/venv-api/corridor-worker daemon start\nINSTALL_DIR/venv-app/corridor-app daemon start\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon start\n</code></pre> <p>To stop the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon stop\nINSTALL_DIR/venv-api/corridor-worker daemon stop\nINSTALL_DIR/venv-app/corridor-app daemon stop\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon stop\n</code></pre> <p>To check the status of the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon status\nINSTALL_DIR/venv-api/corridor-worker daemon status\nINSTALL_DIR/venv-app/corridor-app daemon status\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon status\n</code></pre>"},{"location":"technology/self-hosting/configurations/process-management/#supervisor","title":"Supervisor","text":"<p>To use corridor with Supervisor, some useful configurations are:</p> <ul> <li><code>command</code>: The command to execute. Note, if 2 commands need to be executed, use <code>bash -c \"command1; command2\"</code></li> <li><code>stdout_logfile</code>: Log file location for the stdout logs (<code>%(program_name)s</code> and <code>%(process_num)01d</code> can be used as variables)</li> <li><code>stderr_logfile</code> or <code>redirect_stderr</code>: Log file location for the stderr logs, or redirect all the stderr logs to the stdout stream and hence have a common file for both</li> <li><code>user</code>: The user to run the process as</li> <li><code>environment</code>: The environment variables to be set before the process is run</li> <li><code>numprocs</code>: The number of processes to run</li> </ul> <p>Here are some example configuration files for the Corridor components:</p> <p>Web Application server:</p> <pre><code>[program:corridor-app]\ncommand=INSTALL_DIR/venv-app/bin/corridor-app run\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>API server:</p> <pre><code>[program:corridor-api]\ncommand=\n  bash -c \"INSTALL_DIR/venv-api/bin/corridor-api db upgrade &amp;&amp; INSTALL_DIR/venv-api/bin/corridor-api run\"\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>API - Celery worker:</p> <pre><code>[program:corridor-worker-api]\ncommand=INSTALL_DIR/venv-api/bin/corridor-worker run --queue api\nenvironment=\n  C_FORCE_ROOT=1\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>Spark - Celery worker:</p> <pre><code>[program:corridor-worker-spark]\ncommand=INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark\nenvironment=\n  C_FORCE_ROOT=1\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>Jupyter Notebook:</p> <pre><code>[program:corridor-jupyter]\ncommand=INSTALL_DIR/venv-jupyter/bin/corridor-jupyter run\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre>"},{"location":"technology/self-hosting/configurations/process-management/#systemd","title":"Systemd","text":"<p>Many Linux OS like RHEL have systemd pre-installed. To use systemd, the following steps need to be followed:</p> <ul> <li>Add service file to systemd services folder. For example: <code>/etc/systemd/system/corridor.service</code></li> <li>To start service: <code>sudo systemctl start corridor</code>   And to run the service on startup: <code>sudo systemctl enable corridor</code></li> </ul> <p>Here are some example configuration files for the Corridor components:</p> <p>Web Application server:</p> <pre><code>[Unit]\nDescription=Corridor Web Application\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-app/bin/corridor-app run \\\n  &gt;&gt; /var/log/corridor/corridor-app.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>API server:</p> <pre><code>[Unit]\nDescription=Corridor API\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-api run \\\n  &gt;&gt; /var/log/corridor/corridor-api.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>API - Celery worker:</p> <pre><code>[Unit]\nDescription=Corridor Worker API\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-worker run --queue api \\\n  &gt;&gt; /var/log/corridor/corridor-worker-api.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Spark - Celery worker:</p> <pre><code>[Unit]\nDescription=Corridor Worker Spark\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark \\\n  &gt;&gt; /var/log/corridor/corridor-worker-spark.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Jupyter Notebook:</p> <pre><code>[Unit]\nDescription=Corridor Jupyter\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-jupyter run \\\n  &gt;&gt; /var/log/corridor/corridor-jupyter.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"technology/self-hosting/configurations/saml/","title":"SAML","text":""},{"location":"technology/self-hosting/configurations/saml/#saml","title":"SAML","text":"<p>While an internal authentication is available for simple and quick installations. It is recommended to use an enterprise-grade Identity Provider (IDP) to follow the infosec requirements for your organization. Corridor can integrate into IDPs and seamlessly be a tool in your organization.</p> <p>This section describes the use of SAML (Security Assertion Markup Language) for authentication. By using SAML, it is easy to ensure that the Platform is available only to users who are authorized to use it. It makes having a centralized Identity Provider hassle-free and ensures that the standard security practices like Single Sign-On, 2-factor Authentication, etc. are consistently applied to all the organization's applications.</p>"},{"location":"technology/self-hosting/configurations/saml/#setup","title":"Setup","text":"<p>To set the login based on SAML, the following information is required:</p> <p>From the IDP:</p> <ul> <li>SSO URL: The URL endpoint to initiate Single Sign On requests   Example: <code>http://&lt;idp_domain&gt;/saml/&lt;app_id&gt;/sso</code></li> <li>Entity ID: The URL endpoint to fetch the SAML metadata   Example: <code>http://&lt;idp_domain&gt;/saml/&lt;app_id&gt;</code></li> <li>Name ID: Unique ID to identify each user</li> <li>Certificate: The X509 certificate to used to ensure any messages sent/received are trusted</li> </ul> <p>From Corridor (Service Provider):</p> <ul> <li>ACS URL: <code>http://&lt;sp_domain&gt;/api/v1/saml/acs</code></li> <li>Entity ID: <code>http://&lt;sp_domain&gt;/api/v1/saml/metadata</code></li> <li>Start URL: <code>http://&lt;sp_domain&gt;/api/v1/users/saml/sso</code></li> </ul> <p>On completing the Sign On flow on the IDP side, the information returned to Corridor should contain:</p> <ul> <li>The Name ID</li> <li> <p>The following attributes:</p> <ul> <li>Email (The Attribute's name can be configured with <code>SAML_EMAIL_ATTRIBUTE</code>)</li> <li>List of Roles/Groups of the user (The Attribute's name can be configured with <code>SAML_ROLE_ATTRIBUTE</code>)</li> </ul> </li> </ul>"},{"location":"technology/self-hosting/configurations/saml/#configurations","title":"Configurations","text":"<p>In the API configurations, the following configurations need to be set:</p> <ul> <li><code>SAML_ENABLED = True</code>   Needs to be set to enable SAML as the method of authentication for login.</li> <li><code>SAML_SETTINGS = {...}</code>   Needs to be set as described in the configurations section to connect to the SP and IDP.   The SAML_SETTINGS is a dictionary with the following information defining the SP and IDP information:</li> </ul> <pre><code>{\n    # If strict is True, then the Python Toolkit will reject unsigned\n    # or unencrypted messages if it expects them to be signed or encrypted.\n    # Also it will reject the messages if the SAML standard is not strictly\n    # followed. Destination, NameId, Conditions ... are validated too.\n    \"strict\": true,\n\n    # Enable debug mode (outputs errors).\n    \"debug\": true,\n\n    # Service Provider Data that we are deploying.\n    \"sp\": {\n        # Identifier of the SP entity  (must be a URI)\n        \"entityId\": \"https://&lt;sp_domain&gt;/saml/metadata/\",\n\n        # Specifies info about where and how the &lt;AuthnResponse&gt; message MUST be\n        # returned to the requester, in this case our SP.\n        \"assertionConsumerService\": {\n            # URL Location where the &lt;Response&gt; from the IdP will be returned\n            \"url\": \"https://&lt;sp_domain&gt;/saml/acs\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\"\n },\n\n        # Specifies info about where and how the &lt;Logout Response&gt; message MUST be\n        # returned to the requester, in this case, our SP.\n        \"singleLogoutService\": {\n            # URL Location where the &lt;Response&gt; from the IdP will be returned\n            \"url\": \"https://&lt;sp_domain&gt;/saml/slo\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n        # Specifies the constraints on the name identifier to be used to\n        # represent the requested subject.\n        \"NameIDFormat\": \"urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified\",\n\n        # The x509cert and privateKey of the SP\n        'x509cert': '',\n        'privateKey': ''\n },\n\n    # Identity Provider Data that we want connected with our SP.\n    \"idp\": {\n        # Identifier of the IdP entity  (must be a URI)\n        \"entityId\": \"https://&lt;idp_domain&gt;/saml/metadata\",\n\n        # SSO endpoint info of the IdP. (Authentication Request protocol)\n        \"singleSignOnService\": {\n            # URL Target of the IdP where the Authentication Request Message\n            # will be sent.\n            \"url\": \"https://&lt;idp_domain&gt;/saml/sso\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n\n        # SLO endpoint info of the IdP.\n        \"singleLogoutService\": {\n            # URL Location of the IdP where SLO Request will be sent.\n            \"url\": \"https://&lt;idp_domain&gt;/saml/sls\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n\n        # Public x509 certificate of the IdP\n        \"x509cert\": \"&lt;connector_cert&gt;\"\n        # Instead of using the whole x509cert you can use a fingerprint\n        # (openssl x509 -noout -fingerprint -in \"idp.crt\" to generate it)\n        # \"certFingerprint\": \"\"\n\n }\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/web-servers/","title":"Web Server Setup","text":""},{"location":"technology/self-hosting/configurations/web-servers/#web-server-setup","title":"Web Server Setup","text":""},{"location":"technology/self-hosting/configurations/web-servers/#nginx-configurations","title":"Nginx Configurations","text":"<p>This section described how to use nginx (https://nginx.org/en/) as a web server. The Platform's server components can be made highly performant by using the lightweight Nginx as a reverse-proxy along with a WSGI server. Using nginx enabled the server to scale to a large number of users with ease.</p> <p>To use Nginx, it needs to be installed in the system and the daemon should be running with the appropriate site configurations setup.</p> <p>Here is an example nginx configuration:</p> <pre><code>server {\n  listen 80;\n  server_name localhost 0.0.0.0;\n\n  client_max_body_size 100m;\n  gzip on;\n  gzip_vary on;\n  gzip_min_length 10240;\n  gzip_proxied expired no-cache no-store private auth;\n  gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/javascript application/xml application/json ;\n  gzip_disable \"MSIE [1-6]\\.\";\n\n  location / {\n    proxy_set_header   X-Real-IP        $remote_addr;\n    proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\n    proxy_set_header   X-Forwarded-Proto $scheme;\n    proxy_set_header   Host             $http_host;\n\n    proxy_pass http://localhost:5002;\n    proxy_read_timeout 3600;\n  }\n\n  location /jupyter {\n    proxy_set_header   X-Real-IP        $remote_addr;\n    proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\n    proxy_set_header   X-Forwarded-Proto $scheme;\n    proxy_set_header   Host             $http_host;\n\n    proxy_pass http://localhost:5003;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_read_timeout 86400;\n  }\n}\n</code></pre> <p>Note</p> <p>If the nginx is not listening on port 80 - but listens on another port, the <code>proxy_set_header</code> for <code>Host</code> may have to be modified to <code>proxy_set_header Host $http_host</code> to ensure the correct host information is passed.</p> <p>Note: Permission issues</p> <p>In case of permission issues, ensure user permissions and SELinux is set up correctly.</p> <p>Note: Large file uploads</p> <p>If large files are expected to be uploaded, there are two settings that need to be modified  \u2003 1. add <code>proxy_request_buffering off</code> \u2003 2. add <code>client_max_body_size 0;</code> to server directive which expects large file size payload. This will disable nginx from buffering the large payload files and optimize disk space consumption.</p>"},{"location":"technology/self-hosting/configurations/web-servers/#apache-configurations","title":"Apache Configurations","text":"<p>To setup a secure connection, update the <code>/etc/httpd/sites-available/corridorapp.conf</code> file as below:</p> <pre><code>&lt;VirtualHost *:443&gt;\n    SSLEngine On\n    SSLCertificateFile /certs/app.crt\n    SSLCertificateKeyFile /certs/app.key\n    SSLCertificateChainFile /certs/ca.crt\n    ServerName www.corridorapp.com\n    ServerAlias corridorapp\n    ProxyPass / http://localhost:5002/\n    ProxyPassReverse / http://localhost:5002/\n    ErrorLog /var/www/corridorapp/log/error.log\n    CustomLog /var/www/corridorapp/log/requests.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note</p> <p>If the httpd is not listening on port 80 (443 for SSL) - but listens on another port, the corresponding port has to be added along with <code>VirtualHost</code> keyword and the <code>Listen</code> param's value has to be appropriately updated in <code>/etc/httpd/conf/httpd.conf</code> (<code>/etc/httpd/conf.d/ssl.conf</code> for SSL).</p> <p>Note: Permission issues</p> <p>In case of permission issues, ensure user permissions and SELinux is set up correctly.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/","title":"Minimum Requirements","text":"<p>This section describes the minimum requirements that are needed for a Corridor Installation.</p> <p>Broadly, the components involved are:</p> <ul> <li>Web Application &amp; Worker</li> <li>Spark Worker</li> <li>Jupyter Notebook</li> <li>File Management</li> <li>Metadata Database (SQL RDBMS)</li> <li>Redis - Messaging Queue</li> </ul> <p>For very simple installations, all of these could be installed on the same machine, we recommend keeping them separate to simplify scalability needs.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#web-application","title":"Web Application","text":"<p>A flask application which serves the User Interface and Web APIs which are accessible to users via the browser. It also includes a worker process for long running tasks in the API. This component has 2 processes: <code>corridor-app</code> and <code>corridor-worker</code></p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements","title":"Requirements","text":"<ul> <li>RAM: 4 GB</li> <li>Processor: 4 CPU</li> <li>Installation storage space: 20 GB</li> <li>Python 3.11+</li> </ul> <p>Optional:</p> <ul> <li>Web Server - Example: Nginx</li> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#spark-worker","title":"Spark Worker","text":"<p>Worker to handle any jobs triggered by users which are asynchronously. It is recommended to have at least 2 workers and increase concurrency as required.</p> <p>Note</p> <p>This needs to be installed on a machine that is configured as a Spark Gateway (i.e. A master node or an edge node of the cluster). This is not the data nodes of the cluster itself. The worker process should be able to import the <code>pyspark</code> module.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_1","title":"Requirements","text":"<ul> <li>RAM: 16 GB</li> <li>Processor: 8 CPU</li> <li>HDFS storage space: 500 GB (depends on the data being processed, HDFS space to handle shuffles need to be considered too)</li> <li>Python 3.11+</li> <li>Java 8+</li> <li>Spark 3.3+</li> </ul> <p>Optional:</p> <ul> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>A notebook for free-form analytical usage. We provide Jupyter Notebooks out-of-the-box but can integrate with existing notebook solutions too.</p> <p>Note</p> <p>This needs to be installed on a machine that is configured as a Spark Gateway (i.e. A master node or an edge node of the cluster). This is not the data nodes of the cluster itself. The jupyter notebook kernel should be able to import the <code>pyspark</code> module.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_2","title":"Requirements","text":"<ul> <li>RAM: 4 GB for base services and more as per usage by users</li> <li>Processor: 4 CPU and more as per usage by users</li> <li>Installation storage space: 10 GB</li> <li>Python 3.11+</li> <li>Spark 3.3+</li> </ul> <p>Optional:</p> <ul> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#file-management","title":"File Management","text":"<p>A file system management to store and retrieve files. A NAS storage that can be mounted on all servers and be accessible by all services is ideal.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_3","title":"Requirements","text":"<ul> <li>File storage space: 50 GB</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#metadata-database","title":"Metadata Database","text":"<p>This serves as an internal RDBMS to store the state of the application and various user information.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_4","title":"Requirements","text":"<ul> <li>RAM: 2 GB</li> <li>Processor: 2 CPU</li> <li> <p>Database storage space: 5 GB</p> </li> <li> <p>SQL Databases supported:</p> <ul> <li>Oracle 19+</li> <li>MSSQL 2016+</li> <li>Postgres 11.7+</li> </ul> </li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#redis-messaging-queue","title":"Redis - Messaging Queue","text":"<p>A low-latency task queue to send and receive information about the asynchronous tasks.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_5","title":"Requirements","text":"<ul> <li>RAM: 1 GB</li> <li>Processor: 1 CPU</li> <li>DB Snapshots storage space: 10 GB</li> <li>Redis 4+</li> </ul>"},{"location":"technology/self-hosting/installation/aws/","title":"Install on Amazon Web Services (AWS)","text":"<p>This guide provides an overview of deploying Corridor on Amazon Web Services. Corridor supports three deployment approaches on AWS.</p>"},{"location":"technology/self-hosting/installation/aws/#deployment-options","title":"Deployment Options","text":""},{"location":"technology/self-hosting/installation/aws/#option-1-kubernetes-eks","title":"Option 1: Kubernetes - EKS","text":"<p>Deploy Corridor on Amazon Elastic Kubernetes Service (EKS) for a cloud-native, containerized deployment.</p> <p>Best for:</p> <ul> <li>Organizations with Kubernetes expertise</li> <li>Multi-tenant deployments with namespace isolation</li> <li>Auto-scaling and high availability requirements</li> <li>Modern cloud-native infrastructure</li> </ul> <p>View EKS Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/aws/#option-2-serverless-ecs-fargate","title":"Option 2: Serverless - ECS Fargate","text":"<p>Deploy Corridor using AWS Fargate for a serverless container deployment.</p> <p>Best for:</p> <ul> <li>Organizations wanting serverless infrastructure</li> <li>Minimal infrastructure management</li> <li>Variable workload patterns</li> </ul> <p>View Fargate Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/aws/#option-3-virtual-machines-ec2","title":"Option 3: Virtual Machines - EC2","text":"<p>Deploy Corridor on Amazon EC2 instances for a traditional VM-based deployment.</p> <p>Best for:</p> <ul> <li>Organizations preferring VM-based infrastructure</li> <li>Direct control over the operating system</li> <li>Traditional IT infrastructure patterns</li> <li>Custom hardware requirements</li> </ul> <p>View EC2 Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/aws/#common-aws-services","title":"Common AWS Services","text":"<p>All deployment options utilize these AWS managed services:</p> <ul> <li>RDS: PostgreSQL database for metadata storage</li> <li>S3: Object storage for file management (or EFS for NFS)</li> <li>Application Load Balancer: HTTP(S) load balancing</li> <li>Route 53: Domain name management</li> <li>VPC: Virtual private cloud networking</li> </ul>"},{"location":"technology/self-hosting/installation/aws/#choosing-the-right-option","title":"Choosing the Right Option","text":"Factor EKS (Kubernetes) ECS Fargate EC2 (VMs) Complexity Higher Medium Lower Scalability Automatic Automatic Manual Multi-tenancy Native (namespaces) Task-based Separate VMs Operational Overhead Lower (managed K8s) Lowest Higher Deployment Speed Fast Fastest Moderate Infrastructure Management Minimal None Full Cost Model Node-based Per task Per instance Kubernetes Skills Required Yes No No"},{"location":"technology/self-hosting/installation/aws/aws-ec2/","title":"Install on AWS - EC2","text":"<p>This guide provides an overview of deploying Corridor on a single Amazon EC2 instance.</p>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#background","title":"Background","text":"<p>Corridor can be deployed on a single EC2 instance running all components (app, api, workers, jupyter). This approach provides a simple deployment model suitable for organizations that prefer traditional VM-based infrastructure.</p>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/aws/aws-ec2/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with appropriate permissions</li> <li>AWS CLI installed and configured</li> <li>SSH access to EC2 instance</li> <li>Access to Corridor installation bundle</li> <li>Sufficient AWS service quotas</li> <li>Minimum Requirements and System Dependencies are met</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#required-aws-services","title":"Required AWS Services","text":"<ol> <li> <p>Amazon EC2: Virtual machine for running all Corridor components</p> </li> <li> <p>Instance size based on Minimum Requirements</p> </li> <li>Recommended: t3.2xlarge or larger</li> <li> <p>EBS volume for local storage</p> </li> <li> <p>Amazon RDS: PostgreSQL database for metadata</p> </li> <li>PostgreSQL 14+ recommended</li> <li>Multi-AZ configuration (for production)</li> <li>Automated backups enabled</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#optional-aws-services","title":"Optional AWS Services","text":"<ul> <li>Route 53: For domain management</li> <li>Secrets Manager: For storing sensitive configuration</li> <li>CloudWatch: For logging and monitoring</li> <li>AWS WAF: For DDoS protection and WAF</li> <li>CloudFront: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#architecture-overview","title":"Architecture Overview","text":"<pre><code>EC2 Instance (t3.2xlarge)\n\u251c\u2500\u2500 Corridor Components\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-api (API Server)\n\u2502   \u251c\u2500\u2500 corridor-worker-api (API Worker)\n\u2502   \u251c\u2500\u2500 corridor-worker-spark (Spark Worker)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Local Storage\n    \u2514\u2500\u2500 EBS Volume (/opt/corridor)\n\nAmazon RDS\n\u2514\u2500\u2500 PostgreSQL Database\n    \u251c\u2500\u2500 Metadata\n    \u2514\u2500\u2500 Application Data\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#installation-steps","title":"Installation Steps","text":""},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-1-launch-ec2-instance","title":"Step 1: Launch EC2 Instance","text":"<pre><code># Launch EC2 instance\naws ec2 run-instances \\\n  --image-id ami-0123456789abcdef0 \\\n  --instance-type t3.2xlarge \\\n  --key-name your-key \\\n  --security-group-ids sg-xxxxx \\\n  --subnet-id subnet-xxxxx \\\n  --block-device-mappings '[\n    {\n      \"DeviceName\": \"/dev/xvda\",\n      \"Ebs\": {\n        \"VolumeSize\": 100,\n        \"VolumeType\": \"gp3\"\n      }\n    }\n  ]' \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=corridor-server}]'\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-2-create-rds-instance","title":"Step 2: Create RDS Instance","text":"<pre><code># Create RDS instance\naws rds create-db-instance \\\n  --db-instance-identifier corridor-db \\\n  --db-instance-class db.t3.xlarge \\\n  --engine postgres \\\n  --engine-version 14.9 \\\n  --master-username corridor \\\n  --master-user-password &lt;secure-password&gt; \\\n  --allocated-storage 100 \\\n  --storage-type gp3 \\\n  --multi-az \\\n  --vpc-security-group-ids sg-xxxxx \\\n  --db-subnet-group-name corridor-db-subnet\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-3-install-system-dependencies","title":"Step 3: Install System Dependencies","text":"<p>SSH into the EC2 instance and run:</p> <pre><code># Update system\nsudo yum update -y\n\n# Install dependencies\nsudo yum install -y \\\n    python3.11 \\\n    python3.11-devel \\\n    java-1.8.0-openjdk \\\n    redis \\\n    nginx \\\n    unzip\n\n# Start and enable Redis\nsudo systemctl start redis\nsudo systemctl enable redis\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-4-install-corridor-components","title":"Step 4: Install Corridor Components","text":"<ol> <li>Extract the installation bundle:</li> </ol> <pre><code>cd /tmp\nunzip corridor-bundle.zip\n</code></pre> <ol> <li>Install each component:</li> </ol> <pre><code># Install Web Application Server\nsudo ./corridor-bundle/install app -i /opt/corridor\n\n# Install API Server\nsudo ./corridor-bundle/install api -i /opt/corridor\n\n# Install API Worker\nsudo ./corridor-bundle/install worker-api -i /opt/corridor\n\n# Install Spark Worker\nsudo ./corridor-bundle/install worker-spark -i /opt/corridor\n\n# Install Jupyter\nsudo ./corridor-bundle/install jupyter -i /opt/corridor\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-5-configure-components","title":"Step 5: Configure Components","text":"<ol> <li>Update API configuration in <code>/opt/corridor/instances/default/config/api_config.py</code>:</li> </ol> <pre><code>SQLALCHEMY_DATABASE_URI = \"postgresql://corridor:password@corridor-db.xxxxx.region.rds.amazonaws.com:5432/corridor\"\n</code></pre> <ol> <li>Initialize the database:</li> </ol> <pre><code>/opt/corridor/venv-api/bin/corridor-api db upgrade\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-6-create-service-files","title":"Step 6: Create Service Files","text":"<p>Create systemd service files for each component:</p> <ol> <li>Web Application Server (<code>/etc/systemd/system/corridor-app.service</code>):</li> </ol> <pre><code>[Unit]\nDescription=Corridor Application Server\nAfter=network.target redis.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-app/bin/corridor-app run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>API Server (<code>/etc/systemd/system/corridor-api.service</code>):</li> </ol> <pre><code>[Unit]\nDescription=Corridor API Server\nAfter=network.target redis.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-api/bin/corridor-api run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>API Worker (<code>/etc/systemd/system/corridor-worker-api.service</code>):</li> </ol> <pre><code>[Unit]\nDescription=Corridor API Worker\nAfter=network.target redis.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue api\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Spark Worker (<code>/etc/systemd/system/corridor-worker-spark.service</code>):</li> </ol> <pre><code>[Unit]\nDescription=Corridor Spark Worker\nAfter=network.target redis.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue spark --queue quick_spark\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ol> <li>Jupyter (<code>/etc/systemd/system/corridor-jupyter.service</code>):</li> </ol> <pre><code>[Unit]\nDescription=Corridor Jupyter\nAfter=network.target\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nExecStart=/opt/corridor/venv-jupyter/bin/corridor-jupyter run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#step-7-start-services","title":"Step 7: Start Services","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable services\nsudo systemctl enable corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n\n# Start services\nsudo systemctl start corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/aws/aws-ec2/#service-management","title":"Service Management","text":"<pre><code># Check service status\nsudo systemctl status corridor-app\nsudo systemctl status corridor-api\nsudo systemctl status corridor-worker-api\nsudo systemctl status corridor-worker-spark\nsudo systemctl status corridor-jupyter\nsudo systemctl status redis\n\n# Restart services\nsudo systemctl restart corridor-app\nsudo systemctl restart corridor-api\nsudo systemctl restart corridor-worker-api\nsudo systemctl restart corridor-worker-spark\nsudo systemctl restart corridor-jupyter\nsudo systemctl restart redis\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnet with NAT Gateway</li> <li>Use IAM Instance Profile for AWS service access</li> <li>Configure Security Groups for instance access</li> <li>Enable Systems Manager Session Manager for SSH</li> <li>Store secrets in AWS Secrets Manager</li> <li>Enable CloudWatch Agent for monitoring</li> <li>Configure RDS encryption at rest</li> <li>Enable automated backups for RDS</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-ec2/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<pre><code># EC2 Instance\nresource \"aws_instance\" \"corridor\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.2xlarge\"\n  subnet_id     = aws_subnet.private.id\n\n  root_block_device {\n    volume_size = 100\n    volume_type = \"gp3\"\n    encrypted   = true\n  }\n\n  user_data = file(\"init.sh\")\n\n  tags = {\n    Name = \"corridor-server\"\n  }\n}\n\n# RDS Instance\nresource \"aws_db_instance\" \"corridor\" {\n  identifier           = \"corridor-db\"\n  engine              = \"postgres\"\n  engine_version      = \"14.9\"\n  instance_class      = \"db.t3.xlarge\"\n  allocated_storage   = 100\n  storage_type        = \"gp3\"\n  storage_encrypted   = true\n\n  db_name             = \"corridor\"\n  username           = \"corridor\"\n  password           = var.db_password\n\n  multi_az           = true\n  publicly_accessible = false\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"Mon:04:00-Mon:05:00\"\n\n  vpc_security_group_ids = [aws_security_group.db.id]\n  db_subnet_group_name   = aws_db_subnet_group.main.name\n\n  deletion_protection = true\n\n  tags = {\n    Name = \"corridor-db\"\n  }\n}\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/","title":"Install on AWS - Kubernetes (EKS)","text":"<p>This guide provides an overview of deploying Corridor on Amazon Elastic Kubernetes Service (EKS).</p>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#background","title":"Background","text":"<p>Corridor can be deployed on EKS using Kubernetes to manage containerized services. The deployment leverages AWS's managed services for databases, storage, and networking.</p>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/aws/aws-eks/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with appropriate IAM permissions</li> <li>AWS CLI installed and configured</li> <li><code>kubectl</code> CLI installed</li> <li><code>eksctl</code> CLI installed</li> <li>Access to Corridor container images</li> <li>Sufficient AWS service quotas</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#required-aws-services","title":"Required AWS Services","text":"<ol> <li>Amazon EKS: Managed Kubernetes cluster</li> <li>Private cluster recommended</li> <li>Node autoscaling configured</li> <li> <p>NAT Gateway for egress traffic</p> </li> <li> <p>Amazon RDS: PostgreSQL database for metadata</p> </li> <li>PostgreSQL 14+ recommended</li> <li>Multi-AZ configuration</li> <li> <p>Automated backups enabled</p> </li> <li> <p>Amazon EFS: NFS storage for persistent volumes</p> </li> <li>Standard tier</li> <li>Mount targets in each subnet</li> <li>Access from EKS cluster VPC</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#optional-aws-services","title":"Optional AWS Services","text":"<ul> <li>Route 53: For domain management</li> <li>Secrets Manager: For storing sensitive configuration</li> <li>CloudWatch: For logging and monitoring</li> <li>AWS WAF: For DDoS protection and WAF</li> <li>CloudFront: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#architecture-overview","title":"Architecture Overview","text":"<pre><code>EKS Cluster\n\u251c\u2500\u2500 Application Namespace\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-worker (Celery workers)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Cluster Components\n    \u251c\u2500\u2500 AWS Load Balancer Controller\n    \u251c\u2500\u2500 cert-manager (TLS)\n    \u2514\u2500\u2500 EFS CSI Driver\n</code></pre> <p>Key Components:</p> <ul> <li>Persistent Volumes: EFS-backed storage</li> <li>ALB Ingress: HTTP routing and load balancing</li> <li>cert-manager: Automated Let's Encrypt TLS certificates</li> <li>Redis: Container-based Redis service for message queuing</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#installation-overview","title":"Installation Overview","text":""},{"location":"technology/self-hosting/installation/aws/aws-eks/#step-1-setup-eks-cluster","title":"Step 1: Setup EKS Cluster","text":"<ol> <li>Create EKS cluster</li> <li>Setup NAT Gateway for private cluster internet access</li> <li>Enable cluster autoscaling</li> <li>Configure security groups</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#step-2-setup-supporting-infrastructure","title":"Step 2: Setup Supporting Infrastructure","text":"<ol> <li>Create RDS PostgreSQL instance</li> <li>Create EFS filesystem and mount targets</li> <li>Configure VPC networking and security groups</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#step-3-install-cluster-components","title":"Step 3: Install Cluster Components","text":"<p>Install one-time cluster-level components:</p> <ol> <li>AWS Load Balancer Controller</li> <li>cert-manager for automated TLS certificates</li> <li>EFS CSI Driver</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#step-4-deploy-corridor","title":"Step 4: Deploy Corridor","text":"<ol> <li>Create Kubernetes namespace</li> <li>Create database in RDS</li> <li>Configure Kustomize overlay</li> <li>Create container registry pull secrets</li> <li>Apply Kubernetes manifests using Kustomize</li> <li>Configure DNS and verify deployment</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#installation-steps","title":"Installation Steps","text":"<p>Contact Corridor support for:</p> <ul> <li>Complete EKS cluster creation scripts</li> <li>Terraform/Infrastructure-as-Code templates</li> <li>Kubernetes manifests and Kustomize overlays</li> <li>Client deployment configuration examples</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#post-installation","title":"Post Installation","text":"<p>After deployment:</p> <ol> <li>Configure DNS: Point domain to ALB DNS name</li> <li>Verify TLS: Ensure certificates are issued by cert-manager (check after 5-10 minutes)</li> <li>Create Admin User: Initialize first admin user account</li> <li>Test Connectivity: Verify all services are accessible</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/aws/aws-eks/#view-logs","title":"View Logs","text":"<pre><code># View application logs\nkubectl logs -n &lt;namespace&gt; deploy/corridor-app\n\n# View worker logs  \nkubectl logs -n &lt;namespace&gt; deploy/corridor-worker\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#access-services","title":"Access Services","text":"<pre><code># Access pod shell\nkubectl exec -it -n &lt;namespace&gt; deploy/corridor-app -- /bin/bash\n\n# View pod status\nkubectl get pods -n &lt;namespace&gt;\n\n# View all resources in namespace\nkubectl get all -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#update-deployment","title":"Update Deployment","text":"<pre><code># Restart deployment (pull latest image)\nkubectl rollout restart deployment corridor-app -n &lt;namespace&gt;\n\n# Check rollout status\nkubectl rollout status deployment corridor-app -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#common-operations","title":"Common Operations","text":"<pre><code># View ingress and ALB\nkubectl get ingress -n &lt;namespace&gt;\n\n# Check certificate status\nkubectl get certificate -n &lt;namespace&gt;\n\n# View persistent volume claims\nkubectl get pvc -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#eks-specific-features","title":"EKS-Specific Features","text":"<ul> <li>Managed Node Groups: Simplified node management</li> <li>Node Auto-scaling: Automatic cluster scaling based on workload demands</li> <li>CloudWatch Integration: Built-in logging and metrics collection</li> <li>IAM Roles for Service Accounts: Fine-grained pod permissions</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use Spot Instances: For non-production workloads (up to 90% savings)</li> <li>Enable Cluster Autoscaling: Scale down during off-hours</li> <li>Use Savings Plans: For predictable production workloads</li> <li>Right-size Node Groups: Use appropriate instance types for workloads</li> <li>Use S3 Lifecycle Rules: For backup storage optimization</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnets with NAT Gateway</li> <li>Use IAM Roles for Service Accounts (IRSA)</li> <li>Configure Security Groups for pod-to-pod traffic</li> <li>Use Private EKS Clusters with authorized networks only</li> <li>Enable Control Plane Logging</li> <li>Store secrets in AWS Secrets Manager or Kubernetes secrets</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#example-configurations","title":"Example Configurations","text":""},{"location":"technology/self-hosting/installation/aws/aws-eks/#example-dockerfile","title":"Example Dockerfile","text":"<pre><code>FROM redhat/ubi8\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\nENV CORRIDOR_HOME=/opt/corridor\n# Disable buffering in python to enable faster logging\nENV PYTHONUNBUFFERED=1\n\n# System dependencies\nRUN yum update -y \\\n    &amp;&amp; yum install java-1.8.0-openjdk procps-ng https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm -y \\\n    &amp;&amp; yum clean all\n\n# Setup JAVA_HOME so Spark can find the java-1.8.0\nENV JAVA_HOME=/etc/alternatives/jre\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\nENV UV_LINK_MODE=copy\n\nWORKDIR $CORRIDOR_HOME\nENV PATH=\"/opt/corridor/venv/bin/:$PATH\"\n\nCOPY uv.lock pyproject.toml $CORRIDOR_HOME/\n\n# Install runtime dependencies\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv venv $CORRIDOR_HOME/venv --python 3.11 --seed \\\n    &amp;&amp; source $CORRIDOR_HOME/venv/bin/activate \\\n    &amp;&amp; uv sync --active --frozen --no-install-workspace --no-group dev --no-group test --extra pyspark\n\n# Install corridor packages\nRUN --mount=target=/corridor_wheels,type=bind,source=corridor_wheels \\\n    uv pip install --python $CORRIDOR_HOME/venv --no-cache-dir /corridor_wheels/*.whl\n\n# Expose application ports\nEXPOSE 5002 5003\n\n# Health check (adjust for specific service)\nHEALTHCHECK --interval=30s --timeout=5s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:5002/corr-api || exit 1\n</code></pre> <p>Using Different Services:</p> <p>The same image can run different Corridor services by overriding the command:</p> <pre><code># corridor-app\ncommand: [\"/opt/corridor/venv/bin/corridor-app\", \"run\"]\n\n# corridor-worker\ncommand: [\"/opt/corridor/venv/bin/corridor-worker\", \"run\"]\n\n# corridor-jupyter\ncommand: [\"/opt/corridor/venv/bin/corridor-jupyter\", \"run\"]\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-eks/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<pre><code># terraform.tf - Main configuration\nterraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n\n# Variables\nvariable \"region\" {\n  description = \"AWS Region\"\n  type        = string\n  default     = \"us-west-2\"\n}\n\nvariable \"cluster_name\" {\n  description = \"EKS Cluster Name\"\n  type        = string\n  default     = \"corridor-eks\"\n}\n\n# VPC\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"corridor-vpc\"\n  }\n}\n\n# Private Subnets\nresource \"aws_subnet\" \"private\" {\n  count             = 3\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = \"10.0.${count.index + 1}.0/24\"\n  availability_zone = data.aws_availability_zones.available.names[count.index]\n\n  tags = {\n    Name = \"corridor-private-${count.index + 1}\"\n  }\n}\n\n# NAT Gateway\nresource \"aws_nat_gateway\" \"main\" {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = aws_subnet.public[0].id\n\n  tags = {\n    Name = \"corridor-nat\"\n  }\n}\n\n# EKS Cluster\nresource \"aws_eks_cluster\" \"main\" {\n  name     = var.cluster_name\n  role_arn = aws_iam_role.eks_cluster.arn\n\n  vpc_config {\n    subnet_ids              = aws_subnet.private[*].id\n    endpoint_private_access = true\n    endpoint_public_access  = false\n  }\n\n  enabled_cluster_log_types = [\"api\", \"audit\", \"authenticator\", \"controllerManager\", \"scheduler\"]\n}\n\n# Node Group\nresource \"aws_eks_node_group\" \"main\" {\n  cluster_name    = aws_eks_cluster.main.name\n  node_group_name = \"corridor-nodes\"\n  node_role_arn   = aws_iam_role.eks_nodes.arn\n  subnet_ids      = aws_subnet.private[*].id\n\n  scaling_config {\n    desired_size = 3\n    min_size     = 3\n    max_size     = 10\n  }\n\n  instance_types = [\"m5.xlarge\"]\n\n  tags = {\n    Name = \"corridor-node-group\"\n  }\n}\n\n# RDS Instance\nresource \"aws_db_instance\" \"postgres\" {\n  identifier           = \"corridor-db\"\n  engine              = \"postgres\"\n  engine_version      = \"14\"\n  instance_class      = \"db.t3.xlarge\"\n  allocated_storage   = 100\n  storage_type        = \"gp3\"\n  multi_az           = true\n  db_name             = \"corridor\"\n  username           = \"corridor\"\n  password           = var.db_password\n\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.main.name\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"Mon:04:00-Mon:05:00\"\n\n  deletion_protection = true\n\n  tags = {\n    Name = \"corridor-db\"\n  }\n}\n\n# EFS Filesystem\nresource \"aws_efs_file_system\" \"main\" {\n  creation_token = \"corridor-efs\"\n  encrypted      = true\n\n  tags = {\n    Name = \"corridor-efs\"\n  }\n}\n\nresource \"aws_efs_mount_target\" \"main\" {\n  count           = length(aws_subnet.private)\n  file_system_id  = aws_efs_file_system.main.id\n  subnet_id       = aws_subnet.private[count.index].id\n  security_groups = [aws_security_group.efs.id]\n}\n\n# Outputs\noutput \"cluster_endpoint\" {\n  value       = aws_eks_cluster.main.endpoint\n  description = \"EKS Cluster Endpoint\"\n  sensitive   = true\n}\n\noutput \"cluster_name\" {\n  value       = aws_eks_cluster.main.name\n  description = \"EKS Cluster Name\"\n}\n\noutput \"rds_endpoint\" {\n  value       = aws_db_instance.postgres.endpoint\n  description = \"RDS Instance Endpoint\"\n}\n\noutput \"efs_id\" {\n  value       = aws_efs_file_system.main.id\n  description = \"EFS Filesystem ID\"\n}\n</code></pre> <p>Usage:</p> <pre><code># Initialize Terraform\nterraform init\n\n# Create terraform.tfvars file\ncat &gt; terraform.tfvars &lt;&lt;EOF\nregion = \"us-west-2\"\ndb_password = \"secure-password-here\"\nEOF\n\n# Plan the deployment\nterraform plan\n\n# Apply the configuration\nterraform apply\n\n# Get kubectl credentials\naws eks update-kubeconfig --name corridor-eks --region us-west-2\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/","title":"Install on AWS - ECS Fargate","text":"<p>This guide provides an overview of deploying Corridor on AWS ECS Fargate for a serverless container deployment.</p>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#background","title":"Background","text":"<p>Corridor can be deployed on AWS Fargate using a single service with multiple containers. This deployment leverages AWS's managed services and eliminates the need to manage servers or clusters.</p>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/aws/aws-fargate/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with appropriate IAM permissions</li> <li>AWS CLI installed and configured</li> <li>Access to Corridor container images</li> <li>Sufficient AWS service quotas</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#required-aws-services","title":"Required AWS Services","text":"<ol> <li> <p>Amazon ECS: Container orchestration</p> </li> <li> <p>Fargate launch type</p> </li> <li>Service auto-scaling</li> <li> <p>Application Load Balancer</p> </li> <li> <p>Amazon RDS: PostgreSQL database for metadata</p> </li> <li> <p>PostgreSQL 14+ recommended</p> </li> <li>Multi-AZ configuration</li> <li> <p>Automated backups enabled</p> </li> <li> <p>Amazon EFS: File storage for persistent data</p> </li> <li>Mount targets in each subnet</li> <li>Access from ECS tasks</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#optional-aws-services","title":"Optional AWS Services","text":"<ul> <li>Route 53: For domain management</li> <li>Secrets Manager: For storing sensitive configuration</li> <li>CloudWatch: For logging and monitoring</li> <li>AWS WAF: For DDoS protection and WAF</li> <li>CloudFront: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#architecture-overview","title":"Architecture Overview","text":"<pre><code>ECS Cluster (Fargate)\n\u2514\u2500\u2500 Corridor Service\n    \u2514\u2500\u2500 Task Definition\n        \u251c\u2500\u2500 corridor-app Container (Web UI)\n        \u251c\u2500\u2500 corridor-worker Container (Celery workers)\n        \u251c\u2500\u2500 corridor-jupyter Container (JupyterHub)\n        \u2514\u2500\u2500 redis Container (Message Queue)\n</code></pre> <p>Key Components:</p> <ul> <li>Single ECS Service: Running all containers</li> <li>Shared Task Definition: All components in one task</li> <li>ALB: HTTP routing and load balancing</li> <li>EFS: Shared persistent storage</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#installation-overview","title":"Installation Overview","text":""},{"location":"technology/self-hosting/installation/aws/aws-fargate/#step-1-setup-ecs-infrastructure","title":"Step 1: Setup ECS Infrastructure","text":"<ol> <li>Create VPC with public/private subnets</li> <li>Setup NAT Gateway for private subnet internet access</li> <li>Create ECS cluster</li> <li>Configure security groups</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#step-2-setup-supporting-infrastructure","title":"Step 2: Setup Supporting Infrastructure","text":"<ol> <li>Create RDS PostgreSQL instance</li> <li>Create EFS filesystem and mount targets</li> <li>Configure Application Load Balancer</li> <li>Setup IAM roles and execution policies</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#step-3-create-task-definition","title":"Step 3: Create Task Definition","text":"<p>Create a single task definition containing all containers:</p> <ul> <li>corridor-app container</li> <li>corridor-worker container</li> <li>corridor-jupyter container</li> <li>redis container</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#step-4-deploy-service","title":"Step 4: Deploy Service","text":"<ol> <li>Create ECS service</li> <li>Configure service discovery</li> <li>Setup ALB target group</li> <li>Configure auto-scaling</li> <li>Setup DNS and verify deployment</li> </ol>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#example-task-definition","title":"Example Task Definition","text":"<pre><code>{\n  \"family\": \"corridor\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"4096\",\n  \"memory\": \"16384\",\n  \"executionRoleArn\": \"arn:aws:iam::ACCOUNT_ID:role/ecsTaskExecutionRole\",\n  \"taskRoleArn\": \"arn:aws:iam::ACCOUNT_ID:role/corridorTaskRole\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"corridor-app\",\n      \"image\": \"ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/corridor/ggx:latest\",\n      \"cpu\": 2048,\n      \"memory\": 4096,\n      \"essential\": true,\n      \"command\": [\"/opt/corridor/venv/bin/corridor-app\", \"run\"],\n      \"portMappings\": [\n        {\n          \"containerPort\": 5002,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"WSGI_SERVER\",\n          \"value\": \"gunicorn\"\n        }\n      ],\n      \"secrets\": [\n        {\n          \"name\": \"DB_PASSWORD\",\n          \"valueFrom\": \"arn:aws:secretsmanager:REGION:ACCOUNT_ID:secret:corridor/db-password\"\n        }\n      ],\n      \"mountPoints\": [\n        {\n          \"sourceVolume\": \"corridor-data\",\n          \"containerPath\": \"/opt/corridor/data\",\n          \"readOnly\": false\n        }\n      ],\n      \"dependsOn\": [\n        {\n          \"containerName\": \"redis\",\n          \"condition\": \"START\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/corridor\",\n          \"awslogs-region\": \"REGION\",\n          \"awslogs-stream-prefix\": \"app\"\n        }\n      },\n      \"healthCheck\": {\n        \"command\": [\n          \"CMD-SHELL\",\n          \"curl -f http://localhost:5002/health || exit 1\"\n        ],\n        \"interval\": 30,\n        \"timeout\": 5,\n        \"retries\": 3,\n        \"startPeriod\": 60\n      }\n    },\n    {\n      \"name\": \"corridor-worker\",\n      \"image\": \"ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/corridor/ggx:latest\",\n      \"cpu\": 1024,\n      \"memory\": 4096,\n      \"essential\": true,\n      \"command\": [\"/opt/corridor/venv/bin/corridor-worker\", \"run\"],\n      \"mountPoints\": [\n        {\n          \"sourceVolume\": \"corridor-data\",\n          \"containerPath\": \"/opt/corridor/data\",\n          \"readOnly\": false\n        }\n      ],\n      \"dependsOn\": [\n        {\n          \"containerName\": \"redis\",\n          \"condition\": \"START\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/corridor\",\n          \"awslogs-region\": \"REGION\",\n          \"awslogs-stream-prefix\": \"worker\"\n        }\n      }\n    },\n    {\n      \"name\": \"corridor-jupyter\",\n      \"image\": \"ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/corridor/ggx:latest\",\n      \"cpu\": 512,\n      \"memory\": 4096,\n      \"essential\": false,\n      \"command\": [\"/opt/corridor/venv/bin/corridor-jupyter\", \"run\"],\n      \"portMappings\": [\n        {\n          \"containerPort\": 5003,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"mountPoints\": [\n        {\n          \"sourceVolume\": \"corridor-data\",\n          \"containerPath\": \"/opt/corridor/data\",\n          \"readOnly\": false\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/corridor\",\n          \"awslogs-region\": \"REGION\",\n          \"awslogs-stream-prefix\": \"jupyter\"\n        }\n      }\n    },\n    {\n      \"name\": \"redis\",\n      \"image\": \"redis:7-alpine\",\n      \"cpu\": 512,\n      \"memory\": 1024,\n      \"essential\": true,\n      \"portMappings\": [\n        {\n          \"containerPort\": 6379,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/corridor\",\n          \"awslogs-region\": \"REGION\",\n          \"awslogs-stream-prefix\": \"redis\"\n        }\n      }\n    }\n  ],\n  \"volumes\": [\n    {\n      \"name\": \"corridor-data\",\n      \"efsVolumeConfiguration\": {\n        \"fileSystemId\": \"fs-xxxxxx\",\n        \"transitEncryption\": \"ENABLED\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#example-service-configuration","title":"Example Service Configuration","text":"<pre><code>{\n  \"cluster\": \"corridor\",\n  \"serviceName\": \"corridor\",\n  \"taskDefinition\": \"corridor\",\n  \"desiredCount\": 2,\n  \"launchType\": \"FARGATE\",\n  \"platformVersion\": \"LATEST\",\n  \"networkConfiguration\": {\n    \"awsvpcConfiguration\": {\n      \"subnets\": [\"subnet-xxxxx\", \"subnet-yyyyy\"],\n      \"securityGroups\": [\"sg-zzzzz\"],\n      \"assignPublicIp\": \"DISABLED\"\n    }\n  },\n  \"loadBalancers\": [\n    {\n      \"targetGroupArn\": \"arn:aws:elasticloadbalancing:REGION:ACCOUNT_ID:targetgroup/corridor-app/xxxxx\",\n      \"containerName\": \"corridor-app\",\n      \"containerPort\": 5002\n    },\n    {\n      \"targetGroupArn\": \"arn:aws:elasticloadbalancing:REGION:ACCOUNT_ID:targetgroup/corridor-jupyter/yyyyy\",\n      \"containerName\": \"corridor-jupyter\",\n      \"containerPort\": 5003\n    }\n  ],\n  \"serviceConnectConfiguration\": {\n    \"enabled\": true,\n    \"namespace\": \"corridor\",\n    \"services\": [\n      {\n        \"portName\": \"redis\",\n        \"discoveryName\": \"redis\",\n        \"clientAliases\": [\n          {\n            \"port\": 6379\n          }\n        ]\n      }\n    ]\n  },\n  \"deploymentConfiguration\": {\n    \"deploymentCircuitBreaker\": {\n      \"enable\": true,\n      \"rollback\": true\n    },\n    \"maximumPercent\": 200,\n    \"minimumHealthyPercent\": 100\n  },\n  \"enableECSManagedTags\": true,\n  \"propagateTags\": \"SERVICE\",\n  \"enableExecuteCommand\": true\n}\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/aws/aws-fargate/#view-logs","title":"View Logs","text":"<pre><code># View all container logs\naws logs tail /ecs/corridor --follow\n\n# View specific container logs\naws logs tail /ecs/corridor --filter-pattern \"app\"    # corridor-app logs\naws logs tail /ecs/corridor --filter-pattern \"worker\" # corridor-worker logs\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#access-containers","title":"Access Containers","text":"<pre><code># Execute command in container\naws ecs execute-command \\\n  --cluster corridor \\\n  --task &lt;task-id&gt; \\\n  --container corridor-app \\\n  --command \"/bin/bash\" \\\n  --interactive\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#update-service","title":"Update Service","text":"<pre><code># Update service (new image)\naws ecs update-service \\\n  --cluster corridor \\\n  --service corridor \\\n  --force-new-deployment\n\n# Check deployment status\naws ecs describe-services \\\n  --cluster corridor \\\n  --services corridor\n</code></pre>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use Fargate Spot: For non-production workloads (up to 70% savings)</li> <li>Enable Service Auto-scaling: Scale down during off-hours</li> <li>Use Compute Savings Plans: For predictable workloads</li> <li>Right-size Task Resources: Adjust CPU/memory based on usage</li> </ul>"},{"location":"technology/self-hosting/installation/aws/aws-fargate/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnets with NAT Gateway</li> <li>Use Task IAM Roles for service permissions</li> <li>Configure Security Groups for task communication</li> <li>Enable Execute Command logging</li> <li>Store secrets in AWS Secrets Manager</li> <li>Enable Container Insights for monitoring</li> </ul>"},{"location":"technology/self-hosting/installation/azure/","title":"Install on Microsoft Azure","text":"<p>This guide provides an overview of deploying Corridor on Microsoft Azure. Corridor supports two deployment approaches on Azure.</p>"},{"location":"technology/self-hosting/installation/azure/#deployment-options","title":"Deployment Options","text":""},{"location":"technology/self-hosting/installation/azure/#option-1-kubernetes-aks","title":"Option 1: Kubernetes - AKS","text":"<p>Deploy Corridor on Azure Kubernetes Service (AKS) for a cloud-native, containerized deployment.</p> <p>Best for:</p> <ul> <li>Organizations with Kubernetes expertise</li> <li>Multi-tenant deployments with namespace isolation</li> <li>Auto-scaling and high availability requirements</li> <li>Modern cloud-native infrastructure</li> </ul> <p>View AKS Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/azure/#option-2-virtual-machines-azure-vms","title":"Option 2: Virtual Machines - Azure VMs","text":"<p>Deploy Corridor on Azure Virtual Machines for a traditional VM-based deployment.</p> <p>Best for:</p> <ul> <li>Organizations preferring VM-based infrastructure</li> <li>Direct control over the operating system</li> <li>Traditional IT infrastructure patterns</li> <li>Custom hardware requirements</li> </ul> <p>View Azure VMs Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/azure/#common-azure-services","title":"Common Azure Services","text":"<p>Both deployment options utilize these Azure managed services:</p> <ul> <li>Azure Database for PostgreSQL: Metadata storage</li> <li>Azure Storage: Object storage for file management (or Azure Files for NFS)</li> <li>Application Gateway: HTTP(S) load balancing</li> <li>Azure DNS: Domain name management</li> <li>Virtual Network: Private cloud networking</li> </ul>"},{"location":"technology/self-hosting/installation/azure/#choosing-the-right-option","title":"Choosing the Right Option","text":"Factor AKS (Kubernetes) Azure VMs Complexity Higher Lower Scalability Automatic Manual Multi-tenancy Native (namespaces) Separate VMs Operational Overhead Lower (managed K8s) Higher Deployment Speed Faster Moderate Infrastructure Management Minimal Full Cost Model Node-based Per instance Kubernetes Skills Required Yes No"},{"location":"technology/self-hosting/installation/azure/azure-aks/","title":"Install on Azure - Kubernetes (AKS)","text":"<p>This guide provides an overview of deploying Corridor on Azure Kubernetes Service (AKS).</p>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#background","title":"Background","text":"<p>Corridor can be deployed on AKS using Kubernetes to manage containerized services. The deployment leverages Azure's managed services for databases, storage, and networking.</p>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/azure/azure-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with appropriate permissions</li> <li>Azure CLI installed and configured</li> <li><code>kubectl</code> CLI installed</li> <li>Access to Corridor container images</li> <li>Sufficient Azure resource quotas</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#required-azure-services","title":"Required Azure Services","text":"<ol> <li>Azure Kubernetes Service (AKS): Managed Kubernetes cluster</li> <li>Private cluster recommended</li> <li>Node autoscaling configured</li> <li> <p>Azure CNI networking</p> </li> <li> <p>Azure Database for PostgreSQL: Metadata database</p> </li> <li>PostgreSQL 14+ recommended</li> <li>Zone redundant configuration</li> <li> <p>Automated backups enabled</p> </li> <li> <p>Azure Files Premium: Storage for persistent volumes</p> </li> <li>Premium tier for performance</li> <li>Access from AKS cluster</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#optional-azure-services","title":"Optional Azure Services","text":"<ul> <li>Azure DNS: For domain management</li> <li>Key Vault: For storing sensitive configuration</li> <li>Azure Monitor: For logging and monitoring</li> <li>Azure Front Door: For DDoS protection and WAF</li> <li>Azure CDN: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#architecture-overview","title":"Architecture Overview","text":"<pre><code>AKS Cluster\n\u251c\u2500\u2500 Application Namespace\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-worker (Celery workers)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Cluster Components\n    \u251c\u2500\u2500 NGINX Ingress Controller\n    \u251c\u2500\u2500 cert-manager (TLS)\n    \u2514\u2500\u2500 CSI Storage Drivers\n</code></pre> <p>Key Components:</p> <ul> <li>Persistent Volumes: Azure Files storage</li> <li>NGINX Ingress: HTTP routing and load balancing</li> <li>cert-manager: Automated Let's Encrypt TLS certificates</li> <li>Redis: Container-based Redis service for message queuing</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#installation-overview","title":"Installation Overview","text":""},{"location":"technology/self-hosting/installation/azure/azure-aks/#step-1-setup-aks-cluster","title":"Step 1: Setup AKS Cluster","text":"<ol> <li>Create AKS cluster</li> <li>Configure virtual network</li> <li>Enable cluster autoscaling</li> <li>Configure authorized networks</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#step-2-setup-supporting-infrastructure","title":"Step 2: Setup Supporting Infrastructure","text":"<ol> <li>Create Azure Database for PostgreSQL</li> <li>Create Azure Files share</li> <li>Configure networking and firewall rules</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#step-3-install-cluster-components","title":"Step 3: Install Cluster Components","text":"<p>Install one-time cluster-level components:</p> <ol> <li>NGINX Ingress Controller</li> <li>cert-manager for automated TLS certificates</li> <li>Azure Files CSI Driver</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#step-4-deploy-corridor","title":"Step 4: Deploy Corridor","text":"<ol> <li>Create Kubernetes namespace</li> <li>Create database in Azure PostgreSQL</li> <li>Configure Kustomize overlay</li> <li>Create container registry pull secrets</li> <li>Apply Kubernetes manifests using Kustomize</li> <li>Configure DNS and verify deployment</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#installation-steps","title":"Installation Steps","text":"<p>Contact Corridor support for:</p> <ul> <li>Complete AKS cluster creation scripts</li> <li>Terraform/Infrastructure-as-Code templates</li> <li>Kubernetes manifests and Kustomize overlays</li> <li>Client deployment configuration examples</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#post-installation","title":"Post Installation","text":"<p>After deployment:</p> <ol> <li>Configure DNS: Point domain to Ingress IP address</li> <li>Verify TLS: Ensure certificates are issued by cert-manager</li> <li>Create Admin User: Initialize first admin user account</li> <li>Test Connectivity: Verify all services are accessible</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/azure/azure-aks/#view-logs","title":"View Logs","text":"<pre><code># View application logs\nkubectl logs -n &lt;namespace&gt; deploy/corridor-app\n\n# View worker logs  \nkubectl logs -n &lt;namespace&gt; deploy/corridor-worker\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#access-services","title":"Access Services","text":"<pre><code># Access pod shell\nkubectl exec -it -n &lt;namespace&gt; deploy/corridor-app -- /bin/bash\n\n# View pod status\nkubectl get pods -n &lt;namespace&gt;\n\n# View all resources in namespace\nkubectl get all -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#update-deployment","title":"Update Deployment","text":"<pre><code># Restart deployment (pull latest image)\nkubectl rollout restart deployment corridor-app -n &lt;namespace&gt;\n\n# Check rollout status\nkubectl rollout status deployment corridor-app -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#common-operations","title":"Common Operations","text":"<pre><code># View ingress and IP\nkubectl get ingress -n &lt;namespace&gt;\n\n# Check certificate status\nkubectl get certificate -n &lt;namespace&gt;\n\n# View persistent volume claims\nkubectl get pvc -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#aks-specific-features","title":"AKS-Specific Features","text":"<ul> <li>Managed Node Pools: Simplified node management</li> <li>Virtual Node: Serverless container hosting with ACI</li> <li>Azure Monitor Integration: Built-in logging and metrics</li> <li>Azure AD Integration: Identity and RBAC</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use Spot Node Pools: For non-production workloads (up to 90% savings)</li> <li>Enable Cluster Autoscaling: Scale down during off-hours</li> <li>Use Reserved Instances: For predictable workloads</li> <li>Right-size Node Pools: Use appropriate VM sizes</li> <li>Use Storage Lifecycle: For backup storage optimization</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnets</li> <li>Use Managed Identities for service authentication</li> <li>Configure Network Policies for pod-to-pod traffic</li> <li>Use Private AKS Clusters with authorized networks</li> <li>Enable Azure Policy for Kubernetes</li> <li>Store secrets in Azure Key Vault</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#example-configurations","title":"Example Configurations","text":""},{"location":"technology/self-hosting/installation/azure/azure-aks/#example-dockerfile","title":"Example Dockerfile","text":"<pre><code>FROM redhat/ubi8\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\nENV CORRIDOR_HOME=/opt/corridor\n# Disable buffering in python to enable faster logging\nENV PYTHONUNBUFFERED=1\n\n# System dependencies\nRUN yum update -y \\\n    &amp;&amp; yum install java-1.8.0-openjdk procps-ng https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm -y \\\n    &amp;&amp; yum clean all\n\n# Setup JAVA_HOME so Spark can find the java-1.8.0\nENV JAVA_HOME=/etc/alternatives/jre\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\nENV UV_LINK_MODE=copy\n\nWORKDIR $CORRIDOR_HOME\nENV PATH=\"/opt/corridor/venv/bin/:$PATH\"\n\nCOPY uv.lock pyproject.toml $CORRIDOR_HOME/\n\n# Install runtime dependencies\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv venv $CORRIDOR_HOME/venv --python 3.11 --seed \\\n    &amp;&amp; source $CORRIDOR_HOME/venv/bin/activate \\\n    &amp;&amp; uv sync --active --frozen --no-install-workspace --no-group dev --no-group test --extra pyspark\n\n# Install corridor packages\nRUN --mount=target=/corridor_wheels,type=bind,source=corridor_wheels \\\n    uv pip install --python $CORRIDOR_HOME/venv --no-cache-dir /corridor_wheels/*.whl\n\n# Expose application ports\nEXPOSE 5002 5003\n\n# Health check (adjust for specific service)\nHEALTHCHECK --interval=30s --timeout=5s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:5002/corr-api || exit 1\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-aks/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<pre><code># terraform.tf - Main configuration\nterraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt; 3.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\n# Resource Group\nresource \"azurerm_resource_group\" \"main\" {\n  name     = \"corridor-rg\"\n  location = var.location\n}\n\n# Virtual Network\nresource \"azurerm_virtual_network\" \"main\" {\n  name                = \"corridor-vnet\"\n  address_space       = [\"10.0.0.0/16\"]\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n}\n\nresource \"azurerm_subnet\" \"aks\" {\n  name                 = \"aks-subnet\"\n  resource_group_name  = azurerm_resource_group.main.name\n  virtual_network_name = azurerm_virtual_network.main.name\n  address_prefixes     = [\"10.0.1.0/24\"]\n}\n\n# AKS Cluster\nresource \"azurerm_kubernetes_cluster\" \"main\" {\n  name                = \"corridor-aks\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n  dns_prefix          = \"corridor\"\n\n  default_node_pool {\n    name                = \"default\"\n    node_count          = 3\n    vm_size            = \"Standard_D4s_v3\"\n    enable_auto_scaling = true\n    min_count          = 3\n    max_count          = 10\n    vnet_subnet_id     = azurerm_subnet.aks.id\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  network_profile {\n    network_plugin = \"azure\"\n    network_policy = \"calico\"\n  }\n\n  private_cluster_enabled = true\n\n  addon_profile {\n    oms_agent {\n      enabled = true\n    }\n  }\n}\n\n# PostgreSQL Server\nresource \"azurerm_postgresql_server\" \"main\" {\n  name                = \"corridor-db\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  sku_name = \"GP_Gen5_4\"\n\n  storage_mb                   = 102400\n  backup_retention_days        = 7\n  geo_redundant_backup_enabled = true\n  auto_grow_enabled           = true\n\n  administrator_login          = \"corridor\"\n  administrator_login_password = var.db_password\n  version                     = \"14\"\n  ssl_enforcement_enabled     = true\n}\n\nresource \"azurerm_postgresql_database\" \"main\" {\n  name                = \"corridor\"\n  resource_group_name = azurerm_resource_group.main.name\n  server_name         = azurerm_postgresql_server.main.name\n  charset             = \"UTF8\"\n  collation          = \"English_United States.1252\"\n}\n\n# Azure Files Share\nresource \"azurerm_storage_account\" \"main\" {\n  name                     = \"corridorstorage\"\n  resource_group_name      = azurerm_resource_group.main.name\n  location                 = azurerm_resource_group.main.location\n  account_tier             = \"Premium\"\n  account_replication_type = \"LRS\"\n  account_kind            = \"FileStorage\"\n}\n\nresource \"azurerm_storage_share\" \"main\" {\n  name                 = \"corridor\"\n  storage_account_name = azurerm_storage_account.main.name\n  quota                = 1024\n}\n\n# Outputs\noutput \"aks_name\" {\n  value       = azurerm_kubernetes_cluster.main.name\n  description = \"AKS Cluster Name\"\n}\n\noutput \"postgresql_fqdn\" {\n  value       = azurerm_postgresql_server.main.fqdn\n  description = \"PostgreSQL Server FQDN\"\n}\n\noutput \"storage_account_name\" {\n  value       = azurerm_storage_account.main.name\n  description = \"Storage Account Name\"\n}\n</code></pre> <p>Usage:</p> <pre><code># Initialize Terraform\nterraform init\n\n# Create terraform.tfvars file\ncat &gt; terraform.tfvars &lt;&lt;EOF\nlocation = \"eastus\"\ndb_password = \"secure-password-here\"\nEOF\n\n# Plan the deployment\nterraform plan\n\n# Apply the configuration\nterraform apply\n\n# Get kubectl credentials\naz aks get-credentials --resource-group corridor-rg --name corridor-aks\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/","title":"Install on Azure - Virtual Machines","text":"<p>This guide provides an overview of deploying Corridor on a single Azure Virtual Machine.</p>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#background","title":"Background","text":"<p>Corridor can be deployed on a single Azure VM running all components (app, api, workers, jupyter). This approach provides a simple deployment model suitable for organizations that prefer traditional VM-based infrastructure.</p>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/azure/azure-vms/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure subscription with appropriate permissions</li> <li>Azure CLI installed and configured</li> <li>SSH access to Azure VM</li> <li>Access to Corridor installation bundle</li> <li>Sufficient Azure service quotas</li> <li>Minimum Requirements and System Dependencies are met</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#required-azure-services","title":"Required Azure Services","text":"<ol> <li>Azure Virtual Machines: VM for running all Corridor components</li> <li>Instance size based on Minimum Requirements</li> <li>Recommended: Standard_D8s_v3 or larger</li> <li> <p>Premium SSD for local storage</p> </li> <li> <p>Azure Database for PostgreSQL: Database for metadata</p> </li> <li>PostgreSQL 14+ recommended</li> <li>Zone redundant configuration (for production)</li> <li>Automated backups enabled</li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#optional-azure-services","title":"Optional Azure Services","text":"<ul> <li>Azure DNS: For domain management</li> <li>Key Vault: For storing sensitive configuration</li> <li>Azure Monitor: For logging and monitoring</li> <li>Application Gateway: For WAF and load balancing</li> <li>Azure CDN: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#architecture-overview","title":"Architecture Overview","text":"<pre><code>Azure VM (Standard_D8s_v3)\n\u251c\u2500\u2500 Corridor Components\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-api (API Server)\n\u2502   \u251c\u2500\u2500 corridor-worker-api (API Worker)\n\u2502   \u251c\u2500\u2500 corridor-worker-spark (Spark Worker)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Local Storage\n    \u2514\u2500\u2500 Premium SSD (/opt/corridor)\n\nAzure Database for PostgreSQL\n\u2514\u2500\u2500 PostgreSQL Database\n    \u251c\u2500\u2500 Metadata\n    \u2514\u2500\u2500 Application Data\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#installation-steps","title":"Installation Steps","text":""},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-1-create-azure-vm","title":"Step 1: Create Azure VM","text":"<pre><code># Create resource group\naz group create \\\n  --name corridor-rg \\\n  --location eastus\n\n# Create VM\naz vm create \\\n  --resource-group corridor-rg \\\n  --name corridor-vm \\\n  --image UbuntuLTS \\\n  --size Standard_D8s_v3 \\\n  --admin-username azureuser \\\n  --generate-ssh-keys \\\n  --data-disk-sizes-gb 100\n\n# Configure data disk\naz vm disk attach \\\n  --resource-group corridor-rg \\\n  --vm-name corridor-vm \\\n  --name corridor-data \\\n  --size-gb 100 \\\n  --sku Premium_LRS \\\n  --new\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-2-create-postgresql-database","title":"Step 2: Create PostgreSQL Database","text":"<pre><code># Create PostgreSQL server\naz postgres flexible-server create \\\n  --resource-group corridor-rg \\\n  --name corridor-db \\\n  --location eastus \\\n  --admin-user corridor \\\n  --admin-password &lt;secure-password&gt; \\\n  --sku-name Standard_D4s_v3 \\\n  --tier GeneralPurpose \\\n  --storage-size 128 \\\n  --version 14 \\\n  --high-availability ZoneRedundant\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-3-install-system-dependencies","title":"Step 3: Install System Dependencies","text":"<p>SSH into the Azure VM and run:</p> <pre><code># Update system\nsudo apt-get update\nsudo apt-get upgrade -y\n\n# Install dependencies\nsudo apt-get install -y \\\n    python3.11 \\\n    python3.11-dev \\\n    openjdk-8-jdk \\\n    redis-server \\\n    nginx \\\n    unzip\n\n# Start and enable Redis\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-4-install-corridor-components","title":"Step 4: Install Corridor Components","text":"<ol> <li> <p>Extract the installation bundle: </p><pre><code>cd /tmp\nunzip corridor-bundle.zip\n</code></pre><p></p> </li> <li> <p>Install each component: </p><pre><code># Install Web Application Server\nsudo ./corridor-bundle/install app -i /opt/corridor\n\n# Install API Server\nsudo ./corridor-bundle/install api -i /opt/corridor\n\n# Install API Worker\nsudo ./corridor-bundle/install worker-api -i /opt/corridor\n\n# Install Spark Worker\nsudo ./corridor-bundle/install worker-spark -i /opt/corridor\n\n# Install Jupyter\nsudo ./corridor-bundle/install jupyter -i /opt/corridor\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-5-configure-components","title":"Step 5: Configure Components","text":"<ol> <li> <p>Update API configuration in <code>/opt/corridor/instances/default/config/api_config.py</code>: </p><pre><code>SQLALCHEMY_DATABASE_URI = \"postgresql://corridor:password@corridor-db.postgres.database.azure.com:5432/corridor\"\n</code></pre><p></p> </li> <li> <p>Initialize the database: </p><pre><code>/opt/corridor/venv-api/bin/corridor-api db upgrade\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-6-create-service-files","title":"Step 6: Create Service Files","text":"<p>Create systemd service files for each component:</p> <ol> <li> <p>Web Application Server (<code>/etc/systemd/system/corridor-app.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Application Server\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-app/bin/corridor-app run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>API Server (<code>/etc/systemd/system/corridor-api.service</code>): </p><pre><code>[Unit]\nDescription=Corridor API Server\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-api/bin/corridor-api run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>API Worker (<code>/etc/systemd/system/corridor-worker-api.service</code>): </p><pre><code>[Unit]\nDescription=Corridor API Worker\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue api\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>Spark Worker (<code>/etc/systemd/system/corridor-worker-spark.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Spark Worker\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue spark --queue quick_spark\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>Jupyter (<code>/etc/systemd/system/corridor-jupyter.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Jupyter\nAfter=network.target\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nExecStart=/opt/corridor/venv-jupyter/bin/corridor-jupyter run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#step-7-start-services","title":"Step 7: Start Services","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable services\nsudo systemctl enable corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n\n# Start services\nsudo systemctl start corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/azure/azure-vms/#service-management","title":"Service Management","text":"<pre><code># Check service status\nsudo systemctl status corridor-app\nsudo systemctl status corridor-api\nsudo systemctl status corridor-worker-api\nsudo systemctl status corridor-worker-spark\nsudo systemctl status corridor-jupyter\nsudo systemctl status redis-server\n\n# Restart services\nsudo systemctl restart corridor-app\nsudo systemctl restart corridor-api\nsudo systemctl restart corridor-worker-api\nsudo systemctl restart corridor-worker-spark\nsudo systemctl restart corridor-jupyter\nsudo systemctl restart redis-server\n</code></pre>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnet with NAT Gateway</li> <li>Use Managed Identities for Azure service access</li> <li>Configure Network Security Groups for VM access</li> <li>Enable Azure Bastion for SSH access</li> <li>Store secrets in Azure Key Vault</li> <li>Enable Azure Monitor Agent for monitoring</li> <li>Configure PostgreSQL encryption at rest</li> <li>Enable automated backups for PostgreSQL</li> </ul>"},{"location":"technology/self-hosting/installation/azure/azure-vms/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<pre><code># Azure VM\nresource \"azurerm_virtual_machine\" \"corridor\" {\n  name                  = \"corridor-vm\"\n  location              = azurerm_resource_group.main.location\n  resource_group_name   = azurerm_resource_group.main.name\n  network_interface_ids = [azurerm_network_interface.main.id]\n  vm_size              = \"Standard_D8s_v3\"\n\n  storage_os_disk {\n    name              = \"corridor-os\"\n    caching           = \"ReadWrite\"\n    create_option     = \"FromImage\"\n    managed_disk_type = \"Premium_LRS\"\n  }\n\n  storage_data_disk {\n    name              = \"corridor-data\"\n    managed_disk_type = \"Premium_LRS\"\n    create_option     = \"Empty\"\n    lun               = 0\n    disk_size_gb      = 100\n  }\n\n  os_profile {\n    computer_name  = \"corridor\"\n    admin_username = \"azureuser\"\n  }\n\n  os_profile_linux_config {\n    disable_password_authentication = true\n    ssh_keys {\n      path     = \"/home/azureuser/.ssh/authorized_keys\"\n      key_data = var.ssh_public_key\n    }\n  }\n\n  tags = {\n    Name = \"corridor-server\"\n  }\n}\n\n# PostgreSQL Database\nresource \"azurerm_postgresql_flexible_server\" \"corridor\" {\n  name                = \"corridor-db\"\n  resource_group_name = azurerm_resource_group.main.name\n  location            = azurerm_resource_group.main.location\n  version            = \"14\"\n\n  administrator_login    = \"corridor\"\n  administrator_password = var.db_password\n\n  sku_name = \"Standard_D4s_v3\"\n\n  storage_mb = 131072\n\n  zone_redundant = true\n\n  backup_retention_days = 7\n\n  high_availability {\n    mode = \"ZoneRedundant\"\n  }\n\n  maintenance_window {\n    day_of_week  = 0\n    start_hour   = 3\n    start_minute = 0\n  }\n\n  tags = {\n    Name = \"corridor-db\"\n  }\n}\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/","title":"Install on Google Cloud Platform (GCP)","text":"<p>This guide provides an overview of deploying Corridor on Google Cloud Platform. Corridor supports two deployment approaches on GCP.</p>"},{"location":"technology/self-hosting/installation/gcp/#deployment-options","title":"Deployment Options","text":""},{"location":"technology/self-hosting/installation/gcp/#option-1-kubernetes-gke","title":"Option 1: Kubernetes - GKE","text":"<p>Deploy Corridor on Google Kubernetes Engine (GKE) for a cloud-native, containerized deployment.</p> <p>Best for:</p> <ul> <li>Organizations with Kubernetes expertise</li> <li>Multi-tenant deployments with namespace isolation</li> <li>Auto-scaling and high availability requirements</li> <li>Modern cloud-native infrastructure</li> </ul> <p>View GKE Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/gcp/#option-2-virtual-machines-compute-engine","title":"Option 2: Virtual Machines - Compute Engine","text":"<p>Deploy Corridor on Google Compute Engine VMs for a traditional VM-based deployment.</p> <p>Best for:</p> <ul> <li>Organizations preferring VM-based infrastructure</li> <li>Simpler operational model</li> <li>Direct control over the operating system</li> <li>Traditional IT infrastructure patterns</li> </ul> <p>View VM Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/gcp/#common-gcp-services","title":"Common GCP Services","text":"<p>Both deployment options utilize these GCP managed services:</p> <ul> <li>Cloud SQL: PostgreSQL database for metadata storage</li> <li>Cloud Memorystore: Redis for message queuing</li> <li>Cloud Storage: Object storage for file management (or Filestore for NFS)</li> <li>Cloud Load Balancing: HTTP(S) load balancing</li> <li>Cloud DNS: Domain name management</li> <li>VPC: Virtual private cloud networking</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/#choosing-the-right-option","title":"Choosing the Right Option","text":"Factor GKE (Kubernetes) VMs (Compute Engine) Complexity Higher Lower Scalability Automatic Manual Multi-tenancy Native (namespaces) Separate VMs Operational Overhead Lower (managed K8s) Higher (OS management) Deployment Speed Faster Moderate"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/","title":"Install on GCP - Kubernetes (GKE)","text":"<p>This guide provides an overview of deploying Corridor on Google Kubernetes Engine (GKE).</p>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#background","title":"Background","text":"<p>Corridor can be deployed on GKE using Kubernetes to manage containerized services. The deployment leverages GCP's managed services for databases, storage, and networking.</p>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#prerequisites","title":"Prerequisites","text":"<ul> <li>GCP Project with appropriate permissions</li> <li><code>gcloud</code> CLI installed and configured</li> <li><code>kubectl</code> CLI installed (via gcloud components)</li> <li>Access to Corridor container images</li> <li>Sufficient GCP quota for required resources</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#required-gcp-services","title":"Required GCP Services","text":"<ol> <li> <p>Google Kubernetes Engine (GKE): Managed Kubernetes cluster</p> </li> <li> <p>Private cluster recommended</p> </li> <li>Node autoscaling configured</li> <li> <p>Cloud NAT for egress traffic</p> </li> <li> <p>Cloud SQL: PostgreSQL database for metadata</p> </li> <li> <p>PostgreSQL 14+ recommended</p> </li> <li>High availability configuration</li> <li> <p>Automated backups enabled</p> </li> <li> <p>Google Filestore: NFS storage for persistent volumes</p> </li> <li>Standard tier: 1TB+ capacity</li> <li>Accessible from GKE cluster VPC</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#optional-gcp-services","title":"Optional GCP Services","text":"<ul> <li>Cloud DNS: For domain management</li> <li>Secret Manager: For storing sensitive configuration</li> <li>Cloud Monitoring: For logging and monitoring</li> <li>Cloud Armor: For DDoS protection and WAF</li> <li>Cloud CDN: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#architecture-overview","title":"Architecture Overview","text":"<pre><code>GKE Cluster\n\u251c\u2500\u2500 Application Namespace\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-worker (Celery workers)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Cluster Components\n    \u251c\u2500\u2500 NGINX Ingress Controller\n    \u251c\u2500\u2500 cert-manager (TLS)\n    \u2514\u2500\u2500 NFS Provisioner\n</code></pre> <p>Key Components:</p> <ul> <li>Persistent Volumes: NFS-backed storage via Google Filestore</li> <li>NGINX Ingress: HTTP routing and load balancing</li> <li>cert-manager: Automated Let's Encrypt TLS certificates</li> <li>Redis: Container-based Redis service for message queuing</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#installation-overview","title":"Installation Overview","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#step-1-setup-gke-cluster","title":"Step 1: Setup GKE Cluster","text":"<ol> <li>Create GKE cluster</li> <li>Setup Cloud NAT for private cluster internet access</li> <li>Enable cluster autoscaling</li> <li>Configure authorized networks for cluster access</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#step-2-setup-supporting-infrastructure","title":"Step 2: Setup Supporting Infrastructure","text":"<ol> <li>Create Cloud SQL PostgreSQL instance</li> <li>Create Google Filestore instance for NFS storage</li> <li>Configure VPC networking and firewall rules</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#step-3-install-cluster-components","title":"Step 3: Install Cluster Components","text":"<p>Install one-time cluster-level components:</p> <ol> <li>NGINX Ingress Controller</li> <li>cert-manager for automated TLS certificates</li> <li>NFS subdir external provisioner</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#step-4-deploy-corridor","title":"Step 4: Deploy Corridor","text":"<ol> <li>Create Kubernetes namespace</li> <li>Create database in Cloud SQL</li> <li>Configure Kustomize overlay</li> <li>Create container registry pull secrets</li> <li>Apply Kubernetes manifests using Kustomize</li> <li>Configure DNS and verify deployment</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#installation-steps","title":"Installation Steps","text":"<p>Contact Corridor support for:</p> <ul> <li>Complete GKE cluster creation scripts</li> <li>Terraform/Infrastructure-as-Code templates</li> <li>Kubernetes manifests and Kustomize overlays</li> <li>Client deployment configuration examples</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#post-installation","title":"Post Installation","text":"<p>After deployment:</p> <ol> <li>Configure DNS: Point domain to Ingress external IP address</li> <li>Verify TLS: Ensure certificates are issued by cert-manager (check after 5-10 minutes)</li> <li>Create Admin User: Initialize first admin user account</li> <li>Test Connectivity: Verify all services are accessible</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#view-logs","title":"View Logs","text":"<pre><code># View application logs\nkubectl logs -n &lt;namespace&gt; deploy/corridor-app\n\n# View worker logs\nkubectl logs -n &lt;namespace&gt; deploy/corridor-worker\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#access-services","title":"Access Services","text":"<pre><code># Access pod shell\nkubectl exec -it -n &lt;namespace&gt; deploy/corridor-app -- /bin/bash\n\n# View pod status\nkubectl get pods -n &lt;namespace&gt;\n\n# View all resources in namespace\nkubectl get all -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#update-deployment","title":"Update Deployment","text":"<pre><code># Restart deployment (pull latest image)\nkubectl rollout restart deployment corridor-app -n &lt;namespace&gt;\n\n# Check rollout status\nkubectl rollout status deployment corridor-app -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#common-operations","title":"Common Operations","text":"<pre><code># View ingress and external IP\nkubectl get ingress -n &lt;namespace&gt;\n\n# Check certificate status\nkubectl get certificate -n &lt;namespace&gt;\n\n# View persistent volume claims\nkubectl get pvc -n &lt;namespace&gt;\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#gke-specific-features","title":"GKE-Specific Features","text":"<ul> <li>GKE Autopilot (optional): Fully managed Kubernetes with optimized configurations</li> <li>Node Auto-scaling: Automatic cluster scaling based on workload demands</li> <li>Cloud Monitoring Integration: Built-in logging and metrics collection</li> <li>Binary Authorization: Enforce deployment policies and image attestation</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use Preemptible Nodes: For non-production workloads (up to 80% savings)</li> <li>Enable Cluster Autoscaling: Scale down during off-hours</li> <li>Use Committed Use Discounts: For predictable production workloads</li> <li>Right-size Node Pools: Use appropriate machine types for workloads</li> <li>Use Cloud Storage lifecycle policies for backups</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnets with Cloud NAT</li> <li>Enable Binary Authorization for image verification</li> <li>Configure Network Policies for pod-to-pod traffic</li> <li>Use Private GKE Clusters with authorized networks only</li> <li>Enable Shielded GKE Nodes for secure boot</li> <li>Store secrets in Kubernetes secrets</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#example-configurations","title":"Example Configurations","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#example-dockerfile","title":"Example Dockerfile","text":"<p>Below is an example Dockerfile for containerizing Corridor applications:</p> <pre><code>FROM redhat/ubi8\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\nENV CORRIDOR_HOME=/opt/corridor\n# Disable buffering in python to enable faster logging\nENV PYTHONUNBUFFERED=1\n\n# System dependencies\nRUN yum update -y \\\n    &amp;&amp; yum install java-1.8.0-openjdk procps-ng https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm -y \\\n    &amp;&amp; yum clean all\n\n# Setup JAVA_HOME so Spark can find the java-1.8.0\nENV JAVA_HOME=/etc/alternatives/jre\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\n# NOTE: Don't use UV_COMPILE_BYTECODE=1 here. It increases the image build time significantly\n# Ref: https://docs.astral.sh/uv/guides/integration/docker/#caching\nENV UV_LINK_MODE=copy\n\nWORKDIR $CORRIDOR_HOME\nENV PATH=\"/opt/corridor/venv/bin/:$PATH\"\n\nCOPY uv.lock pyproject.toml $CORRIDOR_HOME/\n\n# Install runtime dependencies\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv venv $CORRIDOR_HOME/venv --python 3.11 --seed \\\n    &amp;&amp; source $CORRIDOR_HOME/venv/bin/activate \\\n    &amp;&amp; uv sync --active --frozen --no-install-workspace --no-group dev --no-group test --extra pyspark\n\n# Install corridor packages\nRUN --mount=target=/corridor_wheels,type=bind,source=corridor_wheels \\\n    uv pip install --python $CORRIDOR_HOME/venv --no-cache-dir /corridor_wheels/*.whl\n\n# Expose application ports\nEXPOSE 5002 5003\n\n# Health check (adjust for specific service)\nHEALTHCHECK --interval=30s --timeout=5s --start-period=60s --retries=3 \\\n    CMD curl -f http://localhost:5002/corr-api || exit 1\n</code></pre> <p>Building the Image:</p> <pre><code># Build wheel distributions first\nuv build --wheel\n\n# Build Docker image with BuildKit\ndocker build -t corridor/ggx:latest .\n\n# For multi-platform builds\ndocker buildx build --platform linux/amd64,linux/arm64 -t corridor/ggx:latest .\n</code></pre> <p>Using Different Services:</p> <p>The same image can run different Corridor services by overriding the command:</p> <pre><code># corridor-app\ncommand: [\"/opt/corridor/venv/bin/corridor-app\", \"run\"]\n\n# corridor-worker\ncommand: [\"/opt/corridor/venv/bin/corridor-worker\", \"run\"]\n\n# corridor-jupyter\ncommand: [\"/opt/corridor/venv/bin/corridor-jupyter\", \"run\"]\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-gke/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<p>Below is an example Terraform configuration for provisioning GCP infrastructure for Corridor on GKE:</p> <pre><code># terraform.tf - Main configuration\nterraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project = var.project_id\n  region  = var.region\n}\n\n# Variables\nvariable \"project_id\" {\n  description = \"GCP Project ID\"\n  type        = string\n}\n\nvariable \"region\" {\n  description = \"GCP Region\"\n  type        = string\n  default     = \"us-central1\"\n}\n\nvariable \"zone\" {\n  description = \"GCP Zone\"\n  type        = string\n  default     = \"us-central1-a\"\n}\n\nvariable \"cluster_name\" {\n  description = \"GKE Cluster Name\"\n  type        = string\n  default     = \"corridor-gke\"\n}\n\n# VPC Network\nresource \"google_compute_network\" \"vpc\" {\n  name                    = \"corridor-vpc\"\n  auto_create_subnetworks = false\n}\n\nresource \"google_compute_subnetwork\" \"subnet\" {\n  name          = \"corridor-subnet\"\n  ip_cidr_range = \"10.0.0.0/24\"\n  region        = var.region\n  network       = google_compute_network.vpc.id\n\n  secondary_ip_range {\n    range_name    = \"pods\"\n    ip_cidr_range = \"10.1.0.0/16\"\n  }\n\n  secondary_ip_range {\n    range_name    = \"services\"\n    ip_cidr_range = \"10.2.0.0/16\"\n  }\n}\n\n# Cloud NAT for private cluster\nresource \"google_compute_router\" \"router\" {\n  name    = \"corridor-router\"\n  region  = var.region\n  network = google_compute_network.vpc.id\n}\n\nresource \"google_compute_router_nat\" \"nat\" {\n  name                               = \"corridor-nat\"\n  router                             = google_compute_router.router.name\n  region                             = var.region\n  nat_ip_allocate_option             = \"AUTO_ONLY\"\n  source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\"\n}\n\n# GKE Cluster\nresource \"google_container_cluster\" \"primary\" {\n  name     = var.cluster_name\n  location = var.zone\n\n  # Private cluster configuration\n  private_cluster_config {\n    enable_private_nodes    = true\n    enable_private_endpoint = false\n    master_ipv4_cidr_block  = \"172.16.0.0/28\"\n  }\n\n  # Network configuration\n  network    = google_compute_network.vpc.name\n  subnetwork = google_compute_subnetwork.subnet.name\n\n  ip_allocation_policy {\n    cluster_secondary_range_name  = \"pods\"\n    services_secondary_range_name = \"services\"\n  }\n\n  # Remove default node pool\n  remove_default_node_pool = true\n  initial_node_count       = 1\n\n  # Maintenance window\n  maintenance_policy {\n    daily_maintenance_window {\n      start_time = \"03:00\"\n    }\n  }\n\n  # Add-ons\n  addons_config {\n    http_load_balancing {\n      disabled = false\n    }\n    horizontal_pod_autoscaling {\n      disabled = false\n    }\n  }\n}\n\n# Node Pool\nresource \"google_container_node_pool\" \"primary_nodes\" {\n  name       = \"primary-node-pool\"\n  location   = var.zone\n  cluster    = google_container_cluster.primary.name\n  node_count = 3\n\n  autoscaling {\n    min_node_count = 3\n    max_node_count = 10\n  }\n\n  node_config {\n    machine_type = \"n2-standard-4\"\n    disk_size_gb = 100\n    disk_type    = \"pd-standard\"\n\n    oauth_scopes = [\n      \"https://www.googleapis.com/auth/cloud-platform\"\n    ]\n\n    metadata = {\n      disable-legacy-endpoints = \"true\"\n    }\n  }\n\n  management {\n    auto_repair  = true\n    auto_upgrade = true\n  }\n}\n\n# Cloud SQL Instance\nresource \"google_sql_database_instance\" \"postgres\" {\n  name             = \"corridor-db\"\n  database_version = \"POSTGRES_14\"\n  region           = var.region\n\n  settings {\n    tier              = \"db-custom-4-16384\"\n    availability_type = \"REGIONAL\"\n    disk_size         = 100\n    disk_type         = \"PD_SSD\"\n\n    backup_configuration {\n      enabled    = true\n      start_time = \"03:00\"\n    }\n\n    ip_configuration {\n      ipv4_enabled    = true\n      private_network = google_compute_network.vpc.id\n    }\n  }\n\n  deletion_protection = true\n}\n\nresource \"google_sql_database\" \"database\" {\n  name     = \"corridor_db\"\n  instance = google_sql_database_instance.postgres.name\n}\n\nresource \"google_sql_user\" \"user\" {\n  name     = \"corridor\"\n  instance = google_sql_database_instance.postgres.name\n  password = var.db_password # Store in terraform.tfvars or use Secret Manager\n}\n\n# Filestore Instance\nresource \"google_filestore_instance\" \"nfs\" {\n  name     = \"corridor-nfs\"\n  location = var.zone\n  tier     = \"STANDARD\"\n\n  file_shares {\n    capacity_gb = 1024\n    name        = \"corridor\"\n  }\n\n  networks {\n    network = google_compute_network.vpc.name\n    modes   = [\"MODE_IPV4\"]\n  }\n}\n\n# Outputs\noutput \"cluster_name\" {\n  value       = google_container_cluster.primary.name\n  description = \"GKE Cluster Name\"\n}\n\noutput \"cluster_endpoint\" {\n  value       = google_container_cluster.primary.endpoint\n  description = \"GKE Cluster Endpoint\"\n  sensitive   = true\n}\n\noutput \"cloudsql_connection_name\" {\n  value       = google_sql_database_instance.postgres.connection_name\n  description = \"Cloud SQL Connection Name\"\n}\n\noutput \"filestore_ip\" {\n  value       = google_filestore_instance.nfs.networks[0].ip_addresses[0]\n  description = \"Filestore NFS IP Address\"\n}\n</code></pre> <p>Usage:</p> <pre><code># Initialize Terraform\nterraform init\n\n# Create terraform.tfvars file\ncat &gt; terraform.tfvars &lt;&lt;EOF\nproject_id = \"your-gcp-project-id\"\nregion     = \"us-central1\"\nzone       = \"us-central1-a\"\ndb_password = \"secure-password-here\"\nEOF\n\n# Plan the deployment\nterraform plan\n\n# Apply the configuration\nterraform apply\n\n# Get kubectl credentials\ngcloud container clusters get-credentials corridor-gke \\\n  --zone us-central1-a \\\n  --project your-gcp-project-id\n</code></pre> <p>Notes:</p> <ul> <li>Store sensitive variables in <code>terraform.tfvars</code> (add to <code>.gitignore</code>)</li> <li>Consider using Terraform Cloud or GCS backend for remote state</li> <li>Adjust machine types and disk sizes based on your requirements</li> <li>Review and customize network policies and firewall rules</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/","title":"Install on GCP - Virtual Machines","text":"<p>This guide provides an overview of deploying Corridor on a single Google Compute Engine (GCE) instance.</p>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#background","title":"Background","text":"<p>Corridor can be deployed on a single GCE instance running all components (app, api, workers, jupyter). This approach provides a simple deployment model suitable for organizations that prefer traditional VM-based infrastructure.</p>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#before-installation","title":"Before Installation","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#prerequisites","title":"Prerequisites","text":"<ul> <li>GCP Project with appropriate permissions</li> <li><code>gcloud</code> CLI installed and configured</li> <li>SSH access to GCE instance</li> <li>Access to Corridor installation bundle</li> <li>Sufficient GCP service quotas</li> <li>Minimum Requirements and System Dependencies are met</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#required-gcp-services","title":"Required GCP Services","text":"<ol> <li>Compute Engine: VM for running all Corridor components</li> <li>Instance size based on Minimum Requirements</li> <li>Recommended: n2-standard-8 or larger</li> <li> <p>Persistent SSD for local storage</p> </li> <li> <p>Cloud SQL: PostgreSQL database for metadata</p> </li> <li>PostgreSQL 14+ recommended</li> <li>High availability configuration</li> <li>Automated backups enabled</li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#optional-gcp-services","title":"Optional GCP Services","text":"<ul> <li>Cloud DNS: For domain management</li> <li>Secret Manager: For storing sensitive configuration</li> <li>Cloud Monitoring: For logging and monitoring</li> <li>Cloud Armor: For DDoS protection and WAF</li> <li>Cloud CDN: For static asset caching</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#architecture-overview","title":"Architecture Overview","text":"<pre><code>GCE Instance (n2-standard-8)\n\u251c\u2500\u2500 Corridor Components\n\u2502   \u251c\u2500\u2500 corridor-app (Web UI)\n\u2502   \u251c\u2500\u2500 corridor-api (API Server)\n\u2502   \u251c\u2500\u2500 corridor-worker-api (API Worker)\n\u2502   \u251c\u2500\u2500 corridor-worker-spark (Spark Worker)\n\u2502   \u251c\u2500\u2500 corridor-jupyter (JupyterHub)\n\u2502   \u2514\u2500\u2500 redis (Message Queue)\n\u2514\u2500\u2500 Local Storage\n    \u2514\u2500\u2500 Persistent SSD (/opt/corridor)\n\nCloud SQL\n\u2514\u2500\u2500 PostgreSQL Database\n    \u251c\u2500\u2500 Metadata\n    \u2514\u2500\u2500 Application Data\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#installation-steps","title":"Installation Steps","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-1-create-gce-instance","title":"Step 1: Create GCE Instance","text":"<pre><code># Create instance\ngcloud compute instances create corridor-vm \\\n  --machine-type=n2-standard-8 \\\n  --image-family=debian-11 \\\n  --image-project=debian-cloud \\\n  --boot-disk-size=100GB \\\n  --boot-disk-type=pd-ssd \\\n  --create-disk=name=corridor-data,size=100GB,type=pd-ssd \\\n  --network=default \\\n  --subnet=default \\\n  --zone=us-central1-a\n\n# Create Cloud SQL instance\ngcloud sql instances create corridor-db \\\n  --database-version=POSTGRES_14 \\\n  --cpu=4 \\\n  --memory=16GB \\\n  --region=us-central1 \\\n  --availability-type=REGIONAL \\\n  --storage-type=SSD \\\n  --storage-size=100GB \\\n  --backup \\\n  --backup-start-time=03:00\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-2-install-system-dependencies","title":"Step 2: Install System Dependencies","text":"<p>SSH into the GCE instance and run:</p> <pre><code># Update system\nsudo apt-get update\nsudo apt-get upgrade -y\n\n# Install dependencies\nsudo apt-get install -y \\\n    python3.11 \\\n    python3.11-dev \\\n    openjdk-8-jdk \\\n    redis-server \\\n    nginx \\\n    unzip\n\n# Start and enable Redis\nsudo systemctl start redis-server\nsudo systemctl enable redis-server\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-3-install-corridor-components","title":"Step 3: Install Corridor Components","text":"<ol> <li> <p>Extract the installation bundle: </p><pre><code>cd /tmp\nunzip corridor-bundle.zip\n</code></pre><p></p> </li> <li> <p>Install each component: </p><pre><code># Install Web Application Server\nsudo ./corridor-bundle/install app -i /opt/corridor\n\n# Install API Server\nsudo ./corridor-bundle/install api -i /opt/corridor\n\n# Install API Worker\nsudo ./corridor-bundle/install worker-api -i /opt/corridor\n\n# Install Spark Worker\nsudo ./corridor-bundle/install worker-spark -i /opt/corridor\n\n# Install Jupyter\nsudo ./corridor-bundle/install jupyter -i /opt/corridor\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-4-configure-components","title":"Step 4: Configure Components","text":"<ol> <li> <p>Update API configuration in <code>/opt/corridor/instances/default/config/api_config.py</code>: </p><pre><code>SQLALCHEMY_DATABASE_URI = \"postgresql://corridor:password@&lt;INSTANCE_IP&gt;:5432/corridor\"\n</code></pre><p></p> </li> <li> <p>Initialize the database: </p><pre><code>/opt/corridor/venv-api/bin/corridor-api db upgrade\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-5-create-service-files","title":"Step 5: Create Service Files","text":"<p>Create systemd service files for each component:</p> <ol> <li> <p>Web Application Server (<code>/etc/systemd/system/corridor-app.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Application Server\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-app/bin/corridor-app run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>API Server (<code>/etc/systemd/system/corridor-api.service</code>): </p><pre><code>[Unit]\nDescription=Corridor API Server\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=WSGI_SERVER=gunicorn\nExecStart=/opt/corridor/venv-api/bin/corridor-api run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>API Worker (<code>/etc/systemd/system/corridor-worker-api.service</code>): </p><pre><code>[Unit]\nDescription=Corridor API Worker\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue api\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>Spark Worker (<code>/etc/systemd/system/corridor-worker-spark.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Spark Worker\nAfter=network.target redis-server.service\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nEnvironment=C_FORCE_ROOT=1\nExecStart=/opt/corridor/venv-api/bin/corridor-worker run --queue spark --queue quick_spark\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> <li> <p>Jupyter (<code>/etc/systemd/system/corridor-jupyter.service</code>): </p><pre><code>[Unit]\nDescription=Corridor Jupyter\nAfter=network.target\n\n[Service]\nType=simple\nUser=corridor\nGroup=corridor\nEnvironment=CORRIDOR_CONFIG_DIR=/opt/corridor/instances/default/config\nExecStart=/opt/corridor/venv-jupyter/bin/corridor-jupyter run\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p></p> </li> </ol>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#step-6-start-services","title":"Step 6: Start Services","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable services\nsudo systemctl enable corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n\n# Start services\nsudo systemctl start corridor-app corridor-api corridor-worker-api corridor-worker-spark corridor-jupyter\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#monitoring-and-operations","title":"Monitoring and Operations","text":""},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#service-management","title":"Service Management","text":"<pre><code># Check service status\nsudo systemctl status corridor-app\nsudo systemctl status corridor-api\nsudo systemctl status corridor-worker-api\nsudo systemctl status corridor-worker-spark\nsudo systemctl status corridor-jupyter\nsudo systemctl status redis-server\n\n# Restart services\nsudo systemctl restart corridor-app\nsudo systemctl restart corridor-api\nsudo systemctl restart corridor-worker-api\nsudo systemctl restart corridor-worker-spark\nsudo systemctl restart corridor-jupyter\nsudo systemctl restart redis-server\n</code></pre>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Deploy in private subnet with Cloud NAT</li> <li>Use Service Accounts for GCP service access</li> <li>Configure Firewall Rules for instance access</li> <li>Enable OS Login for SSH access</li> <li>Store secrets in Secret Manager</li> <li>Enable Cloud Monitoring Agent for monitoring</li> <li>Configure Cloud SQL encryption at rest</li> <li>Enable automated backups for Cloud SQL</li> </ul>"},{"location":"technology/self-hosting/installation/gcp/gcp-vms/#example-terraform-configuration","title":"Example Terraform Configuration","text":"<pre><code># GCE Instance\nresource \"google_compute_instance\" \"corridor\" {\n  name         = \"corridor-vm\"\n  machine_type = \"n2-standard-8\"\n  zone         = \"us-central1-a\"\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-11\"\n      size  = 100\n      type  = \"pd-ssd\"\n    }\n  }\n\n  attached_disk {\n    source = google_compute_disk.data.self_link\n  }\n\n  network_interface {\n    network = \"default\"\n    subnetwork = \"default\"\n  }\n\n  service_account {\n    scopes = [\"cloud-platform\"]\n  }\n\n  metadata = {\n    startup-script = file(\"init.sh\")\n  }\n\n  tags = {\n    Name = \"corridor-server\"\n  }\n}\n\n# Persistent Disk\nresource \"google_compute_disk\" \"data\" {\n  name  = \"corridor-data\"\n  type  = \"pd-ssd\"\n  zone  = \"us-central1-a\"\n  size  = 100\n}\n\n# Cloud SQL Instance\nresource \"google_sql_database_instance\" \"corridor\" {\n  name             = \"corridor-db\"\n  database_version = \"POSTGRES_14\"\n  region           = \"us-central1\"\n\n  settings {\n    tier              = \"db-custom-4-16384\"\n    availability_type = \"REGIONAL\"\n\n    backup_configuration {\n      enabled    = true\n      start_time = \"03:00\"\n    }\n\n    ip_configuration {\n      ipv4_enabled    = true\n      private_network = google_compute_network.vpc.id\n    }\n\n    disk_size = 100\n    disk_type = \"PD_SSD\"\n  }\n\n  deletion_protection = true\n}\n\nresource \"google_sql_database\" \"corridor\" {\n  name     = \"corridor\"\n  instance = google_sql_database_instance.corridor.name\n}\n\nresource \"google_sql_user\" \"corridor\" {\n  name     = \"corridor\"\n  instance = google_sql_database_instance.corridor.name\n  password = var.db_password\n}\n</code></pre>"},{"location":"technology/self-hosting/installation/on-prem/","title":"Install On-Premises or in Data Centers","text":"<p>This guide provides an overview of deploying Corridor on-premises or in your own data center infrastructure.</p>"},{"location":"technology/self-hosting/installation/on-prem/#deployment-options","title":"Deployment Options","text":""},{"location":"technology/self-hosting/installation/on-prem/#option-1-bare-metal-virtual-machines","title":"Option 1: Bare Metal / Virtual Machines","text":"<p>Deploy Corridor directly on physical servers or virtual machines using the installation bundle.</p> <p>Best for:</p> <ul> <li>Organizations with existing VM infrastructure</li> <li>Traditional IT infrastructure patterns</li> <li>Direct control over the operating system</li> <li>Simpler operational model without containers</li> </ul> <p>View VM Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/on-prem/#option-2-docker-containers","title":"Option 2: Docker Containers","text":"<p>Deploy Corridor using Docker containers for a containerized deployment.</p> <p>Best for:</p> <ul> <li>Organizations using Docker/container infrastructure</li> <li>Simplified deployment and updates</li> <li>Resource isolation</li> <li>Modern infrastructure patterns</li> </ul> <p>View Docker Installation Guide \u2192</p>"},{"location":"technology/self-hosting/installation/on-prem/#common-requirements","title":"Common Requirements","text":"<p>Both deployment options require:</p> <ul> <li>Metadata Database: PostgreSQL, Oracle, or SQL Server</li> <li>Message Queue: Redis for Celery task orchestration</li> <li>File Storage: Network-attached storage (NAS) or local storage</li> <li>Web Server: Nginx or Apache (for production)</li> <li>Process Management: Systemd or Supervisor</li> <li>SSL Certificates: For HTTPS access</li> </ul>"},{"location":"technology/self-hosting/installation/on-prem/#choosing-the-right-option","title":"Choosing the Right Option","text":"Factor VMs (Bundle Install) Docker Containers Complexity Lower Moderate Updates Manual reinstall Image replacement Isolation Process-level Container-level Resource Usage Direct Slight overhead Portability OS-dependent OS-independent Rollback Manual Easy (previous image)"},{"location":"technology/self-hosting/installation/on-prem/#architecture-overview","title":"Architecture Overview","text":""},{"location":"technology/self-hosting/installation/on-prem/#vm-based-architecture","title":"VM-Based Architecture","text":"<pre><code>On-Premises Infrastructure\n\u251c\u2500\u2500 App Server VM(s)\n\u2502   \u2514\u2500\u2500 corridor-app (Web UI)\n\u251c\u2500\u2500 Worker VM(s)\n\u2502   \u251c\u2500\u2500 corridor-worker (API worker)\n\u2502   \u2514\u2500\u2500 corridor-worker (Spark worker)\n\u251c\u2500\u2500 Jupyter VM(s)\n\u2502   \u2514\u2500\u2500 corridor-jupyter\n\u251c\u2500\u2500 Database Server (PostgreSQL/Oracle/SQL Server)\n\u251c\u2500\u2500 Redis Server\n\u2514\u2500\u2500 NAS/File Server\n</code></pre>"},{"location":"technology/self-hosting/installation/on-prem/#docker-based-architecture","title":"Docker-Based Architecture","text":"<pre><code>Docker Infrastructure\n\u251c\u2500\u2500 corridor-app (Container)\n\u251c\u2500\u2500 corridor-worker-api (Container)\n\u251c\u2500\u2500 corridor-worker-spark (Container)\n\u251c\u2500\u2500 corridor-jupyter (Container)\n\u251c\u2500\u2500 Database (Container or External)\n\u251c\u2500\u2500 Redis (Container or External)\n\u2514\u2500\u2500 Shared Volumes (Docker Volumes or NAS)\n</code></pre>"},{"location":"technology/self-hosting/installation/on-prem/#installation-overview","title":"Installation Overview","text":""},{"location":"technology/self-hosting/installation/on-prem/#vm-installation-process","title":"VM Installation Process","text":"<ol> <li>Install system dependencies (Python 3.11+, Java 8+ for Spark)</li> <li>Extract Corridor installation bundle</li> <li>Run installation script for each component</li> <li>Configure connections to database, Redis, and storage</li> <li>Setup process management (systemd/supervisor)</li> <li>Configure web server (Nginx/Apache)</li> </ol>"},{"location":"technology/self-hosting/installation/on-prem/#docker-installation-process","title":"Docker Installation Process","text":"<ol> <li>Setup Docker Engine and Docker Compose</li> <li>Pull or build Corridor container images</li> <li>Create docker-compose.yml configuration</li> <li>Configure environment variables and volumes</li> <li>Start containers using docker-compose</li> <li>Configure reverse proxy for external access</li> </ol>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/","title":"Install using scripts","text":"<p>This section describes the method to do a manual installation of Corridor.</p> <p>There is a command-line interface available with each component that is installed:</p> <ul> <li>Web Application Server: <code>corridor-app</code></li> <li>API Server: <code>corridor-api</code></li> <li>API - Celery worker: <code>corridor-worker</code></li> <li>Spark - Celery worker: <code>corridor-worker</code></li> <li>Jupyter Notebook: <code>corridor-jupyter</code></li> </ul>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#pre-requisites","title":"Pre Requisites","text":"<p>Before starting, ensure that the Minimum Requirements and System Dependencies are met.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#installation","title":"Installation","text":"<p>To perform an install, take the installation bundle provided and extract it into a temporary location and run the <code>install</code> script inside it:</p> <pre><code>unzip corridor-bundle.zip\nsudo ./corridor-bundle/install [app | api | worker-api | worker-spark | jupyter]\n</code></pre> <p>When installing, install the specific components of the platform that are required. This will set:</p> <ul> <li>An appropriate virtual-environment inside the provided installation path</li> <li>The configuration file for the component in that section</li> </ul> <p>The complete set of arguments for the installation can be checked with <code>./corridor-bundle/install -h</code>:</p> <pre><code>usage: install [-h] [-i INSTALL_DIR] [-n NAME] component\n\npositional arguments:\n  component             The component to install. Possible values are: api,\n                        app, worker-api, worker-spark, jupyter\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRAS [EXTRAS ...], --extras EXTRAS [EXTRAS ...]\n                        The extra packages to install\n  -i INSTALL_DIR, --install-dir INSTALL_DIR\n                        The location to install the corridor package. Default\n                        value: /opt/corridor\n  --overwrite           Whether to overwrite the configs if already present.\n                        Default behavior is to create config files only if\n                        they don't already exist.\n</code></pre>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#running-the-application","title":"Running the Application","text":"<p>Once the installation of the component is done, it can be run using the provided command-line tool for that component. Also, each component has 1 or more configuration files that may need changes.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#web-application-server","title":"Web Application Server","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/app_config.py</code></li> <li>Run application server: <code>INSTALL_DIR/venv-app/bin/corridor-app run</code></li> <li>The WSGI application: <code>corridor_app.wsgi:app</code></li> </ul> <p>Note</p> <p>The web server (<code>corridor-app run</code>) can be used for production, by setting the <code>WSGI_SERVER</code> config to gunicorn or auto. Avoid using Werkzeug for production.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#api-server","title":"API Server","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run application server: <code>INSTALL_DIR/venv-api/bin/corridor-api run</code></li> <li>The WSGI application: <code>corridor_api.wsgi:app</code></li> </ul> <p>To initialize the database, <code>corridor-api db upgrade</code> needs to be run.</p> <p>Note</p> <p>The web server (<code>corridor-api run</code>) can be used for production, by setting the <code>WSGI_SERVER</code> config to gunicorn or auto. Avoid using Werkzeug for production.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#api-celery-worker","title":"API - Celery worker","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run celery worker: <code>INSTALL_DIR/venv-api/bin/corridor-worker run --queue api</code></li> </ul> <p>Note</p> <p>If the process is running as the root user, the env variable <code>C_FORCE_ROOT=1</code> needs to be set.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#spark-celery-worker","title":"Spark - Celery worker","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run celery worker: <code>INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark</code></li> </ul> <p>Note</p> <p>If the process is running as the root user, the env variable <code>C_FORCE_ROOT=1</code> needs to be set.</p>"},{"location":"technology/self-hosting/installation/on-prem/bundle-install/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li> <p>Configuration File:</p> <ul> <li>Jupyter Hub: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/jupyterhub_config.py</code></li> <li>Jupyter Notebook: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/jupyter_notebook_config.py</code></li> </ul> </li> <li> <p>Run jupyterhub server: <code>INSTALL_DIR/venv-jupyter/bin/corridor-jupyter run</code></p> </li> </ul>"},{"location":"technology/self-hosting/installation/on-prem/docker/","title":"Install using Docker","text":"<p>Corridor provides production-ready Dockerfile templates along with the installation bundle to bootstrap a docker-based installation of the platform.</p> <p>As docker configurations frequently vary based on various organizations - Please reach out to us for instructions on docker-based installs</p> <p>Various components of the platform are addressed as follows in this setup:</p> Component Implementation Options File Management Mounted to host/persistent volume Metadata Database Configurable as an independent service / Can integrate with an existing service Messaging Queue Configurable as an independent service / Can integrate with an existing service SSL Certificates <ul> <li>Mounted from host/persistent volume in read-only mode</li> <li>Copied to image during docker build stage</li> </ul> Platform Configurations <ul> <li>Mounted from host/persistent volume in read-only mode</li> <li>Via environment variables during deployment</li> </ul> Spark Configurable as an independent service / Can integrate with an existing service Jupyterhub Configurable as an independent service / Can integrate with an existing service"},{"location":"technology/self-hosting/installation/on-prem/docker/#system-minimum-requirements","title":"System: Minimum Requirements","text":"<ul> <li>Docker Engine: v20.10+</li> <li>(Optional) Docker compose</li> </ul>"},{"location":"technology/self-hosting/scaling/backups/","title":"Backups &amp; Restore","text":""},{"location":"technology/self-hosting/scaling/backups/#backups-restore","title":"Backups &amp; Restore","text":"<p>To perform backups, it is important to understand the data that the platform uses/saves.</p> <p>Data that the platform writes and the systems used for persistent storage are:</p> <ul> <li>Metadata Database</li> <li>File Management</li> <li>Data Lake</li> <li>Settings</li> </ul> <p>Other than this, Redis also contains some information - but should be considered as temporary storage and not persistent.</p>"},{"location":"technology/self-hosting/scaling/backups/#backing-up-the-systems","title":"Backing up the systems","text":""},{"location":"technology/self-hosting/scaling/backups/#metadata-database","title":"Metadata Database","text":"<p>The entire database used for Corridor needs to be backed up. There are 2 ways to run the backup:</p> <ul> <li>Use the standard backup manager as recommended/preferred for your RDBMS. For example: <code>expdp</code> for Oracle.</li> <li>Use the <code>corridor-API db export</code> command in Corridor to save your database into an SQLite file</li> </ul> <p>The <code>corridor-api db export</code> command will create an SQLite file which is a copy of your database. It includes various referential guarantees that databases provide like unique constraints, foreign keys, etc. It can be directly used as an embedded database with Corridor or can be used to import back into your RDBMS of choice. This also supports converting from 1 RBMS system to another.</p>"},{"location":"technology/self-hosting/scaling/backups/#file-management","title":"File Management","text":"<p>Depending on the system that is being used, backups need to be created. If FTP or NFS is being used - the appropriate volumes/files need to be backed up. If using a Local filesystem, the directory being used for file storage should be backed up.</p>"},{"location":"technology/self-hosting/scaling/backups/#data-lake","title":"Data Lake","text":"<p>The path configured for <code>OUTPUT_DATA_LOCATION</code> should be backed up. It contains various data files created by the platform. The recommended approach to copy files for your data lake should be used.</p>"},{"location":"technology/self-hosting/scaling/backups/#settings","title":"Settings","text":"<p>The instance folder of the configurations folder for Corridor should be backed up. This does not contain any user data, and can be reconfigured later if needed.</p> <p>This should be done for all the components that are running for Corridor - API, App worker, Workers, Jupyter, etc.</p>"},{"location":"technology/self-hosting/scaling/backups/#restoring-the-systems","title":"Restoring the systems","text":""},{"location":"technology/self-hosting/scaling/backups/#metadata-database_1","title":"Metadata Database","text":"<p>The database dump from the RDBMS system can be reloaded into another database and used.</p> <p>If the <code>corridor-api db export</code> method was used to create an SQLite file, the <code>corridor-api db import</code> command can be used to import back the SQLite file into the system where the restore is being run.</p>"},{"location":"technology/self-hosting/scaling/backups/#file-management_1","title":"File Management","text":"<p>The files copied should be kept in the same structure and the user running the corridor process should have permissions to the files on the file management system.</p>"},{"location":"technology/self-hosting/scaling/backups/#data-lake_1","title":"Data Lake","text":"<p>Ideally, the data files should be copied to the same path as the original server where the backup was taken from.</p>"},{"location":"technology/self-hosting/scaling/backups/#settings_1","title":"Settings","text":"<p>The setting files can be restored directly and permissions can be set as required.</p> <p>This should be done for all the components that are running for Corridor - API, App worker, Workers, Jupyter, etc.</p>"},{"location":"technology/self-hosting/scaling/concurrency/","title":"Multiple Corridor Workers","text":""},{"location":"technology/self-hosting/scaling/concurrency/#multiple-corridor-workers","title":"Multiple Corridor Workers","text":"<p>Corridor provides an option to run multiple workers on the same server, without the workers interfering with each other. The user needs to provide a name for each worker and worker-specific configuration in api_config.py, where each configuration is tied to the worker's name.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#custom-corridor-worker-run-command-with-a-worker-name","title":"Custom <code>corridor-worker run</code> command with a worker name","text":"<p>The worker name can be provided with the option <code>--worker-name</code> or <code>-n</code></p> <ul> <li><code>INSTALL_DIR/venv-api/bin/corridor-worker run --worker-name CUSTOM_</code></li> </ul> <p>Note</p> <p>The worker's name needs to have all capital letters. Underscores (<code>_</code>) can be part of the worker's name.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#custom-worker-configurations","title":"Custom worker configurations","text":"<p>Any worker-specific configuration is required to be added to the file:</p> <ul> <li><code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> </ul> <p>To avoid synchronization issues with other workers running on the same server. The worker configurations have to be prefixed with the worker name provided in the <code>corridor-worker run</code> command above.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#configurations","title":"Configurations","text":"<p>Taking the above <code>corridor-worker run</code> command as an example, where the worker name is <code>CUSTOM_</code>, the configurations would be:</p> <ul> <li><code>CUSTOM_WORKER_QUEUES</code></li> <li><code>CUSTOM_WORKER_PIDFILE</code></li> <li><code>CUSTOM_WORKER_LOGFILE</code></li> <li><code>CUSTOM_WORKER_PROCESSES</code></li> <li><code>CUSTOM_CELERY_WORKER_STATE_DB</code></li> <li><code>CUSTOM_CELERY_WORKER_HIJACK_ROOT_LOGGER</code></li> <li><code>CUSTOM_CELERY_WORKER_REDIRECT_STDOUTS</code></li> </ul>"},{"location":"technology/self-hosting/scaling/scalability/","title":"Scalability and Sizing","text":""},{"location":"technology/self-hosting/scaling/scalability/#scalability-and-sizing","title":"Scalability and Sizing","text":"<p>If the platform seems to be slow, there may be an infrastructure constraint in terms of resources that could be causing this. This section describes how different aspects of scalability have been kept in mind when designing the platform.</p> <p>The way to think about scalability in the platform is:</p> <ul> <li>Production</li> <li>Simulation and other jobs</li> <li>User-initiated analytics</li> <li>Application</li> </ul>"},{"location":"technology/self-hosting/scaling/scalability/#production","title":"Production","text":"<p>The production scalability again can be thought of in two parts:</p> <ul> <li>Time to execute the artifact-bundle/policy</li> <li>Throughput for the production orchestration</li> </ul> <p>The time taken for the artifact-bundle depends on the complexity of the logic written in the platform. This can be sped up by writing more efficient codes or simplifying the logic.</p> <p>The throughput for the orchestrations depends on the method of deployment of the Production layer, whether it is an HTTP API, serverless architecture, python runtime execution, etc.</p>"},{"location":"technology/self-hosting/scaling/scalability/#simulation-and-other-jobs","title":"Simulation and other jobs","text":"<p>The simulations and other jobs like Comparison, Validation, etc. Use the Celery workers to perform tasks. There are 2 parts which can be considered here:</p> <ul> <li>Number of parallel jobs to run</li> <li>Cluster nodes available for use per job</li> </ul> <p>If the number of parallel jobs is a concern, due to a large job blocking the queue of later jobs, etc. - this can be handled by adding more \"Spark - Celery Workers\" as needed. The Celery Workers are stateless and all state is maintained in the Task Messaging queue (Redis) and removing/adding more is seamless. Note that adding too many Spark - Celery workers could cause resource contention between the workers themselves and may require an increase in cluster nodes.</p> <p>If the spark jobs themselves are slow - then the issue is more likely the cluster nodes available per job. i.e. the number of Spark workers or YARN containers that are allocated per job. To make this faster, more spark nodes need to be provided to make Spark jobs run faster.</p>"},{"location":"technology/self-hosting/scaling/scalability/#user-initiated-analytics","title":"User-initiated analytics","text":"<p>The platform's Notebook section allows the user to initiate their own jobs in a free-form manner using Python/PySpark. As these jobs are user-dependent, they can utilize a large number of cluster resources if not used correctly. If this causes issues with the Simulation jobs, it is recommended to use separate YARN queues to manage the resource allocation appropriately.</p>"},{"location":"technology/self-hosting/scaling/scalability/#application","title":"Application","text":"<p>The Application's scalability is an important factor to consider when the number of concurrent users starts increasing over time. This can impact the usability of the application itself and can be scaled by identifying which portion of the application is causing the bottleneck. The Metadata Database or the API Server could be the cause of issues here. It can be easily rectified by adding more resources to these components or parallelizing them with Database Read Replicas or API Servers appropriately.</p> <p>Note</p> <p>The API Servers are stateless and can be easily scaled up/down as needed.</p>"}]}