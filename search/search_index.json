{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Corridor GenGuardX (\"GGX\")?","text":"<p>Corridor GenGuardX is a Responsible AI Governance &amp; Testing Automation Platform designed by risk managementexperts to help companies harness the benefits of GenAI.</p> <p>Enabling them to move from experimentation stage to high ROI use cases which usually require strong end-to-end pipeline testing, regulatory governance and continual human in the loop monitoring.</p> <p>Whether its leveraging external agents or in-house custom solutions. Corridor provides trust and comfort to facilitate efficient @scale deployment of GenAI - agnostic of industry.</p>      Your browser does not support the video tag.  <p>It enables the secure and compliant deployment of high-impact GenAI applications, including IVR systems, agent assist tools, and chatbots. It addresses critical challenges often overlooked by existing tools, including robust model risk management, hallucinations, PII leakage, and fair lending bias. GGX ensures comprehensive governance through rigorous full-pipeline testing, out-of-the-box standardized evaluation metrics, regulatory compliance checks, and continuous human-in-the-loop oversight. By streamlining these processes across the LLM pipeline, GGX empowers organizations to confidently transition from use-case experimentation to production, scaling trusted, high-ROI customer-facing solutions.</p> <p>GGX offers a structured framework for registering, refining, evaluating, approving, deploying, and monitoring GenAI applications. It provides:</p> <ul> <li>\u2705 Register &amp; Refine \u2013 A point-and-click interface for building GenAI applications, along with tools for optimizing prompts, retrieval-augmented generation (RAG), and pipelines.</li> <li>\u2705 Evaluate &amp; Approve \u2013 Standardized and customizable testing protocols, dashboards with human-in-the-loop testing, and approval tracking.</li> <li>\u2705 Deploy &amp; Monitor \u2013 Direct-to-production deployment with ongoing monitoring.</li> </ul>"},{"location":"#key-pillars-of-ggx-platform","title":"Key Pillars of GGX Platform:","text":""},{"location":"#1-centralized-governed-platform","title":"1. Centralized Governed Platform","text":"<ul> <li>Organized GenAI Studio for registering, evaluating, and governing LLM pipelines and its components like RAG, LLMs, Prompts</li> <li>Version tracking with comprehensive audit nd governance capabilities</li> <li>Flexibility to recreate production pipelines for iterative testing and updates</li> <li>Automated approval workflows for direct-to-production deployment</li> <li>Built-in Role Governance for proper approval, access, and monitoring rights</li> <li>Strong change management with automated documentation of all the modifications done to an object</li> <li>Automation of end-to-end testing and CI/CD deployment for rapid iteration with control</li> </ul>"},{"location":"#2-standardized-mrmfl-tests","title":"2. Standardized MRM/FL Tests","text":"<ul> <li>Curated datasets and evaluation reports to identify and mitigate risks like toxicity and bias</li> <li>Controlled dashboards for Model Risk Management (MRM) and Fair Lending (FL)</li> <li>Human Integrated Testing (HIT) for real-time interaction and validation</li> <li>Annotation Queues for labeling and performance tracking of production data</li> </ul>"},{"location":"#3-easy-ecosystem-connect-via-apis","title":"3. Easy Ecosystem Connect via APIs","text":"<ul> <li>Seamless connectivity to foundational models (e.g., OpenAI's GPT, Google Gemini, Claude)</li> <li>API integration with banking applications for Retrieval-Augmented Generation (RAG), Models etc.</li> <li>Monitoring of conversation logs with automated dashboards for transparency and compliance</li> <li>Export artifact directly to the production system</li> </ul>"},{"location":"deploy-and-monitor/","title":"Deployment and Monitoring","text":"<p>Once a pipeline is registered on the GGX platform, it can be evaluated and approved within the system. After approval, the locked pipeline artifact can be exported directly to production.</p> <p>For the pipeline in production monitoring is essential to maintaining the reliability, performance, and accuracy of systems in real-world scenarios. The platform automates production monitoring by ingesting data from relevant systems, generating performance metrics, providing intuitive monitoring dashboards and alerting capabilities. Additionally, it offers an Annotation Queues, enabling human reviewers to evaluate and label production data, with automated dashboards for key insights and statistics.</p>"},{"location":"deploy-and-monitor/annotation-queues/","title":"Annotation Queues","text":"<p>It is important to add a Human element when monitoring activity in Production as not all trends can be caught in an automated way. New trends are not always caught using programmatic approaches, but programmatic approaches are useful for repetitive tasks. Bringing a balance of both evaluation methods is key to maintaining a good healthy production system.</p> <p>The Annotation Queue capability is designed to bring in the human evaluation and labelling of production data - but also do it in an efficient way to reduce human error. This capability allows the validation of model outputs in real time by having annotators assess and label the production outcome. The capability can help track a variety of metrics, both standard and custom, on raw and annotated production data, organized digitally for the stakeholders.</p> <p>The capability significantly improves the usual approach to annotating production data using Excel and Google Sheets, creating a more transparent and auditable system to monitor live performance.</p> <p></p>"},{"location":"deploy-and-monitor/annotation-queues/#maintaining-annotation-queues-on-the-platform","title":"Maintaining Annotation Queues on the Platform:","text":"<p>The Annotation Queues module organizes all the existing queues, which are available in the top right dropdown under \"Select an annotation queue\", for easier tracking, searching, and creating new ones.</p>"},{"location":"deploy-and-monitor/annotation-queues/#annotation-queue-registration","title":"Annotation Queue Registration:","text":"<ol> <li>Click on Select an Annotation Queue dropdown in the Annotation Queue module and click on + Create New Annotation.</li> <li>Fill in important details like Name, Description.</li> <li>Upload data files to begin annotation or connect to production tables.</li> <li>Click on the Save button to register the Annotation Queue. Once registered, it will be available in the dropdown to be chosen for annotation work.</li> <li>Select the registered Annotation Queue, and it should open up the details page.</li> </ol> <p>Note:</p> <ul> <li>Multiple files can be added, and the platform will concatenate them automatically as long as the schema matches.</li> <li>The object can be edited to map the Input, Output, and Date Columns.</li> <li>The Date column should be in the format (mm/dd/yyyy).</li> <li>Platform maintains \"Is Accurate\", \"Notes\" and \"Ground Truth\" columns which can be used for labelling.</li> <li>Data will be deduplicated for the labelling process using similar Input and Output values.</li> <li>For new data uploads, \"Is Accurate\", \"Notes\" and \"Ground Truth\" will be filled using the last labelled matching sample.</li> </ul> <p>Once the registration is complete, the data is ready for annotation:</p> <ul> <li>View or export the raw data from the Data Tab.</li> <li>Start data annotation in the Labeling Tab.</li> <li>View standardized and custom reports and metrics in the Statistics Tab.</li> <li>Lastly, add guides and notes for best practices in the Instruction Tab.</li> </ul>"},{"location":"deploy-and-monitor/annotation-queues/#inviting-an-annotator","title":"Inviting an Annotator:","text":"<p>All users onboarded on the platform with access to the Annotation Queues module can collaborate on labeling. The platform tracks all label changes, allows annotators to leave notes, and provides keyboard shortcuts for faster annotations.</p>"},{"location":"deploy-and-monitor/annotation-queues/#benefits-of-annotation-queues","title":"Benefits of Annotation Queues:","text":"<ul> <li>Easy onboarding of data annotated in an external environment.</li> <li>Easy to follow and intuitive user-interface to annotate data.</li> <li>Auditable annotations, platform records all modifications to the data and labels.</li> <li>Automatic ingestion of production data.</li> <li>Enhanced Collaboration with other reviewers.</li> <li>Automated Performance and Progress tracking.</li> <li>Smart algorithms to expedite the labeling process.</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/","title":"CICD & Direct to Production","text":"<p>Typically, production execution environments are kept separate and managed to ensure 100% uptime as these are mission-critical systems for organizations.</p> <p>To tackle the air gap that these systems require - all analytics registered on the platform can be exported out from the system to be put into a Production Execution Environment. These production artifacts are locked to ensure they are not tampered with when they are promoted to production. And there is NO extra dependency required on the production side from Corridor - i.e. there is no requirement for a license key or a corridor installation in production!</p> <p>The production artifact aims to:</p> <ul> <li>Extract the logic/items registered in Corridor - to then use it outside Corridor</li> <li>Self-sufficient with all information encapsulated in the artifact</li> <li>Have minimal dependencies on the runtime-environment where the artifact is run later</li> </ul> <p>Typically a robust Continuous Deployment system is recommended to ensure that Governance is maintained while the production-artifacts are promoted. For example, you can use:</p> <ul> <li>Jenkins</li> <li>GitHub Actions</li> <li>AWS Code Pipelines</li> <li>Azure DevOps Pipelines</li> <li>Gitlab CICD</li> <li>CircleCI</li> </ul> <p>And many others.</p>"},{"location":"deploy-and-monitor/direct-to-production/#deploying-as-apis","title":"Deploying as APIs","text":"<p>If the production system supports calling APIs - the production-artifact can also be wrapped using an API layer and exposed as a REST or SOAP API which can be called by the production system.</p> <p>Typically the APIs should be considered state-less and any extra state management should be handled outside the API, but can be provided in the request payload of the API.</p> <p>Production Artifacts can be deployed as APIs using containerized solutions like:</p> <ul> <li>Docker Compose</li> <li>Kubernetes</li> <li>AWS Elastic Container Service (ECS)</li> <li>AWS Elastic Kubernetes Service (EKS)</li> <li>AWS Fargate</li> <li>Pivotal Cloud Foundry</li> <li>Azure Container Instances</li> <li>Google Cloud Run</li> <li>Google Kubernetes Engine (GKE)</li> <li>VMWare TKGI</li> </ul> <p>The API can also be deployed using application management solutions (server-based or serverless) like:</p> <ul> <li>AWS Beanstalk</li> <li>AWS Lambda</li> <li>Google App Engine (GAE)</li> <li>Google Functions</li> <li>Azure App Service</li> <li>Azure Functions</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/#custom-production-setup","title":"Custom Production Setup","text":"<p>Not all production systems support running direct Python scripts or calling APIs - and some require providing specific GenAI components in a custom interface. To handle cases like this, while Robotic Process Automation (RPA) could be used - many times it is not worth the trouble that RPA brings with it.</p> <p>Because of the transparency that Corridor's Inventory management provides, each part of the pipeline can be deployed independently. For example:</p> <ul> <li>All the LLM configurations like <code>seed</code>, <code>temperature</code>, <code>top_k</code>, etc. can be extracted from the pipeline</li> <li>The prompt templates can be extracted and provided to the production system directly</li> <li>Knowledge files from RAGs can be locked and sent to production</li> </ul>"},{"location":"deploy-and-monitor/direct-to-production/#internals-of-the-production-artifact","title":"Internals of the Production Artifact","text":"<p>The artifact generated from the platform is independent of the platform and can run in an isolated runtime environment or production environment. The information stored in the artifact is useful in many cases, where we might want to:</p> <ul> <li>check the metadata for the objects</li> <li>check the input tables and columns used</li> <li>see the lineage and relationship between the objects</li> <li>run the entire artifact or some components of the artifact to get complete or intermediate results</li> </ul> <p>The artifact consists of the following files:</p> <pre><code>model_a.b.c\n\u251c\u2500\u2500 metadata.json\n\u251c\u2500\u2500 input_info.json\n\u251c\u2500\u2500 ... (additional information about features etc. used)\n\u251c\u2500\u2500 python_dict\n|     \u251c\u2500\u2500 __init__.py\n|     \u251c\u2500\u2500 versions.json\n|     \u2514\u2500\u2500 Additional information\n\u2514\u2500\u2500 pyspark_dataframe\n \u251c\u2500\u2500 __init__.py\n \u251c\u2500\u2500 versions.json\n \u2514\u2500\u2500 Additional information\n</code></pre> <p>The metadata.json contains metadata information about the folder it is in. It will have information about the model, its inputs, its dependent variable, etc. It also has any other metadata information registered in the platform like Groups, Permissible Purpose, etc.</p> <p>The versions.json contains the versions of libraries that were used during the artifact creation - python version, any ML libraries, etc.</p> <p>The input_info.json contains the input data tables needed to be sent to the artifact's main() function.</p> <p>The <code>__init__.py</code> file inside pyspark_dataframe and python_dict folders contain the end-to-end Python function which can be used to run the entire artifact. They support different execution engines:</p> <ul> <li>Batch execution with PySpark (pyspark_dataframe)</li> <li>API execution in a Python environment (python_dict)</li> </ul> <p>To run the artifact, simply call the <code>main()</code> function in the artifact with the needed data. The <code>python_dict/__init__.py</code> contains a <code>main()</code> function into which data can be sent - in the form of a python-dict for low-latency execution. A dataset in the dictionary format is described as a dict with type/values</p>"},{"location":"deploy-and-monitor/direct-to-production/#using-corridor-runtime","title":"Using <code>corridor-runtime</code>","text":"<p><code>corridor-runtime</code> is a utility package created by Corridor, which can help us in doing the above-mentioned tasks in a very easy manner, without having to worry about extracting the artifact bundle (<code>bundle.tar.gz</code>) or the files inside the bundle.</p>"},{"location":"deploy-and-monitor/oversight/","title":"Governance Oversight","text":"<p>The Monitoring Dashboard provides users with a comprehensive overview of all registered objects on the platform which helps in providing a clear Oversight of all activities happening. It offers an interface that enables users to access snapshots and trend statistics related to various objects, jobs, and users. The dashboard provides various metadata information such as properties, attributes, and statuses of the registered objects.</p> <p>Monitoring Dashboard is an indispensable tool for review committees and project managers, offering a rich set of features to monitor, analyze, and review all elements registered on the platform efficiently.</p> <p>The \"Monitoring Dashboard\" is accessible in the \"GenAI Studio\" the sub-menu of all modules and is available at various levels (Pipelines, Models, Prompts, RAGs) - and can be used to</p>"},{"location":"deploy-and-monitor/oversight/#base-views","title":"Base Views","text":"<p>This is the default view that the organization decides to show to all users of the platform. Typically the primary cockpit is to quickly find the overall status of governance across all objects on the Platform.</p> <p>By default, the baseview contains examples of monitoring reports that can help better understand governance activities. They can be adopted or swapped with custom views that better fit the organization's interest.</p> <ul> <li> <p>Approval Status View: It presents an overview of the approval status of different objects grouped under their respective Object Groups.</p> </li> <li> <p>Review History View: The columns show the distribution of reviews based on their history, with time intervals.</p> </li> <li> <p>Last Review Status View: It focuses on the most recent review status of objects, organized by Object Groups. The columns display different review statuses, including Accepted with Flag, Accepted without Flag, Pending Acceptance, and Rejected.</p> </li> <li> <p>Schedule Review Status View: The \"Schedule Review Status View\" offers insights into the scheduled review periods for objects within Object Groups.</p> </li> </ul> <p>Explore these pre-configured views and customize them further to cater to specific monitoring needs and gain deeper insights into the platform's governance.</p>"},{"location":"deploy-and-monitor/oversight/#data-view","title":"Data View","text":"<p>This view presents the complete data for the selected object type in a tabular format. Users can easily navigate to specific objects by clicking on the rows. Additionally, they can apply filters or sort rows based on any specified column.</p> <p>The data view is a comprehensive place to access all information across the entire platform - and is the base for nearly all other types of monitoring - be it creating Custom Views or creating automated Alerts.</p>"},{"location":"deploy-and-monitor/oversight/#automated-alerts","title":"Automated Alerts","text":"<p>Alerts are defined as a set of rules that are designed to identify specific items or events that require immediate attention or further action. These rules are created based on predefined criteria, enabling the system to detect critical situations, anomalies, or deviations from expected behaviour. When the conditions specified in the alert rules are met, the system triggers events such as notifications, emails etc. Ensuring that appropriate actions can be taken promptly to address the identified issues.</p>"},{"location":"deploy-and-monitor/oversight/#creating-alerts","title":"Creating Alerts","text":"<p>On the platform, users have the flexibility to create custom alerts tailored to their specific needs. Custom alerts encompass essential properties, including name, description, conditions, severity, and associated actions.</p> <ul> <li>Click on Settings icon and select Alerts in the dropdown menu option. Now you can view a list of alerts configured for the dashboard.   </li> <li>Click on Create.</li> <li>Define the Alert Name by editing the New Alert header.</li> <li>Severity: Each custom alert can be assigned a severity level, such as High, Medium, or Low. This categorization allows users to prioritize alerts based on their importance and urgency. Different severity levels help stakeholders focus on critical issues</li> <li>Table Name: Select the table name from the dropdown menu.</li> <li>Activate: Click the activate checkbox to mark the alert as active. Activated alerts are evaluated, and the corresponding data is displayed as columns in the Monitoring Dashboard. Muted alerts are temporarily disabled and not evaluated, meaning they won't appear as columns in the dashboard during that period.</li> <li> <p>Fill in the description for the new Custom Alert.</p> </li> <li> <p>Actions   Choose an action type from the \"Type\" dropdown menu to determine the action that will be executed when the alert is triggered. The following actions can be associated with custom alerts:</p> <ul> <li>Send Notification: The system can send notifications to selected users, or user roles informing them about the triggered alert.</li> <li>Add Alert Flag: When an alert condition is met, users have the option to add a predefined flag to the object responsible for the alert. Flags serve as visual indicators to highlight objects that require attention.</li> <li>Create Review: For Approved objects, users can choose to add an ongoing review to the object's responsibility and assign reviewers. This action facilitates a thorough review process for objects flagged by the custom alert.</li> <li>Send Email: Users can configure the system to send email notifications to external users when an alert is triggered. The custom field associated with the objects should contain a string of comma-separated email addresses for users who should receive these emails.</li> </ul> </li> </ul> <p>Note</p> <p>Multiple actions can be triggered based on an alert.</p> <ul> <li>Conditions: Define the conditions that need to be met for an alert to be generated. These conditions are specified using rules based on the columns available in the Monitoring Dashboard Data. By utilizing data from the dashboard, users can set up criteria that trigger the alert when specific thresholds or patterns are detected.</li> </ul> <p></p> <ul> <li>Click on Create to register the alert. A pop-up toast message will be displayed with the text reading Alert Created Successfully.</li> </ul>"},{"location":"deploy-and-monitor/performance/","title":"Performance Tracking","text":""},{"location":"deploy-and-monitor/performance/#overview","title":"Overview","text":"<p>The Metrics Dashboard is a crucial component of the Monitoring Dashboard, providing users with valuable insights into the performance metrics of various objects on the platform.</p>"},{"location":"deploy-and-monitor/performance/#populating-metrics-dashboard-with-data","title":"Populating Metrics Dashboard With Data","text":"<ul> <li>To populate the Metrics Dashboard with recurring simulations for objects on the platform, users need to follow these steps:</li> <li>Go to the approved Object's page and navigate to the Jobs tab.</li> <li> <p>For each row in the Jobs table, an action link labelled \"Select for Metrics MD\" will be visible under specific conditions:</p> <ul> <li>The job must be the Iteration0 of a recurring job (only iteration 0 has this button)</li> <li>There should be at least two iterations left to be completed in the recurring job</li> <li>The job can be of any type (Simulation, Comparison, Validation, etc.)</li> <li>The job should not be marked as \"old\"</li> </ul> </li> </ul> <p>Note: Users are allowed to select jobs run by other individuals, enabling stakeholders to leverage this capability as needed.</p>"},{"location":"deploy-and-monitor/performance/#unselecting-a-job","title":"Unselecting a Job","text":"<ul> <li> <p>To unselect a previously selected job, follow these steps:</p> <ul> <li>Go to the approved Object's page and access the Jobs Tab</li> <li>Find the Iteration0 of the recurring job that was previously selected and click on the action link \"Unselect Job for MD.\"</li> <li>Confirmation popup will appear. Upon confirmation, the job will no longer be tracked in the Metrics Dashboard.</li> </ul> </li> </ul>"},{"location":"deploy-and-monitor/performance/#selecting-another-job-when-a-job-was-previously-selected","title":"Selecting Another Job (When a Job Was Previously Selected)","text":"<ul> <li>To select another job when a job was previously chosen, follow these steps:</li> <li>Go to the approved Object's page and access the Jobs Tab.</li> <li> <p>For each row in the Jobs tab, observe the following conditions:</p> <ul> <li>If there is no action link for MD, the job is not eligible based on the criteria mentioned earlier</li> <li>If the action link \"Select for Metrics MD\" is shown, the job is eligible for selection</li> <li>If the action link \"Unselect Job for MD\" shows, the currently selected row corresponds to the job previously chosen.</li> </ul> </li> <li> <p>Click on \"Select for Metrics MD\" for the job you wish to select.</p> </li> <li>Upon confirmation, the newly selected job will be used to track the object in the Metrics Dashboard.</li> </ul>"},{"location":"deploy-and-monitor/performance/#tracking-metrics-and-handling-job-iterations","title":"Tracking Metrics and Handling Job Iterations","text":"<ul> <li>Users can continue tracking metrics with recurring simulations. When job iterations end:</li> <li>Before job completion, notifications/emails can be sent to warn that there is only one iteration left.</li> <li>Upon job tracking completion, additional notifications/emails can be sent.</li> <li> <p>After job completion:</p> <ul> <li>If a user selects a new recurring job, it will be used from the time the new job was selected</li> <li>If a user unselects the current recurring job, it will be stopped from the time the job was selected</li> <li>If a user takes no action, the last iteration will continue to be shown in the Metrics Dashboard.</li> </ul> </li> </ul>"},{"location":"deploy-and-monitor/performance/#metrics-display-and-thresholds","title":"Metrics Display and Thresholds","text":"<ul> <li> <p>The Metrics Dashboard will exclusively showcase the latest completed job selected by the user.</p> </li> <li> <p>There won't be a default base view, ensuring that the dashboard remains streamlined and focused on user preferences.</p> </li> <li> <p>Only the metrics registered through the UI for the specific object will be presented.</p> </li> </ul> <p>Note: The thresholds utilized on the Job page during individual simulations will not be visible within the Metrics Dashboard.</p>"},{"location":"deploy-and-monitor/performance/#data-view","title":"Data View","text":"<p>This view presents the complete data for the selected object type in a tabular format. Users can easily navigate to specific objects by clicking on the rows. Additionally, they can apply filters or sort rows based on any specified column.</p> <p>The data view is a comprehensive place to access all information across the entire platform - and is the base for nearly all other types of monitoring - be it creating Custom Views or creating automated Alerts.</p>"},{"location":"deploy-and-monitor/performance/#automated-alerts","title":"Automated Alerts","text":"<p>Alerts are defined as a set of rules that are designed to identify specific items or events that require immediate attention or further action. These rules are created based on predefined criteria, enabling the system to detect critical situations, anomalies, or deviations from expected behaviour. When the conditions specified in the alert rules are met, the system triggers events such as notifications, emails etc. ensuring that appropriate actions can be taken promptly to address the identified issues.</p>"},{"location":"deploy-and-monitor/performance/#creating-alerts","title":"Creating Alerts","text":"<p>On the platform, users have the flexibility to create custom alerts tailored to their specific needs. Custom alerts encompass essential properties, including name, description, conditions, severity, and associated actions.</p> <ul> <li>Click on Settings icon and select Alerts in the dropdown menu option. Now you can view a list of alerts configured for the dashboard.</li> <li>Click on Create.</li> <li>Define the Alert Name by editing the New Alert header.</li> <li>Severity: Each custom alert can be assigned a severity level, such as High, Medium, or Low. This categorization allows users to prioritize alerts based on their importance and urgency. Different severity levels help stakeholders focus on critical issues</li> <li>Table Name: Select the table name from the dropdown menu.</li> <li>Activate: Click the activate checkbox to mark the alert as active. Activated alerts are evaluated, and the corresponding data is displayed as columns in the Monitoring Dashboard. Muted alerts are temporarily disabled and not evaluated, meaning they won't appear as columns in the dashboard during that period.</li> <li> <p>Fill in the description for the new Custom Alert.</p> </li> <li> <p>Actions   Choose an action type from the \"Type\" dropdown menu to determine the action that will be executed when the alert is triggered. The following actions can be associated with custom alerts:</p> <ul> <li>Send Notification: The system can send notifications to selected users, or user roles informing them about the triggered alert.</li> <li>Add Alert Flag: When an alert condition is met, users have the option to add a predefined flag to the object responsible for the alert. Flags serve as visual indicators to highlight objects that require attention.</li> <li>Create Review: For Approved objects, users can choose to add an ongoing review to the object's responsibility and assign reviewers. This action facilitates a thorough review process for objects flagged by the custom alert.</li> <li>Send Email: Users can configure the system to send email notifications to external users when an alert is triggered. The custom field associated with the objects should contain a string of comma-separated email addresses for users who should receive these emails.</li> </ul> </li> </ul> <p>Note</p> <p>Multiple actions can be triggered based on an alert.</p> <ul> <li> <p>Conditions: Define the conditions that need to be met for an alert to be generated. These conditions are specified using rules based on the columns available in the Monitoring Dashboard Data. By utilizing data from the dashboard, users can set up criteria that trigger the alert when specific thresholds or patterns are detected.</p> </li> <li> <p>Click on Create to register the alert. A pop-up toast message will be displayed with the text reading Alert Created Successfully.</p> </li> </ul>"},{"location":"evaluate-and-approve/","title":"Evaluations and Approval","text":""},{"location":"evaluate-and-approve/#purpose-of-evaluations","title":"Purpose of Evaluations","text":"<p>Evaluating GenAi pipelines is key to making sure they generate reliable, high-quality responses. Without a solid evaluation process, it is hard to tell if changes\u2014like tweaking prompts, removing LLMs, adjusting parameters, or refining retrieval steps\u2014are actually improving performance or breaking something. By measuring factors like relevance, hallucination rates, and latency, teams can make informed decisions about how to optimize their pipelines. Integrating evaluations into CI/CD pipelines ensures that every update is tested, so performance stays consistent and issues are caught early.</p> <p>The quality of an evaluation depends on having a well-rounded dataset and metrics. If test cases are too limited, models might appear to perform well but fail in real-world scenarios. A diverse dataset ensures that evaluation metrics truly reflect how the model will behave in production. At the end of the day, evaluations help build trust, keeping LLM applications accurate, scalable, and dependable across different use cases.</p>"},{"location":"evaluate-and-approve/#choosing-the-right-evaluation-method","title":"Choosing the Right Evaluation Method","text":"<p>There are two main types of evaluators for assessing LLM performance: automated evaluation (using LLMs or code) and human annotations. Each method serves a different purpose depending on the type of assessment needed.</p> Method How it Works Best For Automated (LLM or Code-Based) Uses an LLM to evaluate another LLM\u2019s output or code to measure accuracy, performance, or behavior. - Fast, scalable qualitative evaluation.  - Reducing cost and latency.  - Automating evaluations with hard-coded criteria (e.g., code generation). Human Annotations Experts manually review and label LLM outputs. - Evaluating automated evaluation methods.  - Applying subject matter expertise for nuanced assessments.  - Providing real-world application feedback. <p>Automated evaluation is efficient for objective assessments and large-scale testing, while human annotations provide deeper insights at a higher cost. Combining both methods can ensure a balanced and reliable evaluation process.</p>"},{"location":"evaluate-and-approve/#genai-evaluation-framework","title":"GenAi Evaluation Framework","text":"<p>Gen AI pipelines can introduce significant risks, making a robust evaluation framework essential for comprehensive testing and validation. Follow these structured steps to ensure effective evaluation:</p> Step Description Identify Risks &amp; Define Evaluation Scope - Assess potential risks across the pipeline.  - Create a comprehensive list of necessary evaluations. Define Evaluation Metrics - Align metrics with business objectives, MRM, and FL requirements.  - Implement custom metrics as needed.  - Use GGX evaluation reports curated by GenAI and risk experts. Prepare Evaluation Datasets - Ensure datasets accurately represent the use case.  - Cover all critical business scenarios for thorough validation.  - Utilize GGX datasets curated by experts to evaluate risks. Run Evaluations - Use standard/custom reports and dashboards for structured testing.  - Interpret evaluation results to refine and enhance the pipeline. Compare with Challengers - Establish alternative components and pipelines for comparison. <p>By following these steps, teams can systematically evaluate their Gen AI pipelines, mitigate risks, and enhance performance with data-driven insights. Corridor provides the ability to Run Evaluation Jobs for registered GenAi components.</p>"},{"location":"evaluate-and-approve/#introduction-to-jobs","title":"Introduction to Jobs","text":"<p>Corridor provides the ability to perform evaluations by running jobs on the registered objects and generating standardised and customized reports/metrics. Evaluations are controlled by Corridor - and run in a dedicated locked environment to ensure reproducibility of results.</p> <p>The platform supports batch evaluation of objects through simulation jobs or comparisons with similar objects on given datasets. Reports and metrics for these evaluations can be customized within Corridor under Resources \u2192 Reports Section. For more details, refer to the Reporting section.</p> <p>Once the job is completed, Corridor records all the details about specific steps of the jobs, logs resource usages and publishes the dashboard containing all the selected reports which can be shared across the team on Corridor for feedback and approval process. The results can be exported outside Corridor or used for automated documentation.</p>"},{"location":"evaluate-and-approve/#approvals-post-evaluations","title":"Approvals post Evaluations","text":"<p>The approval process is a key governance capability of the platform. After the object is fully evaluated using automated dashboard or manual tests, all the evaluation results can be shared with predefined roles within an Approval Workflow, ensuring structured reviews, feedback, and approvals for production use.</p> <p>Once approved, the object is locked, preventing any modifications within the system. This guarantees the artifact remains unchanged before being exported directly to the production system.</p>"},{"location":"evaluate-and-approve/#standardized-ggx-reports","title":"Standardized GGX Reports","text":"<p>Apart from the ability to create customized reports, Corridor already has a battery of tests registered for evaluation and validation of different components of Corridor. GGX Reports are crafted by a team of generative AI risk experts following thorough research and analysis. The list of reports is expanding with all the latest developments in the GenAi Industry. All these reports can be used if applicable and can be maintained or forked for a specific use case.</p> Component Test Name Risk Attribute Description Type Classification Response Pipeline Accuracy Inaccurate Classification or Response Evaluate ability to correctly classify utterances or provide accurate responses to user queries. MRM Yes Yes Pipeline Stability Repeated Utterances Output Variability Evaluate the ability to produce the same classification or nearly similar responses for repetitions of the same utterance. MRM Yes Yes Pipeline Stability Perturbed Utterances Output Variability Evaluate the ability to produce the same classification or similar responses across minor variations of an utterance (e.g., synonyms, grammatical mistakes). MRM Yes Yes Pipeline Bias (Comparative Prompt Analysis) Implicit / Explicit Bias Evaluate bias in outputs (e.g., classification accuracy or response accuracy) based on inferred segments for gender, race, and age. Fair Lending Yes Yes Pipeline Vulnerability Prompt Injection &amp; Prompt Leakage Evaluate the LLM pipeline\u2019s resilience against jailbreak methods for out-of-context or ambiguous inputs. MRM / Fair Lending No Yes Pipeline Toxicity Hate Speech, Toxic Language, Sarcasm Evaluate the LLM\u2019s avoidance of generating or repeating toxic language (e.g., inappropriate, offensive, or harmful language). MRM / Fair Lending No Yes Pipeline Faithfulness Hallucination, Inaccurate Facts Evaluate adherence to provided context without hallucination. MRM / Fair Lending No Yes Prompt Prompt Trust Score Prompt Quality, Prompt Vulnerability and Leakage Evaluate the prompt quality in different dimensions like Grammar, Logical Coherence, Toxicity, Bias, etc., and generate a Trust Score. MRM / Fair Lending N/A N/A Prompt Prompt Classification Accuracy Inaccurate Classification Evaluate the classification accuracy of the prompt on sample data to help in the hill-climbing process. MRM / Fair Lending Yes No LLM Vocabulary Understanding Misinterpretation, Lack of Context Awareness Determine which LLM is best at defining financial services-specific terminology across seven context categories. MRM / Fair Lending N/A N/A LLM Subject Understanding Context Misalignment, Incorrect Reasoning General subject understanding across Math, Science, etc. MRM / Fair Lending N/A N/A LLM Reasoning Capability Logical Fallacies, Incorrect Inferences Assess LLM's capabilities such as complex reasoning, knowledge utilization, language generation, etc. MRM / Fair Lending N/A N/A LLM Toxicity Understanding Failure to Detect Harmful Requests Evaluate a model's ability to identify and classify text statements that could be considered toxic across six toxicity labels. MRM / Fair Lending N/A N/A LLM Toxicity Evaluation Hate Speech, Toxic Language, Sarcasm Evaluating the tendency of LLM generating toxic replies. MRM / Fair Lending N/A N/A LLM Dialect Bias Underrepresentation of Linguistic Variants Assess which LLM responds most consistently to general queries phrased in different language dialects. MRM / Fair Lending N/A N/A LLM Gender Bias with Income as Proxy Fair Lending Risk Evaluation focuses on understanding if there is any systematic bias in assigning job titles to different genders and profiles. MRM / Fair Lending N/A N/A LLM Model Latency Scalability Issues, User Frustration Determine which LLM has the best response latency performance when varying prompt and response length. Business / Tech N/A N/A RAG Retrieval Accuracy Incorrect context retrieval, hallucination Accuracy of the retrieved documents by the RAG system. MRM N/A N/A RAG Knowledge Evaluation Lack of Coverage Coverage of different scenarios and business flows in Knowledge Data. Business N/A N/A RAG Validation Data Evaluation Lack of Coverage, Lack of Potential Scenarios Coverage of different scenarios and business flows in Evaluation Data. MRM N/A N/A"},{"location":"evaluate-and-approve/approval-workflows/","title":"Approval Workflows","text":""},{"location":"evaluate-and-approve/approval-workflows/#what-is-approval-workflow","title":"What is Approval Workflow?","text":"<p>After the development of GenAI pipelines, it is important to have a set of review processes to ensure various committees can collectively review and approve the pipeline. In Corridor, workflows that comprise multiple responsibilities can be created - and every responsibility is comprised of one or more reviewers. An object goes through the approval process from each of the responsibilities and reviewers.</p>"},{"location":"evaluate-and-approve/approval-workflows/#why-it-is-important","title":"Why it is important?","text":"<ul> <li>Creates an approval trail for model updates and changes.</li> <li>Provides clear documentation of who approved what and why.</li> <li>Ensures that only validated versions of models are deployed.</li> <li>Involves key stakeholders (developers, legal, compliance, business teams) in decision-making.</li> <li>Establishes clear ownership over AI pipeline governance.</li> </ul>"},{"location":"evaluate-and-approve/approval-workflows/#registering-approval-workflows","title":"Registering Approval Workflows:","text":"<p>Approval workflows can be created and edited within the Settings section by anyone with the right authority level (mainly Admin and Master roles). Approval workflows are specific to object types.</p> <ol> <li>Go to Settings and click on Approval Workflow Tab.</li> <li>On the listing page click on the Create button to create a new one.</li> <li>Fill in important details like Name, Attributes (Object Types, Description and Status).</li> <li>Click on the Create button at the end to register the Approval Workflow.</li> <li>Once registered the Approval Workflow can be edited to add responsibilities in the Responsibilities tab by clicking on the Add New Responsibility button.</li> <li>Decide on Veto and Editing power to reviewers.</li> <li>Select all the reviewers for the responsibility.</li> <li>Add more responsibilities if required and Save to finally register the Approval Workflow.</li> </ol> <p>Note: The external tools option is available when third-party tools are configured and allows a third-party application to be defined as a responsibility within an approval workflow.</p> <p>Once registered the Approval Workflow can be chosen while registering any object. This would ensure that the model goes through a proper approval process from all the responsibilities and reviewers before getting approved for production usage.</p> <p></p>"},{"location":"evaluate-and-approve/approval-workflows/#keeping-track-of-findingslimitations","title":"Keeping track of Findings/Limitations","text":"<p>Once an object is approved, it is locked. No further changes can be made to it. But sometimes, reviews are done with findings or limitations on the usage and need follow-ups.</p> <p>Corridor provides a flagging capability that can be used to Flag an Object (Even Post Approval) and can be created by anyone with Write access to Settings. An object can be flagged for any reason (e.g. <code>NeedShadowResultsFor2Months</code> or <code>NeedRetrainIn6Months</code>) by any member of any Workflow that oversees objects of the same type. Similarly, a flag can be dropped (i.e., deactivated) by anyone with the authority to add the flag.</p> <p>Once the flag is activated, a warning flag appears beside the object's name on the details page, indicating that the object and its assigned flags should be carefully reviewed before being used in downstream applications.</p>"},{"location":"evaluate-and-approve/comparison/","title":"Comparisons","text":"<p>Platform provides the ability to compare the registered objects for a specific task using standard and customized metrics. Using this comparison capability one can quickly evaluate a list of candidates to select the best one.</p> <p>A comparison task typically involves:</p> <ul> <li>Current object: Object which is currently selected and needs to be compared with others.</li> <li>Challenger objects: Challenger objects are objects of the same type (e.g., models can be   compared with other models, prompts can be compared with prompts, etc.).</li> <li>Data Source: A common data source on which objects would be evaluated.</li> <li>Report &amp; Metrices: Exact evaluation metrics to be compared.</li> </ul> <p>Note: On the platform one can quickly create Copy of current objects and change the definitions, swap in swap out components (Like Models, Prompts, Processing etc.) to create challengers.</p>"},{"location":"evaluate-and-approve/comparison/#how-to-run-a-comparison-task","title":"How to run a Comparison Task?","text":"<ul> <li>Register the object and its challenger versions on the platform.</li> <li>Go to the Details page of an object to be compared and click on the Run -&gt; Comparison button.</li> <li>Provide description about the comparison run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Select challenger objects which need to be compared in Dependencies Section.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li> <p>Click on Run at the bottom and wait for job completion.</p> <ul> <li>Once a job has been submitted it starts in the NEW status</li> <li>The job will go through the following statuses: COMPILING &gt; QUEUED &gt; RUNNING and finally stop at COMPLETED of FAILED</li> </ul> </li> </ul> <p>All comparison tasks are systematically recorded on the platform and displayed on the Jobs page of an object in a structured format. They can also be exported as part of the automated documentation process.</p> <p>Note: The platform allows customizing reports and dashboards specifically for comparison tasks. Note: Corridor allows running jobs in parallel and multiple threads at a time within a job to expedite the evaluation process.</p>"},{"location":"evaluate-and-approve/document-generation/","title":"Document Generation","text":"<p>As Corridor starts to understand the kind of pipeline being created, what models, prompts, etc. it is using and also starts to understand the evaluations being run on the pipeline. It becomes a great central knowledge base of all work done throughout the GenAI lifecycle.</p> <p>Corridor can export all this information into various formats (like Word or PDF) to make it easy to submit documentation and audit reports external to the system.</p> <ul> <li>Automated Documentation are generated for the Prompts, Models, Pipelines in their respective Registry</li> <li>Automated Documentation for evaluations is available on the Job Details tab.</li> </ul> <p>Simply click on the Export button and select the required format for the document to be downloaded</p>"},{"location":"evaluate-and-approve/human-testing/","title":"Human Integrated Testing","text":"<p>The Human Integrated Testing module enables comprehensive testing of GenAI pipelines both before approval (Pre-Approval) and after deployment (Post-Approval/Post-Depoyment), involving humans in the loop.</p>"},{"location":"evaluate-and-approve/human-testing/#what-is-a-pre-approval-testing","title":"What is a Pre-Approval Testing?","text":"<p>The Pre-Approval Testing allows users to test the full end-to-end pipeline in a production-like environment, providing an opportunity to manually validate the final solution before deployment. It simulates real-world scenarios by replicating end-user experiences. This module abstracts the internal technical details, presenting only the final inputs and outputs for a streamlined evaluation process.</p> <p>Note: Currently, only the chat-based pipelines registered on the platform are available for manual testing.</p> <p>It enables reviewers to capture their feedback and scores during testing, offering valuable insights for the development team to improve the solution. Additionally, all testing data can be exported from the platform for different purposes, making it easier for developers and reviewers to collaborate.</p>"},{"location":"evaluate-and-approve/human-testing/#performing-pre-approval-testing-on-the-platform","title":"Performing Pre-Approval Testing on the Platform:","text":"<p>All the chat-based pipelines are available for Pre-Approval Testing.</p> <ol> <li>Click on Start Session in the Pre-Approval Testing module.</li> <li>Choose the pipeline to be tested from a list of registered pipelines.</li> <li>Once the pipeline is selected, a new window will open where you can start interacting.    </li> <li>Once testing is completed, an experience summary can be recorded by providing an overall session rating and testing notes.    </li> </ol> <p>Note:</p> <p>More information about the Testing Session and GenAI Pipeline can be seen in the information panel on the left. There are 4 buttons in the information panel:</p> <ul> <li>View other sessions from the same pipeline.</li> <li>View the context being used in the pipeline. This is additional information that will be utilized by LLM to answer user questions. For example, it can be customer metadata being pulled from some knowledge base.</li> <li>View more details about the pipeline and the Gen AI assets used in this pipeline.</li> <li>View any configurations provided to the pipeline by the Modeler, such as temperature, random seed, etc.   </li> </ul> <p>Every interaction, feedback, and comment in the Test Session is auto-saved immediately and recorded in history and recorded along with report cards in customized groups.</p>"},{"location":"evaluate-and-approve/human-testing/#benefits-of-pre-approval-testing","title":"Benefits of Pre-Approval Testing:","text":"<ul> <li>Human-in-the-loop testing before approving the pipeline for production.</li> <li>Easy to follow and intuitive user interface to test pipelines.</li> <li>Abstraction of complex pipeline logic from MRM/Fair-Lending/Business teams.</li> <li>Record feedback and generate report cards for continuous improvement.</li> <li>Allows Multi-Turn testing for real-world scenarios.</li> <li>Load transcripts from other environments for human scoring.</li> </ul> <p>Similar to this a pipeline can be tested/monitored post approval also. Visit Deploy and Monitor section to know more on this.</p>"},{"location":"evaluate-and-approve/reporting/","title":"Reporting","text":""},{"location":"evaluate-and-approve/reporting/#what-is-a-report","title":"What is a Report?","text":"<p>A report is an analytical entity created to derive insights from a given data source. It is designed to visually communicate findings and statistical analysis through various charts and plots, enabling stakeholders to make informed decisions.</p> <p>A typical report comprises the following components:</p> <ul> <li>Data Source: The foundational datasets from which insights are derived.</li> <li>Computation Logic: The processing and transformations applied to the data, such as statistical calculations, grouping, filtering, and model-based analysis.</li> <li>Visualization Logic: The presentation of raw or computed results using charts, tables, and other visual elements to improve interpretation.</li> </ul> <p>Note: Multiple such reports could be structured in a relevant manner to create a dashboard (details below) in GGX.</p> <p></p>"},{"location":"evaluate-and-approve/reporting/#benefits-of-report-registration","title":"Benefits of Report Registration:","text":"<ul> <li>Customize reports as per MRM, Fair Lending, Business, and Development requirements.</li> <li>Add multiple reports to create use-case specific dashboards.</li> <li>Track all modifications and enable enhanced version management.</li> <li>Enhanced collaboration and approval process with MRM, FL, and other team members.</li> <li>Usage tracking using Lineage Tracking capability.</li> <li>Reusability across multiple cases reduces the need to go through the approval process again.</li> <li>Allows for quick updates to meet changing needs and business logic.</li> </ul>"},{"location":"evaluate-and-approve/reporting/#managing-reports-on-the-platform","title":"Managing Reports on the Platform:","text":"<p>The Report Registry organizes all the reports in a customized manner at a centralized location, allowing easier tracking, monitoring, and custom report creation.</p>"},{"location":"evaluate-and-approve/reporting/#report-registration","title":"Report Registration:","text":"<ol> <li>Go to Resources - Reports and click on the Create button.</li> <li> <p>A new page will appear, requiring the following fields to be filled in:</p> <ul> <li>Name</li> <li>Attributes:<ul> <li>Object Type: The object for which the report will be used (e.g., Foundation Model, Pipeline, RAG, etc.).</li> <li>Job Types: The supported simulation types (Simulation, Comparison, or Validation).<ul> <li>Simulation: Simulate the object on a given data source.</li> <li>Comparison: Compare two object's performance on a data source.</li> <li>Validation: Compare the object on multiple datasets.</li> </ul> </li> </ul> </li> <li>Properties:<ul> <li>Description: Brief description of what the report aims to accomplish.</li> <li>Group: The custom group where the report should be displayed for better organization.</li> <li>Approval Workflow: Predefined approval chain for transitioning from Draft to Approved.</li> </ul> </li> </ul> </li> <li> <p>Define the report parameters:</p> <ul> <li>Name: Intuitive name for the report parameter.</li> <li>Type:<ul> <li>String: A string constant (e.g., a specific group).</li> <li>Number: A float constant (e.g., thresholds like 0.5).</li> <li>String Column: A dataset column used for evaluation.</li> <li>Registered Object: An object already registered (e.g., GPT-4 evaluator model).</li> </ul> </li> <li>Alias: Python name for the parameter.</li> <li>Is Mandatory: Whether the parameter is required for running the simulation.</li> <li>Description: Short description of the report parameter.</li> </ul> </li> <li> <p>Write the formula for the report:</p> <ul> <li>Source Data: Defaulted to \"Simplified Data\" for structured reporting. Other options are deprecated.</li> <li>Select any additional inputs (like Global Functions, Models, etc.) to aid in writing report logic.</li> <li>Report Computation: Filtering, processing, and statistical calculations based on the job data.</li> </ul> </li> </ol> <p>The computation logic can use two default variables:</p> <pre><code>job: A Corridor Job object containing metadata.\njob.current - Access to the current object.\njob.challengers - Dictionary of challengers where the key is object name and value is object.\njob.benchmarks - Dictionary of benchmarks where the key is object name and value is an object.\njob.current.name gives the name of the current object\n\ndata: Dictionary containing job result data.\n{'current': pyspark_dataframe} for Simulation jobs.\n{'current': pyspark_dataframe, 'challenger_1': pyspark_dataframe} for Comparison jobs.\n{'current': pyspark_dataframe, 'benchmark_1': pyspark_dataframe} for Validation jobs.\n</code></pre> <ol> <li> <p>Define report visualization logic:</p> <ul> <li>Select any additional inputs (like Global Functions, Models, etc.) to assist in report logic.</li> <li>Use Plotly, Seaborn, or Matplotlib to generate figures.</li> <li>Metric outputs should return a dictionary, e.g., <code>{'current': accuracy_value}</code>.</li> <li>Add more such outputs using the Add Output button.</li> </ul> </li> <li> <p>Add any additional notes or attachments.</p> </li> <li>Click on the Create button to finalize the report registration.</li> </ol> <p>Notes:</p> <ul> <li>It is advisable to create a clear sketch of the dashboard in advance and carefully plan the types of charts and visualizations required.</li> <li>Most of the time, the String Column type will be used, as the column would already be present in the dataset being used for simulation. Clearly define the report parameter for the string column.</li> <li>If a report parameter is marked as Non-Mandatory, ensure the report code handles it correctly.</li> <li>For Comparison and Validation jobs, dynamically rendered names are used for challengers and benchmarks dictionary.</li> <li>It is recommended to have an ID column in datasets to merge challenger results with current data.</li> <li>If Figure is selected and a pandas DataFrame is returned from the report output, it is rendered as a Grid Table, enabling sorting and filtering without additional code.</li> </ul> <p>Once the report registration is completed, it can be used for creating Dashboard Views which are executed while running jobs (on the output of job data).</p> <p></p> <p></p>"},{"location":"evaluate-and-approve/reporting/#what-are-dashboard-views","title":"What are Dashboard Views?","text":"<p>Dashboard Views are created to specify and organize report outputs that should be generated from job output. It enables precise selection of reports for execution and customization of the dashboard layout, including tab structure and names, content arrangement, plot order, and the height and width of each plot..</p> <p>Dashboards can be specific to some use cases or tasks, e.g., Pipeline Performance Dashboard, Stability Dashboard, MRM-specific Dashboard, Fair Lending Dashboard, Agent Assist Risk Assessment Dashboard, and Classification Models Dashboard.</p> <p></p> <p>The Dashboard View registry organizes all the registered views into customized groups at a centralized location and allows easier tracking, monitoring, and creation of new views.</p>"},{"location":"evaluate-and-approve/reporting/#dashboard-view-registration","title":"Dashboard View Registration:","text":"<ol> <li>Click on Create button in Dashboard View Registry.</li> <li>Fill in important details like Name, Usage Option (Object Type), Properties (Description, Group, Approval Workflow).</li> <li> <p>Create custom tabs in the Tabs Section by clicking on Add Report Tab.</p> <ul> <li>Select Report Name from the dropdown list of reports filtered for the object type.</li> <li>Select Report Output from the dropdown list of outputs associated with the Report.</li> <li>Select Width and Height for the report outputs.</li> </ul> </li> <li> <p>Create additional tabs by clicking on Add Report Tab if required.</p> </li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on the Create button to finally register the function.</li> </ol> <p>Note: Dashboard Views are currently specific to object types (like Models, RAGs, Pipelines, etc.) and job types (like Simulation, Comparison, etc.).</p> <p>Once a Dashboard is registered, it will be available for execution and generating reports while running object-specific jobs.</p>"},{"location":"evaluate-and-approve/reporting/#benefits-of-dashboard-view-registration","title":"Benefits of Dashboard View Registration:","text":"<ul> <li>Use-case specific and use-case agnostic standardized dashboard creation as per MRM, Development, Fair Lending, and Business requirements.</li> <li>Customized layout for tabs and reports.</li> <li>Full Auditability and approvals for future usage.</li> <li>Enhanced reusability for similar use cases.</li> <li>Downstream usage tracking with Lineage Tracking.</li> </ul>"},{"location":"evaluate-and-approve/simulation/","title":"Simulation","text":"<p>The most common type of execution is a Simulation - used to execute analytics contained in the definition of an object that has been registered in the platform. And additionally, run dashboards and reports on that output.</p> <p>A Simulation task typically involves:</p> <ul> <li>Current object: Object which is currently selected and being tested. This can be a Pipeline, Model, RAG, or Prompt.</li> <li>Data Source: The data to run the object on.</li> <li>Report &amp; Metrices: Exact evaluation metrics to be run on the output.</li> </ul>"},{"location":"evaluate-and-approve/simulation/#how-to-run-a-simulation-task","title":"How to run a Simulation Task?","text":"<ul> <li>Register the object on the platform.</li> <li>Go to the Details page of an object to be compared and click on the Run -&gt; Simulation button.</li> <li>Provide description about the run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li> <p>Click on Run at the bottom and wait for job completion.</p> <ul> <li>Once a job has been submitted it starts in the NEW status</li> <li>The job will go through the following statuses: COMPILING &gt; QUEUED &gt; RUNNING and finally stop at COMPLETED of FAILED</li> </ul> </li> </ul> <p>All simulation tasks are systematically recorded on the platform and displayed on the Jobs page of an object in a structured format. They can also be exported as part of the automated documentation process.</p> <p>Note: The platform allows customizing reports and dashboards specifically for simulation tasks. Note: Corridor allows running jobs in parallel and multiple threads at a time within a job to expedite the evaluation process.</p>"},{"location":"integrations/","title":"Overview","text":""},{"location":"integrations/#ggx-integrations","title":"GGX Integrations","text":"<p>Corridor GGX is built with extensibility in mind and supports a wide range of integrations that allow you to seamlessly plug into existing workflows and tools.</p> <p>These integrations are organized into the following categories:</p> <ol> <li> <p>LLM Providers:    Connect to popular foundational models from leading providers to power your generative experiences.    Examples: OpenAI, Gemini (Google), DeepSeek, Anthropic, HuggingFace</p> </li> <li> <p>Agent Providers &amp; Frameworks:    Leverage pre-built agent providers or bring your own orchestration frameworks to create and manage intelligent, multi-step agent workflows with minimal setup.    Examples: Vertex AI, Salesforce Einstein, Microsoft Copilot Studio, Vapi AI</p> </li> <li> <p>Evaluation Libraries:    Plug in evaluation tools to assess your data, RAGs, pipelines, agents, etc. effectively.    Examples: Cleanlab, Ragas</p> </li> </ol>"},{"location":"integrations/evaluation_libraries/cleanlab/","title":"Cleanlab","text":""},{"location":"integrations/evaluation_libraries/cleanlab/#about-cleanlab","title":"About Cleanlab","text":"<p>Pioneered at MIT and proven at 50+ Fortune 500 companies, Cleanlab provides software to detect and remediate inaccurate responses from Enterprise AI applications. Cleanlab detection serves as a trust and reliability layer ensuring Agents, RAG, and Chatbots remain safe and helpful.</p> <p>Recognized among the Forbes AI 50, CB Insights GenAI 50, and Analytics India Magazine\u2019s Top AI Hallucination Detection Tools, the company was founded by three MIT computer science PhDs and is backed by $30M investment from Databricks, Menlo, Bain, TQ, and the founders of GitHub, Okta, and Yahoo.</p>"},{"location":"integrations/evaluation_libraries/cleanlab/#integrating-cleanlab","title":"Integrating Cleanlab","text":"<p>Simply enter your Cleanlab API key once in the Integrations section of GGX. This enables authorized users to access Cleanlab\u2019s capabilities within the platform. Once integrated, Cleanlab can be used as any other python package on the platform. </p> <pre><code>from cleanlab_tlm import TLM\ntlm = TLM()  \ntrustworthiness_score = tlm.get_trustworthiness_score(\"&lt;your prompt&gt;\", response=\"&lt;your response&gt;\")\n</code></pre>"},{"location":"integrations/evaluation_libraries/cleanlab/#potential-usage-within-ggx","title":"Potential Usage Within GGX","text":"<p>Cleanlab strengthens GGX by adding a trust and reliability layer that evaluates the accuracy, relevance, and safety of agents and their underlying components. With Cleanlab one can can systematically identify hallucinations, off-topic responses, unsafe outputs, and other critical issues across agents, RAG pipelines, and broader LLM workflows. Cleanlab\u2019s metrics and scores can be used to create insightful reports for validating systems and can also serve as guardrails to ensure your agents respond safely.</p>"},{"location":"integrations/evaluation_libraries/cleanlab/#key-use-cases","title":"Key Use Cases","text":"<ul> <li> <p>RAG System Evaluation   Automatically detect inaccuracies, assess retrieval quality, and surface knowledge gaps in RAG pipelines.</p> </li> <li> <p>Agent &amp; Pipeline Response Evaluation   Cleanlab\u2019s TLM (Trustworthiness Language Model) assigns confidence scores to LLM responses, flagging hallucinations, ambiguous answers, and unsafe content with detailed explanations.</p> </li> <li> <p>Data Quality &amp; Reliability   Identify and resolve issues such as mislabeled data, ambiguous examples, or statistical outliers ensuring your models are trained or validated on clean, trustworthy data.</p> </li> </ul>"},{"location":"integrations/evaluation_libraries/cleanlab/#example-use-case-enhancing-ivr-system-evaluation-with-cleanlab","title":"Example Use Case: Enhancing IVR System Evaluation with Cleanlab","text":"<p>Cleanlab significantly help test IVR (Interactive Voice Response) systems by providing trustworthiness scores for various AI-driven processes. It evaluates the reliability of LLM decisions in classifying caller intent and routing calls, providing responses. Furthermore, Cleanlab can assesses the accuracy of human data labels used in validation, helping to pinpoint mislabeled data. For the auto-generated call summaries and candidate responses, Cleanlab's scores can highlight problematic outputs and areas where the agent may need further improvement. </p> <p></p> <p>for e.g. Check below, Cleanlab can score how trustworthy each response is, making it easy to spot which types of questions or categories have issues.</p> <p></p>"},{"location":"integrations/evaluation_libraries/cleanlab/#want-to-learn-more","title":"Want to Learn More ?","text":"<ul> <li>Cleanlab Codex Documentation </li> <li>Cleanlab TLM Overview</li> <li>Cleanlab Studio Overview</li> </ul>"},{"location":"register-and-refine/","title":"GenerativeAI Lifecycle Management","text":""},{"location":"register-and-refine/#generativeai-lifecycle","title":"GenerativeAI Lifecycle:","text":"<p>The Generative AI development lifecycle is a systematic framework designed to help businesses create effective AI-driven solutions across various applications. Each phase is vital in ensuring the technology\u2019s accuracy, dependability, and ethical implementation.</p> <p></p> <p>Below is a breakdown of each stage and its significance:</p>"},{"location":"register-and-refine/#1-business-use-case-identification","title":"1. Business Use Case Identification","text":"<p>The first step in the lifecycle involves defining the problem statement and objectives that the solution aims to address. Clearly defining the business goals is very crucial first step as it lays the foundation for a well-structured development and validation strategy.</p>"},{"location":"register-and-refine/#2-training-and-validation-data-collection","title":"2. Training and Validation Data Collection","text":"<p>A GenAI pipeline relies heavily on high-quality, diverse datasets that have all the business scenarios for testing and validation. Data must be gathered from reliable sources, preprocessed for consistency, and cleaned to remove noise. The platform enforces strong data governance and data quality checks that ensure integrity, compliance, and fairness, which directly impact model performance and validation. Usually, sample data is created out of this for quick testing during the pipeline development phase to expedite iterations.</p>"},{"location":"register-and-refine/#3-pipeline-development","title":"3. Pipeline Development","text":"<p>Developing an AI pipeline is an iterative process aimed at rapidly building the first version of prompts, models, and Retrieval-Augmented Generation (RAG) components to address a specific business use case. Once the initial version is ready, incremental improvements can be made by refining each component and analyzing performance gains.</p> <ul> <li> <p>Crafting the Initial Prompt   Designing an effective prompt is crucial for generating accurate and relevant responses. However, prompt engineering is an iterative process that requires multiple refinements as development progresses.</p> </li> <li> <p>Connecting to External Knowledge   Enhancing model responses with external knowledge sources improves contextual accuracy. In GenAI pipelines, this is achieved using RAG techniques. Building a robust RAG architecture ensures the pipeline retrieves the most relevant information, significantly impacting overall accuracy.</p> </li> <li> <p>Choosing the Best LLM for the Use Case   Selecting an optimal LLM depends on various factors such as the business problem, expected outcomes, cost, and operational efficiency. Choosing the right model ensures the pipeline aligns with business objectives.</p> </li> <li> <p>Pipeline Creation   Once the foundational components are established, end-to-end pipeline development begins. Often creating multiple pipeline versions and comparing them helps in selecting the best-performing one. Once can swap in swap out multiple building blocks of the pipeline to get to the best state.</p> </li> <li> <p>Experimentation with Algorithms and Settings   Optimizing a pipeline requires experimenting with different models, algorithms, and hyperparameter settings (e.g., temperature, top-k, top-p). Fine-tuning these parameters ensures the model generates responses that align with performance goals.</p> </li> </ul>"},{"location":"register-and-refine/#4-evaluations-automated-and-human-based","title":"4. Evaluations - Automated and Human-based","text":"<p>Before deploying a GenAI pipeline, it must be rigorously tested for accuracy, fairness, stability and robustness. A combination of automated evaluation techniques and human assessments ensures comprehensive validation. Tracking performance metrics and refining the model based on evaluation results helps maintain reliability and transparency.</p>"},{"location":"register-and-refine/#5-move-to-production","title":"5. Move to Production","text":"<p>Once the pipeline is validated, it is deployed into production environments where it integrates with existing business systems so that can be used for real-world decision-making.</p>"},{"location":"register-and-refine/#6-production-monitoring","title":"6. Production Monitoring","text":"<p>Continuous monitoring is crucial to maintaining performance and mitigating issues such as response drift and biases. Implementing monitoring tools ensures sustained accuracy and compliance with business and regulatory requirements.</p>"},{"location":"register-and-refine/#how-does-corridor-help-with-genai-lifecycle-management","title":"How does Corridor help with GenAI Lifecycle Management?","text":"<ul> <li>Data Integration: Supports validation and testing data registration by integrating with production data lakes and in-house storage solutions.</li> <li>Component Inventory: Maintains a registry of essential GenAI pipeline components, including models, RAGs, prompts, and use-case-specific pipelines to enhance reusability and develop smaller building blocks.</li> <li>Governance and Collaboration: Provides features such as version management, change history, approvals, ongoing reviews, lineage tracking, impact assessment, and team collaboration to enhance governance, experiment tracking, MRM, FL and business approvals.</li> <li>Evaluation and Reporting: Enables assessment of complete pipelines and individual components using standardized (tailored for MRM, Fair Lending, and business needs) and custom reports.</li> <li>Human-Integrated Pre-Production Testing: Facilitates real-world-like testing by allowing different teams to conduct robust validation before deployment.</li> <li>Artifact Export and Documentation: Supports production artifact export and ODD generations.</li> <li>Monitoring: Connects to production data sources for response labelling and automated performance monitoring and alerting.</li> </ul>"},{"location":"register-and-refine/lineage-tracking/","title":"Lineage Tracking","text":"<p>Lineage tracking records and visualizes the complete lifecycle of an object and depicts the flow of execution, showing how objects interact, are transformed and executed. It captures its origins, building blocks used, transformations, and usages across the organization.</p>"},{"location":"register-and-refine/lineage-tracking/#why-lineage-tracking-is-important","title":"Why Lineage Tracking is Important?","text":"<ul> <li>Ensures Traceability by providing a clear link between all artefacts, ensuring transparency in dependencies and usages.</li> <li>Easier Impact Assessment for modifications.<ul> <li>Provides a clear view of all dependent objects that would be affected by changes.</li> <li>Identifies necessary updates across dependencies to introduce changes.</li> </ul> </li> <li>Enhances Collaboration by providing visibility into how different teams and models interact within the pipeline.</li> <li>Ensures Compliance with regulations like the EU AI Act (Article 13 &amp; 14), which mandates transparency and human oversight.</li> <li>Identifying redundant processes.</li> </ul>"},{"location":"register-and-refine/lineage-tracking/#lineage-structure-on-ggx-platform","title":"Lineage Structure on GGX Platform:","text":"<p>A Lieage is created and automatically maintained by the platform as soon as an object is registered. The lineage graph which is present on the details page is a horizontal-tree representation that maps how an object is created, used, and executed within the pipeline.</p> <p>It consists of three key elements:</p> <ul> <li>Precedents (Inputs): Represent the sources or dependencies that contribute to the creation of an object.</li> <li>Dependents (Usage): Indicate other objects that rely on a given object.</li> </ul> <p></p>"},{"location":"register-and-refine/prompt-optimization/","title":"Prompt Optimization","text":"<p>Prompt optimization is the process of refining prompts to get better and more accurate responses from AI models. When we create prompts for LLMs to handle complex tasks, the goal is to make intent as clear as possible. However, fully conveying every detail in one attempt is extremely difficult. The process often involves repeated testing and refining, making small adjustments until the model delivers the desired outcome.</p> <p>This process of making changes testing impact and maintaining a clear track record of all the experiments can be challenging if done manually. The GGX Platform simplifies and automates this process by introducing Hill Climbing Experimentations.</p>"},{"location":"register-and-refine/prompt-optimization/#what-is-hill-climbing","title":"What is Hill Climbing?","text":"<p>Hill Climbing is a method used to solve optimization problems by gradually improving a solution. It starts with an initial guess and makes small adjustments, keeping any changes that lead to a better result. This process continues until no further improvements can be made, similar to climbing a hill by always moving upward.</p>"},{"location":"register-and-refine/prompt-optimization/#how-it-works","title":"How It Works?","text":"<ul> <li>Initialization \u2013 Start with an initial prompt.</li> <li>Evaluation \u2013 Run an evaluation using fixed components:<ul> <li>Predefined LLM with set hyperparameters</li> <li>Standardized metrics for performance measurement</li> <li>A consistent dataset for testing</li> </ul> </li> <li>Modification \u2013 Make small, targeted improvements to the prompt.</li> <li>Comparison \u2013 Rerun the evaluation and measure performance changes.</li> <li>Update \u2013 If the new prompt improves results, it becomes the new baseline.</li> <li>Iteration \u2013 Repeat the process until the desired performance is achieved.</li> </ul>"},{"location":"register-and-refine/prompt-optimization/#benefits-of-hill-climbing-capability","title":"Benefits of Hill-Climbing Capability:","text":"<ul> <li>The platform maintains clear logs of all prompt updates and their impact on model performance.</li> <li>Once the best-performing prompt is identified, it can be synced back to the base pipeline.</li> <li>Enables multiple team members to collaborate simultaneously on optimization.</li> <li>Customizable reports for tracking the Hill-Climbing progress.</li> </ul> <p>Note: The platform also provides flexibility to integrate external prompt optimizers like Google Prompt Optimizer.</p>"},{"location":"register-and-refine/prompt-optimization/#how-to-run-hill-climbing-tasks-on-platform","title":"How to run Hill Climbing Tasks on Platform?","text":"<ul> <li>Go to object Details page.</li> <li>Click on the Run -&gt; Hill Climbing button.</li> <li>Provide description about the run.</li> <li>Select Dashboard to be evaluated in Dashboard Selection.</li> <li>Make changes to the prompts which are part of the object.</li> <li>Prepare the data in Data Sources which will be used for evaluation.</li> <li>Click on Run at the bottom and wait for job completion.</li> <li>Once the job is completed one can check the optmization progress in Dashboards.</li> </ul>"},{"location":"register-and-refine/collaboration/","title":"Collaboration","text":"<p>The platform has been designed in a way that enhances collaboration among teams in the development, testing and monitoring of various GenAi pipelines. The assets from various workstreams can be organized at a central location enabling developers, reviewers and testers to build, iterate, and reuse together.</p>"},{"location":"register-and-refine/collaboration/#requesting-object-access-and-sharing-objects","title":"Requesting Object Access and Sharing Objects:","text":"<p>Registered objects in Draft/Pending Approval statuses can be shared with users on the platform or users can raise the access request.</p> <p>There are currently two types of accesses (Read and Write) that can be requested or provided.</p> <ul> <li>Read - With Read access, the user can View, Run Evaluations, Download Documentation and Artifacts, and Reuse in another object.</li> <li>Write- With Write access, the user can Edit, Delete, Send for Approval, and Share with others.</li> </ul> <p>When requesting access, the user can choose to request access to the object only or request access to the object along with its lineage.</p> <p>Important Notes:</p> <ul> <li>Users can only request access to the objects they can potentially have access to based on the user role configuration in Settings.</li> <li>Re-sharing of objects with complete lineage is required if new objects are added to it.</li> <li>When an object is shared a notification is sent to the Receiver that an object has been shared.</li> <li>A Non-Owner will not be able to share the object if he/she has 'Read' permissions only.</li> <li>Users can also share access to objects using jupyter notebook using the Corridor package.</li> <li>Accesses granted through share can be changed/revoked by either the object owner or anybody with write access to the object.</li> </ul>"},{"location":"register-and-refine/collaboration/#access-management","title":"Access Management:","text":"<p>The Role-Based Access Management capability is available to govern how users, with a given role access, can use the platform. Every onboarded user on the platform is assigned a User Role.</p> <p>Roles can be created based on the following framework:</p> <ul> <li>The platform is structured as a set of modules (Data Vault, GenAI Studio, Resources, etc.). The modules can be Enabled, Disabled or Hidden.</li> <li>Each module comprises different pages (e.g., Table Registry, Quality Procedure, Prompts, RAGs, Reports, etc.). The pages can be Enabled, Disabled or Hidden.</li> <li>Within each page, objects such as Tables, Models, Pipelines, and Reports can be accessed.</li> <li>Objects have properties and attributes on which access can also be controlled.</li> <li>Access can be of different types: read, write, approve, and superuser.</li> <li>Access to each of the above components can be granted independently using the Roles capability.</li> </ul> <p>Note: Granular access control can be given to roles (such as restricting access to a particular collection of objects) using custom rules.</p>"},{"location":"register-and-refine/collaboration/#integrate-external-updates-to-objects","title":"Integrate External Updates to Objects:","text":"<p>The registered object definition can be exported from the platform, modified externally, and re-synced using Corridor commands. The platform automatically tracks and records all external changes for clarity and consistency.</p>"},{"location":"register-and-refine/collaboration/#grouping-objects","title":"Grouping Objects:","text":"<p>Groups provide a way to classify objects for control and display purposes. Groups are specific to object types.</p> <ul> <li>When defining Roles, Administrators can use groups to specify access rights. Different teams can create custom groups that are accessible only to the relevant team members to register, test and monitor their assets.</li> <li>In the registries objects are displayed by group thus making things easy to find and work on.</li> </ul>"},{"location":"register-and-refine/collaboration/#workspaces","title":"Workspaces:","text":"<p>The platform enables the creation of multiple workspaces, allowing teams to work independently without visibility into each other's work.</p>"},{"location":"register-and-refine/collaboration/#monitoring-dashboard-and-alerting","title":"Monitoring Dashboard and Alerting:","text":"<p>The platform enables the creation of customized dashboards for key stakeholders and top management, providing a bird's-eye view of activities across different teams and stages of the application lifecycle.</p>"},{"location":"register-and-refine/inventory-management/","title":"Inventory Management","text":""},{"location":"register-and-refine/inventory-management/#overview","title":"Overview","text":"<p>The platform allows registering, Tracking and Monitoring of Data and GenAi assets (like RAG, Models, LLMs and Pipelines) at a centralized location.</p>"},{"location":"register-and-refine/inventory-management/#why-inventory-management-is-helpful","title":"Why Inventory Management is Helpful?","text":"<ul> <li>Governance and Compliance: Helps track AI models, datasets, and dependencies for regulatory audits.</li> <li>Reusability &amp; Efficiency: Prevents duplication of efforts by enabling teams to reuse registered and approved assets and standardized inventories reducing onboarding time for new teams.</li> <li>Security &amp; Access Control: Centralized inventories allow proper role-based access management.</li> <li>Monitoring &amp; Continuous Improvement: Ensures GenAI systems can be tested before moving to production and periodically monitored post-production.</li> </ul>"},{"location":"register-and-refine/inventory-management/#asset-regesteries","title":"Asset Regesteries:","text":"<p>Read more on different registries below:</p> <ul> <li>Data Inventory</li> <li>LLMs and Models</li> <li>Prompts</li> <li>RAGs</li> <li>End-to-End Pipeline Assets</li> </ul>"},{"location":"register-and-refine/inventory-management/#how-platform-helps-in-managing-inventories","title":"How Platform Helps in Managing Inventories?","text":"<p>The platform offers extensive capabilities to streamline onboarding and efficiently manage assets.</p> <ul> <li>Multiple registries are available to centralize and manage smaller, reusable components of the pipeline.</li> <li>Customized groups for creating assets within a registry.</li> <li>Permissible Purpose Tracking that enables automatic validation to ensure components are used only for authorized purposes.</li> <li>Flexible and granular access management.</li> <li>Change History tracking for registered assets.</li> <li>Lineage Tracking of registered assets.</li> <li>Sharing of assets within and across teams.</li> <li>Creating Custom Fields for the registry apart from the default ones.</li> </ul>"},{"location":"register-and-refine/inventory-management/#metadata-tagging","title":"Metadata Tagging","text":"<p>When any item is added to an entity - during the registration, various fields can be tagged to that object. Some basic fields are mandatory - for example:</p> <ul> <li>Alias - A Python variable name to refer to the object by</li> <li>Type - The data type that the object returns</li> <li>Description - A free format field that can be used to describe the object being created.</li> <li>Group - Useful to organize items, making them easier to search and find later</li> <li>Permissible Purpose - A governance tracking mechanism to ensure items are used correctly</li> <li>Location (of Data) - A data lake location where the data resides and can be fetched from</li> <li>Training &amp; Validation Data - Used when creating models</li> </ul> <p>New fields can be added to any of the registries to facilitate better inventory management in Settings &gt; Fields &amp; Screens section. Fields of various types can be added:</p> <ul> <li>Short Text</li> <li>Long Text</li> <li>Date Time</li> <li>File</li> <li>Single Select</li> <li>Multi Select</li> <li>Multiple Files</li> </ul> <p>They can be marked as mandatory and customized with descriptions, placeholder values, default values, etc. and even be made mandatory to fill in. Values for fields can also be programmatically computed - with Python formulae.</p>"},{"location":"register-and-refine/inventory-management/#data-type","title":"Data Type","text":"<p>Data Types on the platform are useful to declare clear types that can be used for documentation. Data Types are very flexible on the platform. The types supported are:</p> <ul> <li> <p>Scalar Types:</p> <ul> <li>Numerical</li> <li>String</li> <li>DateTime</li> <li>Boolean</li> </ul> </li> <li> <p>Array Types:</p> <ul> <li>Array[Numerical]</li> <li>Array[String]</li> <li>Array[Array[DateTime]]</li> <li>and other types can be created by mixing existing types ...</li> </ul> </li> <li> <p>Struct Types:</p> <ul> <li>Struct[decision: String, ranking: Numerical]</li> <li>Struct[amt: Array[Numerical], flag: Boolean]</li> <li>Struct[info: Map[Numerical,String],details: Struct[id: String,date: DateTime,age: Numerical]]</li> <li>and other types can be created by mixing existing types ...</li> </ul> </li> <li> <p>Map Types:</p> <ul> <li>Map[Numerical, String]</li> <li>Map[String, Numerical]</li> <li>Map[Numerical, Array[Boolean]]</li> <li>and other types can be created by mixing existing types ...</li> </ul> </li> </ul>"},{"location":"register-and-refine/inventory-management/global-functions/","title":"Global Functions","text":""},{"location":"register-and-refine/inventory-management/global-functions/#what-are-global-functions","title":"What are Global Functions?","text":"<p>Global function gives the ability to execute a set of analytical operations multiple times using different objects as inputs without having to rewrite those operations every time. It supports inputs and outputs of any type, including DataFrames, dictionaries, and other standard Python data types, without mandatorily requiring predefined input-output formats.</p> <p>Global Function can be used across the platform in GenAI Studio, Reports, Simulation Data-Sources, or even within other Global Functions, offering enhanced reusability.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#maintaining-global-functions-on-the-platform","title":"Maintaining Global Functions on the Platform:","text":"<p>The Global Function registry organizes all the registered utilities into customized groups at this centralized location, allowing easier tracking, monitoring, and new function creation.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#global-function-registration","title":"Global Function Registration:","text":"<ol> <li>Click on Create button in Global function registry.</li> <li>Fill in important details like Name, Attribute (Output Type, Alias), Properties (Description, Group, Approval Workflow).</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select other global functions if required in Inputs section to help in writing logic.</li> <li>Write code logic in definition source.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Click on Save button to finally register the function.</li> </ol> <p>Note: Output type is not mandatory for function registry. If missing platform allows returning any type including dataframes, dictionaries etc.</p> <p>Once registration is completed, the Global Functions can be used all across the platform in GenAI Studio, Report Writing, etc.</p>"},{"location":"register-and-refine/inventory-management/global-functions/#benefits-of-global-functions-registration","title":"Benefits of Global Functions Registration:","text":"<ul> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Better collaboration for continuous model building and testing.</li> <li>Full auditability and approvals for future usages.</li> </ul>"},{"location":"register-and-refine/inventory-management/model-catalog/","title":"Model Catalog","text":""},{"location":"register-and-refine/inventory-management/model-catalog/#what-is-a-model","title":"What is a Model?","text":"<p>A model is a software program that uses algorithms/rules to make informed decisions, predictions, or generations based on a set of inputs without being given explicit instructions for every scenario. (e.g., ML models, Lookup tables, If-Else rules, LLMs, etc.)</p> <p>A model typically includes one or more of the following components:</p> <ul> <li>Model file: Stores learned weights/parameters, lookup tables, tensors, and other important data to be used for initializing the model.</li> <li>Initialization Logic: Prepares the model or client for processing multiple inputs if needed.</li> <li>Scoring Logic: Code that applies the initialized model to provide inputs to generate/predict responses.</li> </ul> <p>GGX supports the registration of various types of models:</p> <ul> <li>API-based models: Connect to externally hosted models like OpenAI, Gemini, etc., using APIs.</li> <li>Python-based models: Lightweight Python logic using various libraries or rule-based models.</li> <li>Custom models: Uploaded model files, including Scikit-learn models, NLP models like BERT, and any fine-tuned models.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/model-catalog/#managing-models-on-the-platform","title":"Managing Models on the Platform:","text":"<p>The Model Catalog organizes all the registered models into customized groups at this centralized location, allowing easier tracking, monitoring, and model creation.</p>"},{"location":"register-and-refine/inventory-management/model-catalog/#registering-a-model","title":"Registering a Model:","text":"<ol> <li>Click on Create button in Model Catalog.</li> <li>Fill in important details like Name, Attributes (Output Type, Alias), Properties (Group, Permissible Purpose, Description, Approval Workflow).</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select registered resources (like Model, Global Functions, Prompts, etc.) to use in model definition.</li> <li>Select Input Type (API-Based, Python-based, or Custom registration).</li> <li>Upload weights if required. Define model logic by writing code in Initialization Logic and Scoring Logic.</li> <li>Select Training and Validation data(if any) used for building or testing the models.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on Save to complete the registration process.</li> </ol> <p>The registered models can be evaluated in the Model Catalog or used in downstream objects (like RAG, Model, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/model-catalog/#benefits-of-model-registration","title":"Benefits of Model Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered models using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> <li>Fingerprinting of external API connectivity.</li> <li>Better collaboration for continuous model building and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/pipelines/","title":"Pipelines","text":""},{"location":"register-and-refine/inventory-management/pipelines/#what-is-a-pipeline","title":"What is a Pipeline?","text":"<p>A pipeline is a use-case-specific combination of multiple reusable components such as Models, RAGs, Prompts, Guardrails, and Other Pipelines. It processes inputs through a logically structured sequence of these components to generate or predict an output.</p> <p>Pipelines in GGX can be categorized into two types:</p> <ul> <li>Chat-based Pipelines: Maintain history and context across multiple user messages and pipeline responses, enabling context-aware interactions.</li> <li>Free-Flow Pipelines: Process one input at a time, generating a single output without retaining context from previous interactions.</li> </ul> <p>A typical pipeline consists of:</p> <ul> <li>Resources: Like Model, RAGs, Prompts, Guardrails, Other Pipelines, etc.</li> <li>Pipeline Initialization Logic: Prepares the pipeline for multiple requests if needed.</li> <li>Pipeline Scoring Logic: Code that applies the initialized pipelines to provided inputs and uses the various resources to generate/predict responses.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/pipelines/#managing-pipelines-on-the-platform","title":"Managing Pipelines on the Platform","text":"<p>The Pipeline Registry organizes all the registered pipelines into customized groups at a centralized location, allowing easier tracking, monitoring, and new pipeline creation.</p>"},{"location":"register-and-refine/inventory-management/pipelines/#registering-a-pipeline","title":"Registering a Pipeline:","text":"<ol> <li>Click on Create button in the Pipeline Registry.</li> <li>Fill in important details like Name, Attributes (Output Type, Alias, Pipeline Type), Properties (Group, Permissible Purpose, Description, Approval Workflow)</li> <li>Define Input Arguments or Configs along with their types and default values.</li> <li>Select registered resources (like Model, Global Functions, Prompts, RAGs, etc.) to use in pipeline definition.</li> <li>Select Input Type (Python-based or Custom registration).</li> <li>Upload custom files/models if required and define pipeline logic by writing code in Initialization Logic and Scoring Logic.</li> <li>Add notes and attach documentation if available in the Additional Information section.</li> <li>Lastly, click Save to complete the registration process.</li> </ol> <p>Note:</p> <ul> <li>The output type for Chat pipelines is fixed as a dictionary <code>{\"output\": string, \"context\": custom type}</code>. The context type can be defined in the Formula Section.</li> <li>For chat pipelines, there is a default argument called <code>user_message</code>.</li> <li>For chat pipelines, <code>history</code>, <code>context</code>, and <code>user_message</code> are available by default, maintaining historical messages in the standard OpenAI format (<code>Struct[content: String, role: String]</code>), context for information retention across turns, and <code>user_message</code> for the current message.</li> </ul> <p>Registered pipelines can be evaluated in the Pipeline Registry, Human Integrated Testing, or used in downstream pipelines. Live monitoring of pipelines can be done Monitoring Module and Annotation Queues.</p> <p></p>"},{"location":"register-and-refine/inventory-management/pipelines/#benefits-of-pipeline-registration","title":"Benefits of Pipeline Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered pipelines using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring becomes easier.</li> <li>Human Integrated Testing and feedback logging for chat-based pipelines.</li> <li>Extract ready-to-productionize executable artifacts.</li> <li>Better Collaboration for continuous development and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/prompts/","title":"Prompt Registry","text":""},{"location":"register-and-refine/inventory-management/prompts/#what-is-a-prompt","title":"What is a Prompt?","text":"<p>A prompt is a natural language instruction provided to a generative model to direct its response or produce a desired outcome. It may include questions, commands, contextual details, few-shot examples, or partial inputs for the model to complete or extend.</p> <p>A prompt typically includes:</p> <ul> <li>Prompt Template: An instruction containing placeholders.</li> <li>Input Arguments: Dynamic inputs that replace the placeholders.</li> </ul> <p>Note: System prompts might not always have Input Arguments.</p> <p>Additionally, to generate the prompt using the template and input arguments, there should be a:</p> <ul> <li>Creation Logic: Code that generates, retrieves, or formats the input arguments and fills them into the template.</li> </ul> <p>Note: If a prompt contains no processing or formatting logic, the creation logic can directly return the prompt template.</p> <p></p>"},{"location":"register-and-refine/inventory-management/prompts/#managing-prompts-on-the-platform","title":"Managing Prompts on the Platform:","text":"<p>The Prompt Registry organizes all the registered prompts into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/prompts/#registering-a-prompt","title":"Registering a Prompt:","text":"<ol> <li>Click on Create button in Prompt Registry.</li> <li>Fill in important details like Name, Attributes (alias), Properties (Description, Group, Permissible Purpose, Approval Workflow).</li> <li>Select registered resources (like Model, Global Functions, etc.) which can help with prompt creation. These resources can be used in the prompt creation logic section.</li> <li>Define the Input Arguments, Prompt Template, Prompt Creation Logic.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on the Save button to complete the registration process.</li> </ol> <p>The registered prompts can be evaluated in the Prompt Registry or can be used in downstream objects (like RAG, Model, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/prompts/#benefits-of-prompt-registration","title":"Benefits of Prompt Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Perform evaluations using standardized and custom validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> </ul>"},{"location":"register-and-refine/inventory-management/rags/","title":"RAGs","text":""},{"location":"register-and-refine/inventory-management/rags/#what-is-a-rag","title":"What is a RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is an AI approach that helps language models by integrating a retrieval mechanism that fetches relevant external information in real-time. This information allows the model to generate more accurate, up-to-date, and context-aware responses beyond its pre-trained knowledge.</p> <p>The retrieval mechanism in a RAG system typically consists of:</p> <ul> <li>Knowledge Source: A repository of external information. This can include documents, vector databases, Neo4j, or other structured/unstructured data sources.</li> <li>Initialization Logic: Prepares the knowledge source or client for processing multiple retrieval requests if needed.</li> <li>Retrieval Logic: A set of algorithms designed to fetch the most relevant information based on provided inputs. It often leverages embedding models, similarity functions, and ranking techniques to define the retrieval process.</li> </ul> <p>GGX supports the registration of various types of Retrieval Systems:</p> <ul> <li>API-based Retrieval: Communicates with external knowledge sources like Neo4j, vector databases, etc., using APIs to retrieve information from outside environments.</li> <li>Python-based Retrieval: For lightweight Python logic using various libraries or rule-based retrieval systems.</li> <li>Custom Retrieval: Leverages uploaded knowledge sources like CSV, Vector databases, etc., to retrieve information.</li> </ul> <p></p>"},{"location":"register-and-refine/inventory-management/rags/#managing-rags-on-the-platform","title":"Managing RAGs on the platform:","text":"<p>The RAG Registry organizes all the registered RAGs into customized groups at this centralized location, allowing easier tracking, monitoring, and new RAG creation.</p>"},{"location":"register-and-refine/inventory-management/rags/#registering-a-rag","title":"Registering a RAG:","text":"<ol> <li>Click on Create button in RAG Registry.</li> <li>Fill in important details like Name, Attributes (Output Type, Alias), Properties (Group, Permissible Purpose, Description, Approval Workflow).</li> <li>Define Input Arguments along with their types and default values.</li> <li>Select registered resources (like Model, Global Functions, Prompts, etc.) to use in RAG definition.</li> <li>Select Input Type (API-Based, Python-based, or Custom registration).</li> <li>Upload custom knowledge file if required. Define RAG logic by writing code in Initialization Logic and Retrieval Logic.</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on Save to complete the registration process.</li> </ol> <p>The registered RAGs can be evaluated in the RAG Registry or used in downstream objects (like Models, Pipeline, Reports, etc.).</p>"},{"location":"register-and-refine/inventory-management/rags/#benefits-of-rag-registration","title":"Benefits of RAG Registration:","text":"<ul> <li>Automated tracking and recording of modifications with efficient version upgrades.</li> <li>Automatic detection of Permissible Purpose violations.</li> <li>Testing and Comparison with other registered RAGs using custom and standardized validation kits.</li> <li>Enhances reusability across downstream applications and enables usage tracking with Lineage Tracking.</li> <li>Journey to production becomes more transparent and fully auditable, and production monitoring gets easier.</li> <li>Extract ready-to-productionize executable artifact.</li> <li>Fingerprinting of external API connectivity.</li> <li>Better Collaboration for continuous development and testing.</li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/","title":"Data Assets","text":"<p>The Table Registry records the location, content, and structure of source data tables used for analytics.</p> <p>The table can be registered using one of the following methods:</p> <ul> <li> <p>Connection to a Data Lake: A direct link to a data lake allows specifying the location of the data file. The link must point to a valid PySpark data file.</p> </li> <li> <p>Table Upload: Datasets with fewer rows can be uploaded directly in CSV or Excel format.</p> </li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/#managing-tables-on-the-platform","title":"Managing Tables on the Platform","text":"<p>The Table Registry organizes all the registered tables into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#registering-a-table","title":"Registering a Table:","text":"<ul> <li>Click on Create button in Table Registry.</li> <li>Fill in important details like Name, and Attributes (Alias, Group, Input Type, Location, Description).</li> <li>Select an Input Type (Data Lake or Upload Data) and provide a data link or upload files accordingly.</li> <li>Finally, click on the Save button to complete the registration.</li> </ul> <p>Note: After registering the table, users can Edit and click on the Fetch Columns button to automatically load the table columns and their types.</p> <p>Once the table is registered, data quality can be evaluated through registered Quality Checks or it can be used for validation and testing.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#benefits-of-table-registration","title":"Benefits of Table Registration:","text":"<ul> <li>Automated change history records that tracks all the modifications to the tables.</li> <li>Track the lineage of table usage in downstream applications.</li> <li>Run Quality Checks on the tables.</li> <li>Use tables for validation and testing in a fully auditable manner.</li> <li>Export tables outside the platform when required with a single click.</li> </ul>"},{"location":"register-and-refine/inventory-management/table-registry/#what-is-a-quality-check","title":"What is a Quality Check?","text":"<p>The Quality Check enables the analysis of data and the creation of standard or custom reports based on registered tables in the Table Registry. It supports the generation of profiling metrics, descriptive statistics, invalid entry detection, outlier analysis, and other custom reports and metrics to assess data quality effectively before using the data for downstream tasks like running jobs.</p> <p>Note: The Quality Check object currently supports linking only one table at a time, enabling the generation of multiple metrics and reports for a single table per analysis.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#managing-quality-checks-on-the-platform","title":"Managing Quality Checks on the Platform:","text":"<p>The Quality Check Registry organizes all the registered quality checks into customized groups at this centralized location and allows easier tracking, monitoring, and creating new ones.</p>"},{"location":"register-and-refine/inventory-management/table-registry/#registering-a-quality-check","title":"Registering a Quality Check:","text":"<ol> <li>Click on Create button in Quality Check registry.</li> <li>Fill in important details like Name, Attributes (Data-Table, Group, Descriptions, Select Data-Columns).</li> <li>Add notes, attach documentation if available in the Additional Information section.</li> <li>Lastly, click on the Save button to complete the registration process.</li> </ol>"},{"location":"register-and-refine/inventory-management/table-registry/#benefits-of-quality-check-registration","title":"Benefits of Quality Check Registration:","text":"<ul> <li>Analyze and monitor data using standard and custom reports.</li> <li>Share data analysis and evaluations with other team members.</li> </ul>"},{"location":"register-and-refine/version-management/","title":"Version Management","text":"<p>Version management is the process of systematically tracking, organizing, and controlling changes to an object. A version of an object is created every time the object\u2019s definition is either newly created or subsequently changed.</p>"},{"location":"register-and-refine/version-management/#benefits-of-version-management","title":"Benefits of Version Management?","text":"<ul> <li>Ensures Reproducibility of results and artefacts.</li> <li>Facilitates Collaboration by managing contributions from multiple users.</li> <li>Auditability &amp; Compliance by maintaining a clear history of changes and versions.</li> <li>Allows Rollback &amp; Recovery by reverting to stable versions in case of failures or unintended modifications.</li> <li>Experiment Tracking by enabling comparison of different versions.</li> <li>Supports Continuous Improvement by keeping track of incremental changes and their impact over time.</li> </ul>"},{"location":"register-and-refine/version-management/#maintaining-version-on-the-platform","title":"Maintaining Version on the platform:","text":"<p>A Draft Version (i.e. Version 1) is created as soon as an object is registered on the platform. All the modification done to a Draft Version is automatically logged in the Change History tab. Post approval process, the object can be cloned to create a new draft version (i.e. Version 2) which goes through the same cycle of changes and approvals.</p> <p></p>"},{"location":"register-and-refine/version-management/#version-tracking-within-drafts","title":"Version Tracking within Drafts","text":"<p>Change History is a structured log of modifications made to Generative AI pipelines over time ensuring a clear audit trail of changes and the ability to revert to any previous versions if needed.</p> <p>Maintaining a comprehensive change history is very important for several reasons:</p> <ul> <li>Enhances traceability, allowing teams to track when, why, and by whom changes were made.</li> <li>Improves accountability, ensuring responsible AI deployment and governance.</li> <li>Facilitates debugging, helping teams identify and roll back problematic updates.</li> <li>Ensures compliance with regulations like the EU AI Act (Article 12), which mandates record-keeping for high-risk AI systems.</li> <li>Builds trust by providing transparency into model evolution.</li> </ul> <p>All objects start their lifecycle as a \"Draft\". Drafts typically are modified multiple times before they are approved and locked. To ensure a clear record of these changes, the platform keeps track of all changes being made to a Draft.</p> <p>The history of changes on a given object is recorded in the Change History tab. The Change History is a collection of snapshots:</p> <ul> <li>The platform takes a snapshot of an object every time the object is edited and saved</li> <li>A snapshot is an exact copy of an object at the time of saving</li> <li>A change can be reverted back by restoring the snapshot at that point in time</li> <li>Snapshots can be Named and previewed or even restored as a copy</li> </ul> <p></p>"},{"location":"register-and-refine/version-management/#version-tracking-for-approved-items","title":"Version Tracking for Approved items","text":"<p>Once a Draft is approved - it can not be modified. But by creating a clone (i.e. a new version) new changes can be made to continue working on that item. The new version will be a copy of the object being cloned with the same Name, Alias, and Type.</p> <p>Only the latest approved version of an object can be cloned.</p> <p>By design, all previous uses of the older version of the object will keep using the previous versions - and not be updated to the new version unless it is explicitly updated to use the newer version. This is different compared to changes made to a Draft, as changes in a Draft are immediately propagated downstream.</p>"},{"location":"technology/self-hosting/","title":"Self-Hosted Corridor","text":"<p>Pipeline Hosting</p> <p>For guides on how the analytics and pipelines written in Corridor can be deployed to Production - refer to the Direct to Production guide.</p> <p>Guides that cover the installation, configuration, and scaling of Self-Hosted Corridor instances for analytical use.</p> <ul> <li>Minimum Requirements</li> <li> <p>Installing on your own infrastructure</p> <ul> <li>Bare metal installs - VMs or Physical Machines</li> <li>Docker based installs</li> </ul> </li> <li> <p>Configurations: How to configure your self-hosted instance of Corridor</p> <ul> <li>SSO Integration - Microsoft AD, Okta, Google Workspace, etc.</li> <li>RDBMS - Oracle, MS SQL Server, Postgres, etc.</li> <li>Web Servers - Nginx, Apache, etc.</li> <li>Integrating packages - Wheelhouse, Artifactory, etc.</li> <li>Automated Approval Steps - Jenkins, ServiceNow, JIRA, etc.</li> <li>Data Lakes - HDFS, Hive, Snowflake, etc.</li> <li>Notifications - Email, Slack, Teams, etc.</li> <li>Process Management - Systemd, Supervisor, etc.</li> </ul> </li> <li> <p>Scaling to 100s and 1000s of users</p> <ul> <li>Concurrency - Increasing number of parallel runs</li> <li>Scaling to number of users</li> <li>Backup Management</li> </ul> </li> <li> <p>Hardening your Corridor instance</p> </li> </ul>"},{"location":"technology/self-hosting/#architectural-overview","title":"Architectural Overview","text":"<p>The Analytical layer of Corridor for analysts to test and validate their logic and get the required approvals and compliance checks. The production layer is NOT described here as Corridor is isolated from the Production side.</p> <p>Corridor is divided into various components to keep it modular and enable easy scaling for cloud-based deployments and also to manage high loads without much change. Each of the components can be installed on separate machines or any subset can be installed in the same machine.</p> <p>The components are divided into:</p> <ul> <li>Web Application Server: The web application server for the analytical UI of the platform</li> <li>API Server: The API for business logic</li> <li>API - Celery worker: The worker for asynchronous API tasks</li> <li>Spark - Celery worker: The worker for asynchronous spark tasks</li> <li>Jupyter Notebook: The Jupyter Notebook server for free-form analytical use</li> <li>File Management: The file management server to manage files</li> <li>Metadata Database (SQL RDBMS): The database with all metadata provided in the Web Application</li> <li>Messaging Queue (Redis): The messaging queue to orchestrate worker tasks</li> <li>Authentication Provider: The identity and auth provider for access and permissions</li> <li>Proxy / Load Balancers: Load Balancers / Proxies to simplify the install</li> </ul> <p>Here is a typical network diagram of how the installation would look like: </p>"},{"location":"technology/self-hosting/hardening/","title":"Hardening - Security","text":"<p>This section describes additional setup that would be needed to make Corridor secure. All the below are recommended but optional, and can be configured as needed.</p>"},{"location":"technology/self-hosting/hardening/#data-storage-security","title":"Data Storage Security","text":"<p>Corridor saves data in the following locations:</p> <ul> <li>Data Lake</li> <li>File Management System</li> <li>Metadata Database</li> <li>Redis (only short-term storage)</li> <li>Jupyter Content Manager (For notebooks)</li> </ul> <p>Any data stored in them should be encrypted and backups should be maintained as needed.</p>"},{"location":"technology/self-hosting/hardening/#network-security","title":"Network Security","text":"<p>The following network connections are created in Corridor, and should be secured:</p> <ul> <li>Web Application \u2194\ufe0e API Server: HTTPS connection</li> <li>API Server / API - Celery \u2194\ufe0e File Management: FTPS connection (if using FTP)</li> <li>API Server / API - Celery \u2194\ufe0e Metadata Database: SSL connections to Database</li> <li>API Server / API - Celery / Spark - Celery \u2194\ufe0e Redis: TLS authenticated Redis connection</li> <li>Spark - Celery / Jupyter \u2194\ufe0e Spark: Kerberos</li> <li>Corridor Package \u2194\ufe0e API Server: HTTPS connection</li> <li>Jupyter \u2194\ufe0e Web Application: HTTPS connection</li> <li>End User \u2194\ufe0e Web Application / Jupyter: HTTPS / WSS connection</li> </ul>"},{"location":"technology/self-hosting/hardening/#securing-each-component","title":"Securing each component","text":"<p>This section describes the steps to follow for each of the Corridor components to ensure it can be accessed securely.</p>"},{"location":"technology/self-hosting/hardening/#common","title":"Common","text":"<ul> <li>Ensure that all configuration and installation files are readable only by the user that the process is running with</li> </ul>"},{"location":"technology/self-hosting/hardening/#web-application-server","title":"Web Application Server","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>When setting up HTTPS, also set the JWT_COOKIE_SECURE configuration to ensure JWT cookies are sent in a secure manner</li> <li>Set a strong and unique SECRET_KEY</li> <li>It should use a reliable Authentication Provider (Avoid using the inbuilt authentication provider)</li> </ul>"},{"location":"technology/self-hosting/hardening/#api-server","title":"API Server","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>Set a strong and unique SECRET_KEY</li> <li>Ensure API Keys are set to ensure only authorized access to the APIs</li> </ul>"},{"location":"technology/self-hosting/hardening/#api-celery-worker","title":"API - Celery worker","text":"<p>No specific steps are required for the API - Celery workers as no other component connects to it directly.</p>"},{"location":"technology/self-hosting/hardening/#spark-celery-worker","title":"Spark - Celery worker","text":"<ul> <li>Ensure that the cluster is Kerberized</li> <li>Ensure the standard security practices for Spark are followed as described in the   Spark - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li>Ensure that the application is served using a standard web server like Nginx or Apache httpd in   front of the WSGI server</li> <li>Setup the secure HTTPS protocol at the WSGI Server or the Web Server using an SSL certificate</li> <li>Ensure the standard security practices for Jupyter are followed as described in the   Jupyter - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#file-management","title":"File Management","text":"<p>For Local File System: Ensure that the Hard disk being used is encrypted.</p> <p>For FTP: Ensure the FTPS protocol is being used and the underlying data is encrypted</p>"},{"location":"technology/self-hosting/hardening/#metadata-database-sql-rdbms","title":"Metadata Database (SQL RDBMS)","text":"<ul> <li>Ensure that the connection to the SQL database is secured using any of the authentication methods   available to the RDBMS.</li> <li>Ensure that the Database is encrypted.</li> <li>Ensure the standard security practices for the RDBS are followed as described in its documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#messaging-queue-redis","title":"Messaging Queue (Redis)","text":"<p>For Redis:</p> <ul> <li>Use a secure protocol like TLS (if using Redis) when accessing the queue</li> <li>Limit the incoming connections by whitelisting the IPs of the Redis clients</li> <li>Enable the authentication feature in Redis is enabled</li> <li>Ensure the standard security practices for Redis are followed as described in the   Redis - Security documentation.</li> </ul>"},{"location":"technology/self-hosting/hardening/#authentication-provider","title":"Authentication Provider","text":"<ul> <li>For LDAP: Ensure the secure LDAPS is used to create connections</li> <li>For SAML: Ensure a valid x509 certificate is used to authenticate messages being sent/received</li> </ul>"},{"location":"technology/self-hosting/configurations/approvals/","title":"CI CD Integrations","text":""},{"location":"technology/self-hosting/configurations/approvals/#ci-cd-integrations","title":"CI CD Integrations","text":"<p>There are scenarios when approval of an object is not confined to the reviewers registered on the platform. The client may have an external application, where other stakeholders are already onboarded and would want to review objects on the platform. Corridor provides the ability to interact with such 3<sup>rd</sup> party applications, using API calls, and integrating their review. This can be accomplished by defining a custom handler that uses the base approval handler class exposed by Corridor.</p>"},{"location":"technology/self-hosting/configurations/approvals/#handler-class","title":"Handler class","text":"<p>The user would need to define a <code>CustomApprovalHandler</code> which would inherit Corridor's base approval handler class: <code>corridor_api.config.handlers.ApprovalHandler</code></p> <p>The logic for approval would be defined by the following method(s) inside <code>CustomApprovalHandler</code>.</p> <ul> <li><code>send_action(review, action)</code></li> <li><code>receive_action(review_id, payload)</code></li> </ul>"},{"location":"technology/self-hosting/configurations/approvals/#example","title":"Example","text":"<p>The example focuses on a model, which needs to be sent for review. We can configure what information we want to send to the tool (in <code>send_action</code>). The tool will expose an endpoint that would take that information, create a model entry on their side, carry out the necessary approval process, and send the feedback to us (in <code>receive_action</code>). <code>reviewId</code> is the key and would be used for communications.</p> <pre><code>from corridor_api.config.handlers import ApprovalHandler\n\n\nclass CustomApprovalHandler(ApprovalHandler):\n name = 'external_tool'\n\n    def send_action(self, review, action):\n        '''\n :param review: review_object\n :param action: Action taken by the user on the platform for the given review\n - 'Request Approval'\n - 'Resubmit'\n - 'Cancel'\n - 'Remind'\n - 'Edit' (if the object is in the 'Pending Approval' state and it is edited)\n '''\n url = 'http://externaltool.example.com/cp_review/'  # assuming the 3rd party app is running on PORT: 7006\n review_id = review.id\n object_ = review.object  # `corridor` object\n\n        from corridor import Model\n\n        # if we need to restrict the 3rd party approvals to Models only\n        if not isinstance(object_, Model):\n            raise NotImplementedError(f'{type(object_)} is not expected to be used with \"{self.name}\" tool!!!')\n\n json_info = {\n            'modelId': object_.parent_id,\n            'modelName': object_.name,\n            'modelVersion': object_.version,\n            'modelVersionId': object_.id,\n            'modelGroup': object_.group,\n            'createdBy': object_.created_by,\n            'reviewId': review_id,\n            'responsibilityId': review.responsibility.id,\n            'responsibilityName': review.responsibility.name,\n            'action': action,\n            'comment': review.comment,\n }\n headers = {}  # any headers can be configured (optional)\n res = requests.post(url + str(review_id), json=json_info, headers=headers)\n        return {'status': res.status_code}\n\n    def receive_action(self, review_id, payload):\n        '''\n :param review_id:  id corresponding to the review object\n (this is the same id that was sent by CP when requesting the review)\n :param payload:    payload expects 2 kwargs\n - 'action': one of 'Accept'/'Need Info'/'Need Changes'/'Reject'/'Comment'\n - 'comment': any comment which the external_tool's reviewer makes\n :return:           dictionary with `action` and `comment` for the review with id: `review_id`\n '''\n        # The external app needs to do a POST call with `action` and `comment` as part of the payload.\n        # the endpoint would look like below (assuming corridor-api is running on port 5000):\n        #   `http://localhost:5000/api/v1/models/review/&lt;&lt;reviewId&gt;&gt;/external`\n action = payload.get('action')\n comment = payload.get('comment')\n\n        # do some processing, if required\n comment = 'No comment' if comment is None else comment\n\n        return {'action': action, 'comment': comment}\n</code></pre>"},{"location":"technology/self-hosting/configurations/approvals/#configurations","title":"Configurations","text":"<p>Approval handler-related configurations need to be set in <code>api_config.py</code> along with other configurations. (assuming the <code>CustomApprovalHandler</code> class is defined in the file <code>custom_approval_handler.py</code>).</p> <pre><code>THIRD_PARTY_APPROVALS = {\n    'external_tool': {\n        'handler': 'custom_approval_handler.CustomApprovalHandler',\n },\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/common-configs/","title":"Common Configs","text":"<p>After the installation, there are some configurations that need to be configured for each of the components. This section describes all the available configurations needed for each component to work correctly. Using these configurations the setup can be tweaked as needed.</p> <p>The configurations for each component needs to be present in the corresponding config file:</p> <ul> <li><code>api_config.py</code>: For the API Server and Celery Tasks</li> <li><code>app_config.py</code>: For the Web Application Server</li> <li><code>jupyterhub_config.py</code>: For the Jupyter Notebook (Jupyterhub)</li> <li><code>jupyter_notebook_config.py</code>: For the Jupyter Notebook (Notebook server)</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#configuration-using-environment-variables","title":"Configuration using Environment Variables","text":"<p>It is possible to configure the platform using environment variables instead of (or in combination with) config files mentioned above. This might be more convenient in a cloud based deployment setting or while using a centralized secret management system (like Hashicorp Vault).</p> <p>When working with secret management systems, configurations could be loaded as environment variables during deployment of the platform.</p> <p>When configuring API/APP component via environment variables, prepend the configuration key with <code>CORRIDOR_</code>. Below are some examples for some standard data types,</p> Setting in <code>api_config.py</code> Environment Variable Equivalent <code>LICENSE_KEY = xxxxxxx</code> <code>export CORRIDOR_LICENSE_KEY=xxxxxxx</code> <code>WORKER_PROCESSES = 1</code> <code>export CORRIDOR_WORKER_PROCESSES=1</code> <code>REQUIRE_SIMULATION = False</code> <code>export CORRIDOR_REQUIRE_SIMULATION=false</code> <code>WORKER_QUEUES = ['api', 'spark', 'quick_spark']</code> <code>export CORRIDOR_WORKER_QUEUES=\"['api', 'spark', 'quick_spark']\"</code>"},{"location":"technology/self-hosting/configurations/common-configs/#api","title":"API","text":"<p>The API Configurations help in controlling how the API Server and Celery workers behave.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>LICENSE_KEY</code>: The corridor license-key to use to enable the application</li> <li><code>API_KEYS</code>: The API keys to accept requests from</li> <li><code>SQLALCHEMY_DATABASE_URI</code>: The Database URI to connect to for the Metadata Database</li> <li><code>FS_URI</code>: The FileSystem URI to connect to for File Management</li> <li><code>CELERY_BROKER_URL</code>: The URL of the Celery Broker (The Redis server for task queue management)</li> <li><code>CELERY_RESULT_BACKEND</code>: The URL of the Celery Backend (The Redis server for task queue management)</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#web-application","title":"Web Application","text":"<p>These configurations help in controlling how the Web Application Server behaves.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>SECRET_KEY</code>: Ensure a unique secret key for your setup is used</li> <li><code>REST_API_SERVER_URL</code>: The URL of the API Server for business login and metadata</li> <li><code>REST_API_KEY</code>: The API Key to use when connecting to the API Server</li> <li><code>NOTEBOOK_CONFIGS__link</code>: URL to a notebook solution</li> </ul>"},{"location":"technology/self-hosting/configurations/common-configs/#jupyter","title":"Jupyter","text":"<p>The Jupyter configurations are divided into 2 sections: jupyterhub and jupyter-notebook configurations.</p>"},{"location":"technology/self-hosting/configurations/common-configs/#jupyterhub-configurations","title":"JupyterHub Configurations","text":"<p>The configurations used by the Corridor Platform are the same as the standard Jupyter Hub configurations.</p> <p>Some of the commonly used configurations are:</p> <ul> <li><code>c.JupyterHub.bind_url</code>: The URL to host JupyterHub on</li> <li><code>c.Authenticator.auth_api_url</code>: The Corridor Web Application Server (When using the Corridor Authentication)</li> <li><code>c.Spawner.env_keep</code>: And environment variables to be kept when spawning the user jupyter-notebooks</li> <li><code>c.Authenticator.auth_api_url</code>: The API for the Authentication. The URL of the Web Application Server.</li> </ul> <p>There are also additional env variables needed by the <code>corridor</code> Python Package:</p> <ul> <li><code>os.environ['CORRIDOR_API_URL']</code>: The Corridor API Server URL</li> <li><code>os.environ['CORRIDOR_API_KEY']</code>: The Corridor API Key to use (if set)</li> </ul>"},{"location":"technology/self-hosting/configurations/database/","title":"Metadata: Database Setup","text":""},{"location":"technology/self-hosting/configurations/database/#metadata-database-setup","title":"Metadata: Database Setup","text":"<p>This section describes the different database options available for the Metadata Database. Choosing the right database is critical, as it will impact the usability of the API and Web Application of the users. The metadata database stores all the information entered into the Web Application in a structured so it can be efficiently used by other components.</p> <p>The platform currently supports the following databases:</p> <ul> <li>Oracle Database - Industry-standard</li> <li>SQLite - Meant for testing only</li> <li>MS SQL - Enterprise-level solution</li> <li>PostgreSQL - Open-source and reliable</li> </ul>"},{"location":"technology/self-hosting/configurations/database/#postgresql","title":"POSTGRESQL","text":"<p>To use postgresql, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'postgresql://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;'\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#sqlite","title":"SQLite","text":"<p>Supported versions: sqlite 3+</p> <p>Sqlite is an easy to setup database which is file-based. To use sqlite with Corridor, set the following configuration:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'sqlite:///&lt;filepath&gt;\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#oracle-db","title":"Oracle DB","text":"<p>Supported versions: Oracle DB 19+</p> <p>To use oracle, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'oracle://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;'\n</code></pre>"},{"location":"technology/self-hosting/configurations/database/#ms-sql","title":"MS SQL","text":"<p>Supported versions: SQL Server 2016+</p> <p>This required the the unixODBC devel libraries (<code>yum install unixODBC-devel</code>) and SQL Server ODBC driver to be installed.</p> <p>To use mssql, use the following configurations:</p> <pre><code>SQLALCHEMY_DATABASE_URI = 'mssql+pyodbc://&lt;username&gt;:&lt;password&gt;@&lt;hostname&gt;/&lt;dbname&gt;?driver=ODBC+Driver+17+for+SQL+Server'\n</code></pre>"},{"location":"technology/self-hosting/configurations/datalake/","title":"Datalake Integration","text":""},{"location":"technology/self-hosting/configurations/datalake/#datalake-integration","title":"Datalake Integration","text":"<p>Corridor provides the ability to connect to different kinds of data lakes which could have data saved as <code>parquet</code>, <code>orc</code>, <code>avro</code>, <code>hive tables</code> or any in any other format.</p> <p>The user could define a custom file handler that would have the logic to read/write to/from the data lake.</p>"},{"location":"technology/self-hosting/configurations/datalake/#example","title":"Example","text":"<p>The example focuses on creating a data source handler to read from hive tables. The user needs to inherit the base class: <code>DataSourceHandler</code> and define the functions:</p> <ul> <li><code>read_from_location</code></li> <li><code>write_to_location</code></li> </ul> <pre><code>from corridor_api.config.handlers import DataSourceHandler\n\n\nclass HiveTable(DataSourceHandler):\n    \"\"\"\n    Consider a case where every data table is a table in the Hive metastore.\n    The table `location` identifier is the table name.\n    \"\"\"\n\n    name = 'hive'\n    write_format = 'parquet'\n\n    def read_from_location(self, location, nrow=None):\n        try:\n            import findspark\n\n            findspark.init()\n            import pyspark\n        except ImportError:\n            import pyspark\n\n        spark = pyspark.sql.SparkSession.builder.getOrCreate()\n\n        data = spark.table(location)\n        if nrow is not None:\n            data = data.limit(nrow)\n        return data\n\n    def write_to_location(self, data, location, mode='error'):\n        return data.write.format(self.write_format).saveAsTable(location)\n</code></pre>"},{"location":"technology/self-hosting/configurations/datalake/#configuration","title":"Configuration","text":"<p>Once the handler class is created, it can be set up in <code>api_config.py</code> as below: (assuming the handler is defined in a file called <code>hive_table_hander.py</code> alongside <code>api_config.py</code>)</p> <pre><code>LAKE_DATA_SOURCE_HANDLER = 'hive_table_handler.HiveTable'\n</code></pre>"},{"location":"technology/self-hosting/configurations/notifications/","title":"Email Notifications","text":""},{"location":"technology/self-hosting/configurations/notifications/#email-notifications","title":"Email Notifications","text":"<p>Corridor provides an option to send email notifications to the user on their registered email id when an event occurs on the platform (e.g. completion of simulation, approval request for an object).</p> <p>The different events for which notifications are triggered on the Platform:</p> <ul> <li>Completion or failure of job run</li> <li>Workflow Status change of an object</li> <li>Review status changes or comments added during approval process</li> <li>Sharing of an object</li> </ul> <p>Note</p> <p>The user cannot customize which notifications will be sent as email.</p>"},{"location":"technology/self-hosting/configurations/notifications/#configuration","title":"Configuration","text":"<p>The email notifications can be configured in <code>api_config.py</code> file with the parameter: <code>NOTIFICATION_PROVIDERS</code>. The value should be a dictionary with the key being <code>email</code>. The value for <code>email</code> should be a dictionary again with the email configuration details. The email configuration details include:</p> <ul> <li><code>from</code>: The email id from which the notifications are to be sent</li> <li><code>username</code>: Username corresponding to the email id</li> <li><code>password</code>: The password for the email id</li> <li><code>host</code>: The host of the SMTP server</li> <li><code>port</code>: The port number to use</li> <li><code>ssl</code>: Should ssl be used</li> <li><code>html</code>: Should the email be parsed as an HTML file</li> </ul>"},{"location":"technology/self-hosting/configurations/notifications/#example","title":"Example","text":"<pre><code>NOTIFICATION_PROVIDERS = {\n    'email': {\n        'from': 'user@example.com',\n        'username': 'user',\n        'password': 'password',\n        'host': 'smtp.server.com',\n        'port': 465,\n        'ssl': True,\n        'html': True,\n    },\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/packages/","title":"Additional Libraries","text":""},{"location":"technology/self-hosting/configurations/packages/#additional-libraries","title":"Additional Libraries","text":"<p>Corridor provides an option to allow the users to be able to use additional libraries which are not available out of the box.</p>"},{"location":"technology/self-hosting/configurations/packages/#configuration","title":"Configuration","text":"<p>There are 5 steps which have to be followed so that the user is able to validate the definition and run successful jobs on the platform.</p> <ol> <li>Install the library in the virtual environment of API servers.</li> <li>Install the library in the virtual environment of Worker-API servers.</li> <li>Install the library in the virtual environment of Worker-Spark servers.</li> <li>Install the library on the spark cluster.</li> <li>Add the library to the Allowed Python Imports in the <code>Platform Settings</code> tab in the platform UI</li> </ol>"},{"location":"technology/self-hosting/configurations/process-management/","title":"Process Management","text":""},{"location":"technology/self-hosting/configurations/process-management/#process-management","title":"Process Management","text":"<p>For ease of maintenance and monitoring, it is recommended to use a process management tool to ensure the component daemons are running correctly. Using a process manager can simplify restarts, reboots, status-checks, logging, and configurations.</p> <p>The process management tools that are frequently used are Systemd, init-script, etc. In general, it is recommended to use the tool that the OS provides to handle these tasks.</p> <p>To simplify the installation, Supervisor can also be used, which provides application-level process management that is independent of the host setup.</p> <p>The tools available for process management require <code>sudo</code> access which might not be an option in some situations. Corridor has its own process manager for each of the components, which uses daemonisation to handle long-running, background processes.</p>"},{"location":"technology/self-hosting/configurations/process-management/#daemon-mode","title":"Daemon Mode","text":"<p>No additional configurations are required when running Corridor processes using Corridor-Daemons. We can use the below set of commands to start/stop/check_status. The logfile and pidfile for each process can be saved at custom locations by using the parameters:</p> <ul> <li>logfile: location of the log file</li> <li>pidfile: location of the pid file</li> </ul> <p>The default logfile directory is: <code>INSTALL_DIR/instances/INSTANCE/logs</code></p> <p>The default pidfile directory is: <code>INSTALL_DIR/instances/INSTANCE/pids</code></p> <p>To start the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon start\nINSTALL_DIR/venv-api/corridor-worker daemon start\nINSTALL_DIR/venv-app/corridor-app daemon start\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon start\n</code></pre> <p>To stop the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon stop\nINSTALL_DIR/venv-api/corridor-worker daemon stop\nINSTALL_DIR/venv-app/corridor-app daemon stop\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon stop\n</code></pre> <p>To check the status of the processes, we can do:</p> <pre><code>INSTALL_DIR/venv-api/corridor-api daemon status\nINSTALL_DIR/venv-api/corridor-worker daemon status\nINSTALL_DIR/venv-app/corridor-app daemon status\nINSTALL_DIR/venv-jupyter/corridor-jupyter daemon status\n</code></pre>"},{"location":"technology/self-hosting/configurations/process-management/#supervisor","title":"Supervisor","text":"<p>To use corridor with Supervisor, some useful configurations are:</p> <ul> <li><code>command</code>: The command to execute. Note, if 2 commands need to be executed, use <code>bash -c \"command1; command2\"</code></li> <li><code>stdout_logfile</code>: Log file location for the stdout logs (<code>%(program_name)s</code> and <code>%(process_num)01d</code> can be used as variables)</li> <li><code>stderr_logfile</code> or <code>redirect_stderr</code>: Log file location for the stderr logs, or redirect all the stderr logs to the stdout stream and hence have a common file for both</li> <li><code>user</code>: The user to run the process as</li> <li><code>environment</code>: The environment variables to be set before the process is run</li> <li><code>numprocs</code>: The number of processes to run</li> </ul> <p>Here are some example configuration files for the Corridor components:</p> <p>Web Application server:</p> <pre><code>[program:corridor-app]\ncommand=INSTALL_DIR/venv-app/bin/corridor-app run\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>API server:</p> <pre><code>[program:corridor-api]\ncommand=\n  bash -c \"INSTALL_DIR/venv-api/bin/corridor-api db upgrade &amp;&amp; INSTALL_DIR/venv-api/bin/corridor-api run\"\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>API - Celery worker:</p> <pre><code>[program:corridor-worker-api]\ncommand=INSTALL_DIR/venv-api/bin/corridor-worker run --queue api\nenvironment=\n  C_FORCE_ROOT=1\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>Spark - Celery worker:</p> <pre><code>[program:corridor-worker-spark]\ncommand=INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark\nenvironment=\n  C_FORCE_ROOT=1\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre> <p>Jupyter Notebook:</p> <pre><code>[program:corridor-jupyter]\ncommand=INSTALL_DIR/venv-jupyter/bin/corridor-jupyter run\nstdout_logfile=/var/log/corridor/%(program_name)s.log\nredirect_stderr=true\nuser=root\n</code></pre>"},{"location":"technology/self-hosting/configurations/process-management/#systemd","title":"Systemd","text":"<p>Many Linux OS like RHEL have systemd pre-installed. To use systemd, the following steps need to be followed:</p> <ul> <li>Add service file to systemd services folder. For example: <code>/etc/systemd/system/corridor.service</code></li> <li>To start service: <code>sudo systemctl start corridor</code>   And to run the service on startup: <code>sudo systemctl enable corridor</code></li> </ul> <p>Here are some example configuration files for the Corridor components:</p> <p>Web Application server:</p> <pre><code>[Unit]\nDescription=Corridor Web Application\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-app/bin/corridor-app run \\\n  &gt;&gt; /var/log/corridor/corridor-app.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>API server:</p> <pre><code>[Unit]\nDescription=Corridor API\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-api run \\\n  &gt;&gt; /var/log/corridor/corridor-api.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>API - Celery worker:</p> <pre><code>[Unit]\nDescription=Corridor Worker API\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-worker run --queue api \\\n  &gt;&gt; /var/log/corridor/corridor-worker-api.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Spark - Celery worker:</p> <pre><code>[Unit]\nDescription=Corridor Worker Spark\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark \\\n  &gt;&gt; /var/log/corridor/corridor-worker-spark.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Jupyter Notebook:</p> <pre><code>[Unit]\nDescription=Corridor Jupyter\nAfter=syslog.target network.target\n\n[Service]\nUser=root\nExecStart=/bin/bash -c 'INSTALL_DIR/venv-api/bin/corridor-jupyter run \\\n  &gt;&gt; /var/log/corridor/corridor-jupyter.log 2&gt;&amp;1'\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"technology/self-hosting/configurations/saml/","title":"SAML","text":""},{"location":"technology/self-hosting/configurations/saml/#saml","title":"SAML","text":"<p>While an internal authentication is available for simple and quick installations. It is recommended to use an enterprise-grade Identity Provider (IDP) to follow the infosec requirements for your organization. Corridor can integrate into IDPs and seamlessly be a tool in your organization.</p> <p>This section describes the use of SAML (Security Assertion Markup Language) for authentication. By using SAML, it is easy to ensure that the Platform is available only to users who are authorized to use it. It makes having a centralized Identity Provider hassle-free and ensures that the standard security practices like Single Sign-On, 2-factor Authentication, etc. are consistently applied to all the organization's applications.</p>"},{"location":"technology/self-hosting/configurations/saml/#setup","title":"Setup","text":"<p>To set the login based on SAML, the following information is required:</p> <p>From the IDP:</p> <ul> <li>SSO URL: The URL endpoint to initiate Single Sign On requests   Example: <code>http://&lt;idp_domain&gt;/saml/&lt;app_id&gt;/sso</code></li> <li>Entity ID: The URL endpoint to fetch the SAML metadata   Example: <code>http://&lt;idp_domain&gt;/saml/&lt;app_id&gt;</code></li> <li>Name ID: Unique ID to identify each user</li> <li>Certificate: The X509 certificate to used to ensure any messages sent/received are trusted</li> </ul> <p>From Corridor (Service Provider):</p> <ul> <li>ACS URL: <code>http://&lt;sp_domain&gt;/api/v1/saml/acs</code></li> <li>Entity ID: <code>http://&lt;sp_domain&gt;/api/v1/saml/metadata</code></li> <li>Start URL: <code>http://&lt;sp_domain&gt;/api/v1/users/saml/sso</code></li> </ul> <p>On completing the Sign On flow on the IDP side, the information returned to Corridor should contain:</p> <ul> <li>The Name ID</li> <li> <p>The following attributes:</p> <ul> <li>Email (The Attribute's name can be configured with <code>SAML_EMAIL_ATTRIBUTE</code>)</li> <li>List of Roles/Groups of the user (The Attribute's name can be configured with <code>SAML_ROLE_ATTRIBUTE</code>)</li> </ul> </li> </ul>"},{"location":"technology/self-hosting/configurations/saml/#configurations","title":"Configurations","text":"<p>In the API configurations, the following configurations need to be set:</p> <ul> <li><code>SAML_ENABLED = True</code>   Needs to be set to enable SAML as the method of authentication for login.</li> <li><code>SAML_SETTINGS = {...}</code>   Needs to be set as described in the configurations section to connect to the SP and IDP.   The SAML_SETTINGS is a dictionary with the following information defining the SP and IDP information:</li> </ul> <pre><code>{\n    # If strict is True, then the Python Toolkit will reject unsigned\n    # or unencrypted messages if it expects them to be signed or encrypted.\n    # Also it will reject the messages if the SAML standard is not strictly\n    # followed. Destination, NameId, Conditions ... are validated too.\n    \"strict\": true,\n\n    # Enable debug mode (outputs errors).\n    \"debug\": true,\n\n    # Service Provider Data that we are deploying.\n    \"sp\": {\n        # Identifier of the SP entity  (must be a URI)\n        \"entityId\": \"https://&lt;sp_domain&gt;/saml/metadata/\",\n\n        # Specifies info about where and how the &lt;AuthnResponse&gt; message MUST be\n        # returned to the requester, in this case our SP.\n        \"assertionConsumerService\": {\n            # URL Location where the &lt;Response&gt; from the IdP will be returned\n            \"url\": \"https://&lt;sp_domain&gt;/saml/acs\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\"\n },\n\n        # Specifies info about where and how the &lt;Logout Response&gt; message MUST be\n        # returned to the requester, in this case, our SP.\n        \"singleLogoutService\": {\n            # URL Location where the &lt;Response&gt; from the IdP will be returned\n            \"url\": \"https://&lt;sp_domain&gt;/saml/slo\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n        # Specifies the constraints on the name identifier to be used to\n        # represent the requested subject.\n        \"NameIDFormat\": \"urn:oasis:names:tc:SAML:2.0:nameid-format:unspecified\",\n\n        # The x509cert and privateKey of the SP\n        'x509cert': '',\n        'privateKey': ''\n },\n\n    # Identity Provider Data that we want connected with our SP.\n    \"idp\": {\n        # Identifier of the IdP entity  (must be a URI)\n        \"entityId\": \"https://&lt;idp_domain&gt;/saml/metadata\",\n\n        # SSO endpoint info of the IdP. (Authentication Request protocol)\n        \"singleSignOnService\": {\n            # URL Target of the IdP where the Authentication Request Message\n            # will be sent.\n            \"url\": \"https://&lt;idp_domain&gt;/saml/sso\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n\n        # SLO endpoint info of the IdP.\n        \"singleLogoutService\": {\n            # URL Location of the IdP where SLO Request will be sent.\n            \"url\": \"https://&lt;idp_domain&gt;/saml/sls\",\n            # SAML protocol binding to be used when returning the &lt;Response&gt;\n            # message.\n            \"binding\": \"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\"\n },\n\n        # Public x509 certificate of the IdP\n        \"x509cert\": \"&lt;connector_cert&gt;\"\n        # Instead of using the whole x509cert you can use a fingerprint\n        # (openssl x509 -noout -fingerprint -in \"idp.crt\" to generate it)\n        # \"certFingerprint\": \"\"\n\n }\n}\n</code></pre>"},{"location":"technology/self-hosting/configurations/web-servers/","title":"Web Server Setup","text":""},{"location":"technology/self-hosting/configurations/web-servers/#web-server-setup","title":"Web Server Setup","text":""},{"location":"technology/self-hosting/configurations/web-servers/#nginx-configurations","title":"Nginx Configurations","text":"<p>This section described how to use nginx (https://nginx.org/en/) as a web server. The Platform's server components can be made highly performant by using the lightweight Nginx as a reverse-proxy along with a WSGI server. Using nginx enabled the server to scale to a large number of users with ease.</p> <p>To use Nginx, it needs to be installed in the system and the daemon should be running with the appropriate site configurations setup.</p> <p>Here is an example nginx configuration:</p> <pre><code>server {\n  listen 80;\n  server_name localhost 0.0.0.0;\n\n  client_max_body_size 100m;\n  gzip on;\n  gzip_vary on;\n  gzip_min_length 10240;\n  gzip_proxied expired no-cache no-store private auth;\n  gzip_types text/plain text/css text/xml text/javascript application/x-javascript application/javascript application/xml application/json ;\n  gzip_disable \"MSIE [1-6]\\.\";\n\n  location / {\n    proxy_set_header   X-Real-IP        $remote_addr;\n    proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\n    proxy_set_header   X-Forwarded-Proto $scheme;\n    proxy_set_header   Host             $http_host;\n\n    proxy_pass http://localhost:5002;\n    proxy_read_timeout 3600;\n  }\n\n  location /jupyter {\n    proxy_set_header   X-Real-IP        $remote_addr;\n    proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;\n    proxy_set_header   X-Forwarded-Proto $scheme;\n    proxy_set_header   Host             $http_host;\n\n    proxy_pass http://localhost:5003;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_read_timeout 86400;\n  }\n}\n</code></pre> <p>Note</p> <p>If the nginx is not listening on port 80 - but listens on another port, the <code>proxy_set_header</code> for <code>Host</code> may have to be modified to <code>proxy_set_header Host $http_host</code> to ensure the correct host information is passed.</p> <p>Note: Permission issues</p> <p>In case of permission issues, ensure user permissions and SELinux is set up correctly.</p> <p>Note: Large file uploads</p> <p>If large files are expected to be uploaded, there are two settings that need to be modified  \u2003 1. add <code>proxy_request_buffering off</code> \u2003 2. add <code>client_max_body_size 0;</code> to server directive which expects large file size payload. This will disable nginx from buffering the large payload files and optimize disk space consumption.</p>"},{"location":"technology/self-hosting/configurations/web-servers/#apache-configurations","title":"Apache Configurations","text":"<p>To setup a secure connection, update the <code>/etc/httpd/sites-available/corridorapp.conf</code> file as below:</p> <pre><code>&lt;VirtualHost *:443&gt;\n    SSLEngine On\n    SSLCertificateFile /certs/app.crt\n    SSLCertificateKeyFile /certs/app.key\n    SSLCertificateChainFile /certs/ca.crt\n    ServerName www.corridorapp.com\n    ServerAlias corridorapp\n    ProxyPass / http://localhost:5002/\n    ProxyPassReverse / http://localhost:5002/\n    ErrorLog /var/www/corridorapp/log/error.log\n    CustomLog /var/www/corridorapp/log/requests.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note</p> <p>If the httpd is not listening on port 80 (443 for SSL) - but listens on another port, the corresponding port has to be added along with <code>VirtualHost</code> keyword and the <code>Listen</code> param's value has to be appropriately updated in <code>/etc/httpd/conf/httpd.conf</code> (<code>/etc/httpd/conf.d/ssl.conf</code> for SSL).</p> <p>Note: Permission issues</p> <p>In case of permission issues, ensure user permissions and SELinux is set up correctly.</p>"},{"location":"technology/self-hosting/installation/bundle-install/","title":"Install using scripts","text":"<p>This section describes the method to do a manual installation of Corridor.</p> <p>There is a command-line interface available with each component that is installed:</p> <ul> <li>Web Application Server: <code>corridor-app</code></li> <li>API Server: <code>corridor-api</code></li> <li>API - Celery worker: <code>corridor-worker</code></li> <li>Spark - Celery worker: <code>corridor-worker</code></li> <li>Jupyter Notebook: <code>corridor-jupyter</code></li> </ul>"},{"location":"technology/self-hosting/installation/bundle-install/#pre-requisites","title":"Pre Requisites","text":"<p>Before starting, ensure that the Minimum Requirements and System Dependencies are met.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#installation","title":"Installation","text":"<p>To perform an install, take the installation bundle provided and extract it into a temporary location and run the <code>install</code> script inside it:</p> <pre><code>unzip corridor-bundle.zip\nsudo ./corridor-bundle/install [app | api | worker-api | worker-spark | jupyter]\n</code></pre> <p>When installing, install the specific components of the platform that are required. This will set:</p> <ul> <li>An appropriate virtual-environment inside the provided installation path</li> <li>The configuration file for the component in that section</li> </ul> <p>The complete set of arguments for the installation can be checked with <code>./corridor-bundle/install -h</code>:</p> <pre><code>usage: install [-h] [-i INSTALL_DIR] [-n NAME] component\n\npositional arguments:\n  component             The component to install. Possible values are: api,\n                        app, worker-api, worker-spark, jupyter\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTRAS [EXTRAS ...], --extras EXTRAS [EXTRAS ...]\n                        The extra packages to install\n  -i INSTALL_DIR, --install-dir INSTALL_DIR\n                        The location to install the corridor package. Default\n                        value: /opt/corridor\n  --overwrite           Whether to overwrite the configs if already present.\n                        Default behavior is to create config files only if\n                        they don't already exist.\n</code></pre>"},{"location":"technology/self-hosting/installation/bundle-install/#running-the-application","title":"Running the Application","text":"<p>Once the installation of the component is done, it can be run using the provided command-line tool for that component. Also, each component has 1 or more configuration files that may need changes.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#web-application-server","title":"Web Application Server","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/app_config.py</code></li> <li>Run application server: <code>INSTALL_DIR/venv-app/bin/corridor-app run</code></li> <li>The WSGI application: <code>corridor_app.wsgi:app</code></li> </ul> <p>Note</p> <p>The web server (<code>corridor-app run</code>) can be used for production, by setting the <code>WSGI_SERVER</code> config to gunicorn or auto. Avoid using Werkzeug for production.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#api-server","title":"API Server","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run application server: <code>INSTALL_DIR/venv-api/bin/corridor-api run</code></li> <li>The WSGI application: <code>corridor_api.wsgi:app</code></li> </ul> <p>To initialize the database, <code>corridor-api db upgrade</code> needs to be run.</p> <p>Note</p> <p>The web server (<code>corridor-api run</code>) can be used for production, by setting the <code>WSGI_SERVER</code> config to gunicorn or auto. Avoid using Werkzeug for production.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#api-celery-worker","title":"API - Celery worker","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run celery worker: <code>INSTALL_DIR/venv-api/bin/corridor-worker run --queue api</code></li> </ul> <p>Note</p> <p>If the process is running as the root user, the env variable <code>C_FORCE_ROOT=1</code> needs to be set.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#spark-celery-worker","title":"Spark - Celery worker","text":"<ul> <li>Configuration File: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> <li>Run celery worker: <code>INSTALL_DIR/venv-api/bin/corridor-worker run --queue spark --queue quick_spark</code></li> </ul> <p>Note</p> <p>If the process is running as the root user, the env variable <code>C_FORCE_ROOT=1</code> needs to be set.</p>"},{"location":"technology/self-hosting/installation/bundle-install/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li> <p>Configuration File:</p> <ul> <li>Jupyter Hub: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/jupyterhub_config.py</code></li> <li>Jupyter Notebook: <code>INSTALL_DIR/instances/INSTANCE_NAME/config/jupyter_notebook_config.py</code></li> </ul> </li> <li> <p>Run jupyterhub server: <code>INSTALL_DIR/venv-jupyter/bin/corridor-jupyter run</code></p> </li> </ul>"},{"location":"technology/self-hosting/installation/docker/","title":"Install using Docker","text":"<p>Corridor provides production-ready Dockerfile templates along with the installation bundle to bootstrap a docker-based installation of the platform.</p> <p>As docker configurations frequently vary based on various organizations - Please reach out to us for instructions on docker-based installs</p> <p>Various components of the platform are addressed as follows in this setup:</p> Component Implementation Options File Management Mounted to host/persistent volume Metadata Database Configurable as an independent service / Can integrate with an existing service Messaging Queue Configurable as an independent service / Can integrate with an existing service SSL Certificates <ul> <li>Mounted from host/persistent volume in read-only mode</li> <li>Copied to image during docker build stage</li> </ul> Platform Configurations <ul> <li>Mounted from host/persistent volume in read-only mode</li> <li>Via environment variables during deployment</li> </ul> Spark Configurable as an independent service / Can integrate with an existing service Jupyterhub Configurable as an independent service / Can integrate with an existing service"},{"location":"technology/self-hosting/installation/docker/#system-minimum-requirements","title":"System: Minimum Requirements","text":"<ul> <li>Docker Engine: v20.10+</li> <li>(Optional) Docker compose</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/","title":"Minimum Requirements","text":"<p>This section describes the minimum requirements that are needed for a Corridor Installation.</p> <p>Broadly, the components involved are:</p> <ul> <li>Web Application &amp; Worker</li> <li>Spark Worker</li> <li>Jupyter Notebook</li> <li>File Management</li> <li>Metadata Database (SQL RDBMS)</li> <li>Redis - Messaging Queue</li> </ul> <p>For very simple installations, all of these could be installed on the same machine, we recommend keeping them separate to simplify scalability needs.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#web-application","title":"Web Application","text":"<p>A flask application which serves the User Interface and Web APIs which are accessible to users via the browser. It also includes a worker process for long running tasks in the API. This component has 2 processes: <code>corridor-app</code> and <code>corridor-worker</code></p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements","title":"Requirements","text":"<ul> <li>RAM: 4 GB</li> <li>Processor: 4 CPU</li> <li>Installation storage space: 20 GB</li> <li>Python 3.11+</li> </ul> <p>Optional:</p> <ul> <li>Web Server - Example: Nginx</li> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#spark-worker","title":"Spark Worker","text":"<p>Worker to handle any jobs triggered by users which are asynchronously. It is recommended to have at least 2 workers and increase concurrency as required.</p> <p>Note</p> <p>This needs to be installed on a machine that is configured as a Spark Gateway (i.e. A master node or an edge node of the cluster). This is not the data nodes of the cluster itself. The worker process should be able to import the <code>pyspark</code> module.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_1","title":"Requirements","text":"<ul> <li>RAM: 16 GB</li> <li>Processor: 8 CPU</li> <li>HDFS storage space: 500 GB (depends on the data being processed, HDFS space to handle shuffles need to be considered too)</li> <li>Python 3.11+</li> <li>Java 8+</li> <li>Spark 3.3+</li> </ul> <p>Optional:</p> <ul> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>A notebook for free-form analytical usage. We provide Jupyter Notebooks out-of-the-box but can integrate with existing notebook solutions too.</p> <p>Note</p> <p>This needs to be installed on a machine that is configured as a Spark Gateway (i.e. A master node or an edge node of the cluster). This is not the data nodes of the cluster itself. The jupyter notebook kernel should be able to import the <code>pyspark</code> module.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_2","title":"Requirements","text":"<ul> <li>RAM: 4 GB for base services and more as per usage by users</li> <li>Processor: 4 CPU and more as per usage by users</li> <li>Installation storage space: 10 GB</li> <li>Python 3.11+</li> <li>Spark 3.3+</li> </ul> <p>Optional:</p> <ul> <li>Process Management - Example: Supervisor or Systemd</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#file-management","title":"File Management","text":"<p>A file system management to store and retrieve files. A NAS storage that can be mounted on all servers and be accessible by all services is ideal.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_3","title":"Requirements","text":"<ul> <li>File storage space: 50 GB</li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#metadata-database","title":"Metadata Database","text":"<p>This serves as an internal RDBMS to store the state of the application and various user information.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_4","title":"Requirements","text":"<ul> <li>RAM: 2 GB</li> <li>Processor: 2 CPU</li> <li> <p>Database storage space: 5 GB</p> </li> <li> <p>SQL Databases supported:</p> <ul> <li>Oracle 19+</li> <li>MSSQL 2016+</li> <li>Postgres 11.7+</li> </ul> </li> </ul>"},{"location":"technology/self-hosting/installation/minimum-requirements/#redis-messaging-queue","title":"Redis - Messaging Queue","text":"<p>A low-latency task queue to send and receive information about the asynchronous tasks.</p>"},{"location":"technology/self-hosting/installation/minimum-requirements/#requirements_5","title":"Requirements","text":"<ul> <li>RAM: 1 GB</li> <li>Processor: 1 CPU</li> <li>DB Snapshots storage space: 10 GB</li> <li>Redis 4+</li> </ul>"},{"location":"technology/self-hosting/scaling/backups/","title":"Backups &amp; Restore","text":""},{"location":"technology/self-hosting/scaling/backups/#backups-restore","title":"Backups &amp; Restore","text":"<p>To perform backups, it is important to understand the data that the platform uses/saves.</p> <p>Data that the platform writes and the systems used for persistent storage are:</p> <ul> <li>Metadata Database</li> <li>File Management</li> <li>Data Lake</li> <li>Settings</li> </ul> <p>Other than this, Redis also contains some information - but should be considered as temporary storage and not persistent.</p>"},{"location":"technology/self-hosting/scaling/backups/#backing-up-the-systems","title":"Backing up the systems","text":""},{"location":"technology/self-hosting/scaling/backups/#metadata-database","title":"Metadata Database","text":"<p>The entire database used for Corridor needs to be backed up. There are 2 ways to run the backup:</p> <ul> <li>Use the standard backup manager as recommended/preferred for your RDBMS. For example: <code>expdp</code> for Oracle.</li> <li>Use the <code>corridor-API db export</code> command in Corridor to save your database into an SQLite file</li> </ul> <p>The <code>corridor-api db export</code> command will create an SQLite file which is a copy of your database. It includes various referential guarantees that databases provide like unique constraints, foreign keys, etc. It can be directly used as an embedded database with Corridor or can be used to import back into your RDBMS of choice. This also supports converting from 1 RBMS system to another.</p>"},{"location":"technology/self-hosting/scaling/backups/#file-management","title":"File Management","text":"<p>Depending on the system that is being used, backups need to be created. If FTP or NFS is being used - the appropriate volumes/files need to be backed up. If using a Local filesystem, the directory being used for file storage should be backed up.</p>"},{"location":"technology/self-hosting/scaling/backups/#data-lake","title":"Data Lake","text":"<p>The path configured for <code>OUTPUT_DATA_LOCATION</code> should be backed up. It contains various data files created by the platform. The recommended approach to copy files for your data lake should be used.</p>"},{"location":"technology/self-hosting/scaling/backups/#settings","title":"Settings","text":"<p>The instance folder of the configurations folder for Corridor should be backed up. This does not contain any user data, and can be reconfigured later if needed.</p> <p>This should be done for all the components that are running for Corridor - API, App worker, Workers, Jupyter, etc.</p>"},{"location":"technology/self-hosting/scaling/backups/#restoring-the-systems","title":"Restoring the systems","text":""},{"location":"technology/self-hosting/scaling/backups/#metadata-database_1","title":"Metadata Database","text":"<p>The database dump from the RDBMS system can be reloaded into another database and used.</p> <p>If the <code>corridor-api db export</code> method was used to create an SQLite file, the <code>corridor-api db import</code> command can be used to import back the SQLite file into the system where the restore is being run.</p>"},{"location":"technology/self-hosting/scaling/backups/#file-management_1","title":"File Management","text":"<p>The files copied should be kept in the same structure and the user running the corridor process should have permissions to the files on the file management system.</p>"},{"location":"technology/self-hosting/scaling/backups/#data-lake_1","title":"Data Lake","text":"<p>Ideally, the data files should be copied to the same path as the original server where the backup was taken from.</p>"},{"location":"technology/self-hosting/scaling/backups/#settings_1","title":"Settings","text":"<p>The setting files can be restored directly and permissions can be set as required.</p> <p>This should be done for all the components that are running for Corridor - API, App worker, Workers, Jupyter, etc.</p>"},{"location":"technology/self-hosting/scaling/concurrency/","title":"Multiple Corridor Workers","text":""},{"location":"technology/self-hosting/scaling/concurrency/#multiple-corridor-workers","title":"Multiple Corridor Workers","text":"<p>Corridor provides an option to run multiple workers on the same server, without the workers interfering with each other. The user needs to provide a name for each worker and worker-specific configuration in api_config.py, where each configuration is tied to the worker's name.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#custom-corridor-worker-run-command-with-a-worker-name","title":"Custom <code>corridor-worker run</code> command with a worker name","text":"<p>The worker name can be provided with the option <code>--worker-name</code> or <code>-n</code></p> <ul> <li><code>INSTALL_DIR/venv-api/bin/corridor-worker run --worker-name CUSTOM_</code></li> </ul> <p>Note</p> <p>The worker's name needs to have all capital letters. Underscores (<code>_</code>) can be part of the worker's name.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#custom-worker-configurations","title":"Custom worker configurations","text":"<p>Any worker-specific configuration is required to be added to the file:</p> <ul> <li><code>INSTALL_DIR/instances/INSTANCE_NAME/config/api_config.py</code></li> </ul> <p>To avoid synchronization issues with other workers running on the same server. The worker configurations have to be prefixed with the worker name provided in the <code>corridor-worker run</code> command above.</p>"},{"location":"technology/self-hosting/scaling/concurrency/#configurations","title":"Configurations","text":"<p>Taking the above <code>corridor-worker run</code> command as an example, where the worker name is <code>CUSTOM_</code>, the configurations would be:</p> <ul> <li><code>CUSTOM_WORKER_QUEUES</code></li> <li><code>CUSTOM_WORKER_PIDFILE</code></li> <li><code>CUSTOM_WORKER_LOGFILE</code></li> <li><code>CUSTOM_WORKER_PROCESSES</code></li> <li><code>CUSTOM_CELERY_WORKER_STATE_DB</code></li> <li><code>CUSTOM_CELERY_WORKER_HIJACK_ROOT_LOGGER</code></li> <li><code>CUSTOM_CELERY_WORKER_REDIRECT_STDOUTS</code></li> </ul>"},{"location":"technology/self-hosting/scaling/scalability/","title":"Scalability and Sizing","text":""},{"location":"technology/self-hosting/scaling/scalability/#scalability-and-sizing","title":"Scalability and Sizing","text":"<p>If the platform seems to be slow, there may be an infrastructure constraint in terms of resources that could be causing this. This section describes how different aspects of scalability have been kept in mind when designing the platform.</p> <p>The way to think about scalability in the platform is:</p> <ul> <li>Production</li> <li>Simulation and other jobs</li> <li>User-initiated analytics</li> <li>Application</li> </ul>"},{"location":"technology/self-hosting/scaling/scalability/#production","title":"Production","text":"<p>The production scalability again can be thought of in two parts:</p> <ul> <li>Time to execute the artifact-bundle/policy</li> <li>Throughput for the production orchestration</li> </ul> <p>The time taken for the artifact-bundle depends on the complexity of the logic written in the platform. This can be sped up by writing more efficient codes or simplifying the logic.</p> <p>The throughput for the orchestrations depends on the method of deployment of the Production layer, whether it is an HTTP API, serverless architecture, python runtime execution, etc.</p>"},{"location":"technology/self-hosting/scaling/scalability/#simulation-and-other-jobs","title":"Simulation and other jobs","text":"<p>The simulations and other jobs like Comparison, Validation, etc. Use the Celery workers to perform tasks. There are 2 parts which can be considered here:</p> <ul> <li>Number of parallel jobs to run</li> <li>Cluster nodes available for use per job</li> </ul> <p>If the number of parallel jobs is a concern, due to a large job blocking the queue of later jobs, etc. - this can be handled by adding more \"Spark - Celery Workers\" as needed. The Celery Workers are stateless and all state is maintained in the Task Messaging queue (Redis) and removing/adding more is seamless. Note that adding too many Spark - Celery workers could cause resource contention between the workers themselves and may require an increase in cluster nodes.</p> <p>If the spark jobs themselves are slow - then the issue is more likely the cluster nodes available per job. i.e. the number of Spark workers or YARN containers that are allocated per job. To make this faster, more spark nodes need to be provided to make Spark jobs run faster.</p>"},{"location":"technology/self-hosting/scaling/scalability/#user-initiated-analytics","title":"User-initiated analytics","text":"<p>The platform's Notebook section allows the user to initiate their own jobs in a free-form manner using Python/PySpark. As these jobs are user-dependent, they can utilize a large number of cluster resources if not used correctly. If this causes issues with the Simulation jobs, it is recommended to use separate YARN queues to manage the resource allocation appropriately.</p>"},{"location":"technology/self-hosting/scaling/scalability/#application","title":"Application","text":"<p>The Application's scalability is an important factor to consider when the number of concurrent users starts increasing over time. This can impact the usability of the application itself and can be scaled by identifying which portion of the application is causing the bottleneck. The Metadata Database or the API Server could be the cause of issues here. It can be easily rectified by adding more resources to these components or parallelizing them with Database Read Replicas or API Servers appropriately.</p> <p>Note</p> <p>The API Servers are stateless and can be easily scaled up/down as needed.</p>"}]}